\documentclass[8pt]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algorithmic}

% Color definitions
\definecolor{mlblue}{RGB}{31, 119, 180}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlpurple}{RGB}{148, 103, 189}
\definecolor{mlgray}{RGB}{127, 127, 127}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Define theorem environments
\theoremstyle{plain}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}

% Title information
\title{Mathematical Theory of Narrative Economics}
\subtitle{Stochastic Models and Econometric Methods for Narrative Quantification}
\author{Prof. Dr. Joerg Osterrieder}
\institute{Advanced Mathematical Finance Seminar}
\date{\today}

\begin{document}

% Title slide
\begin{frame}[t]
\titlepage
\end{frame}

% Table of contents
\begin{frame}[t]{Theoretical Framework Overview}
\tableofcontents
\vfill
\footnotesize
\textbf{Focus:} Mathematical foundations and theoretical models for narrative dynamics in financial markets. Emphasis on rigorous econometric identification, stochastic process theory, and optimal control. Empirical validation relegated to appendix.
\end{frame}

% PART I: MATHEMATICAL FOUNDATIONS
\section{Mathematical Foundations of Narrative Economics}

\begin{frame}[t]
\vfill
\centering
\begin{beamercolorbox}[sep=8pt,center]{title}
\usebeamerfont{title}\Large Mathematical Foundations of Narrative Economics\par
\end{beamercolorbox}
\vfill
\end{frame}

\begin{frame}[t]{Narrative Contagion as Epidemic Model}
\textbf{SIR-Type Dynamics for Narrative Spread}

Let $S(t)$, $I(t)$, $R(t)$ denote susceptible, infected, and recovered populations:

\begin{align}
\frac{dI}{dt} &= \beta S(t)I(t) - \gamma I(t) + \eta(t) \\
\frac{dS}{dt} &= -\beta S(t)I(t) + \delta R(t) \\
\frac{dR}{dt} &= \gamma I(t) - \delta R(t)
\end{align}

where $\eta(t)$ represents exogenous narrative shocks (news events).

\vfill
\textbf{Basic Reproduction Number:} $R_0 = \frac{\beta}{\gamma}$

\textbf{Critical Threshold:} Narrative becomes viral when $R_0 > 1$

\footnotesize
\textbf{Extension:} Multi-narrative competition via coupled SIR systems with cross-immunity terms.
\end{frame}

\begin{frame}[t]{Information-Theoretic Framework}
\textbf{Shannon Entropy and Mutual Information}

Narrative uncertainty measured via entropy:
\begin{equation}
H(N) = -\sum_{n \in \mathcal{N}} p(n) \log_2 p(n)
\end{equation}

Information content about returns:
\begin{equation}
I(N;R) = \sum_{n,r} p(n,r) \log_2 \frac{p(n,r)}{p(n)p(r)} = H(R) - H(R|N)
\end{equation}

\textbf{Transfer Entropy} (directional causality):
\begin{equation}
TE_{N \rightarrow R} = \sum p(r_{t+1}, r_t^k, n_t^l) \log \frac{p(r_{t+1}|r_t^k, n_t^l)}{p(r_{t+1}|r_t^k)}
\end{equation}

\vfill
\footnotesize
\textbf{Theorem:} $TE_{N \rightarrow R} > 0$ implies Granger causality from narratives to returns.
\end{frame}

\begin{frame}[t]{Stochastic Process Models}
\textbf{Jump-Diffusion Narrative Intensity}

Narrative intensity $N_t$ follows:
\begin{equation}
dN_t = \kappa(\theta - N_t)dt + \sigma \sqrt{N_t}dW_t + h dJ_t
\end{equation}

where:
\begin{itemize}
\item $\kappa$: mean reversion speed
\item $\theta$: long-run mean intensity
\item $\sigma$: diffusion volatility
\item $J_t$: Poisson process with intensity $\lambda$
\item $h$: jump size (drawn from $\mathcal{N}(\mu_J, \sigma_J^2)$)
\end{itemize}

\textbf{Moment Generating Function:}
\begin{equation}
\mathbb{E}[e^{uN_T}] = \exp\left\{A(T-t,u) + B(T-t,u)N_t\right\}
\end{equation}

where $A(\cdot)$ and $B(\cdot)$ solve Riccati ODEs.
\end{frame}

\begin{frame}[t]{Hawkes Process for Narrative Clustering}
\textbf{Self-Exciting Point Process Model}

Intensity of narrative events:
\begin{equation}
\lambda(t) = \mu + \sum_{t_i < t} \phi(t - t_i)
\end{equation}

Exponential kernel: $\phi(t) = \alpha e^{-\beta t}$

\textbf{Branching ratio:} $n = \frac{\alpha}{\beta} < 1$ (stability condition)

\textbf{Proposition:} The expected number of events triggered by a single event:
\begin{equation}
\mathbb{E}[\text{offspring}] = \int_0^\infty \phi(s)ds = \frac{\alpha}{\beta}
\end{equation}

\vfill
\footnotesize
\textbf{Application:} Model clustering of narrative events (e.g., cascade of negative news).
\end{frame}

\begin{frame}[t]{Percolation Theory for Critical Mass}
\textbf{Bond Percolation on Narrative Networks}

Consider narrative spread on network $G(V,E)$ with bond probability $p$:

\textbf{Critical Threshold:}
\begin{equation}
p_c = \frac{1}{\lambda_{\max}(A) - 1}
\end{equation}

where $\lambda_{\max}(A)$ is the largest eigenvalue of adjacency matrix.

\textbf{Giant Component Size:}
For $p > p_c$, fraction in giant component:
\begin{equation}
S = 1 - \exp(-S \cdot z \cdot p)
\end{equation}

where $z$ is average degree.

\textbf{Phase Transition:}
\begin{itemize}
\item $p < p_c$: Narrative remains localized (subcritical)
\item $p = p_c$: Critical point (power-law cluster sizes)
\item $p > p_c$: Global narrative contagion (supercritical)
\end{itemize}
\end{frame}

\begin{frame}[t]{Fisher Information and Divergence Measures}
\textbf{Fisher Information Matrix for Narrative Parameters}

For parameter vector $\bm{\theta} = (\beta, \kappa, \sigma, \lambda)$:
\begin{equation}
\mathcal{I}(\bm{\theta})_{ij} = -\mathbb{E}\left[\frac{\partial^2 \log L(\bm{\theta}|N)}{\partial \theta_i \partial \theta_j}\right]
\end{equation}

\textbf{CramÃ©r-Rao Bound:}
\begin{equation}
\text{Var}(\hat{\theta}_i) \geq [\mathcal{I}^{-1}(\bm{\theta})]_{ii}
\end{equation}

\textbf{Divergence Measures Between Narrative Distributions:}

KL Divergence:
\begin{equation}
D_{KL}(P||Q) = \int p(n) \log\frac{p(n)}{q(n)}dn
\end{equation}

Wasserstein Distance:
\begin{equation}
W_p(P,Q) = \left(\inf_{\gamma \in \Pi(P,Q)} \int ||x-y||^p d\gamma(x,y)\right)^{1/p}
\end{equation}
\end{frame}

% PART II: ECONOMETRIC THEORY
\section{Advanced Econometric Theory}

\begin{frame}[t]
\vfill
\centering
\begin{beamercolorbox}[sep=8pt,center]{title}
\usebeamerfont{title}\Large Advanced Econometric Theory\par
\end{beamercolorbox}
\vfill
\end{frame}

\begin{frame}[t]{High-Dimensional Narrative Selection}
\textbf{LASSO with Time-Varying Penalties}

Objective function for narrative selection:
\begin{equation}
\hat{\beta} = \arg\min_\beta \frac{1}{T}\sum_{t=1}^T (r_t - \bm{n}_t'\beta)^2 + \sum_{j=1}^p \lambda_j(t)|\beta_j|
\end{equation}

Adaptive weights: $\lambda_j(t) = \lambda \cdot \hat{\sigma}_j / |\hat{\beta}_j^{OLS}|^\gamma$

\textbf{Oracle Property:} Under conditions:
\begin{enumerate}
\item $\lambda_T/\sqrt{T} \rightarrow 0$ and $\lambda_T T^{(\gamma-1)/2} \rightarrow \infty$
\item Irrepresentable condition holds
\end{enumerate}

Then: $P(\hat{\mathcal{S}} = \mathcal{S}^*) \rightarrow 1$ and $\sqrt{T}(\hat{\beta}_{\mathcal{S}^*} - \beta_{\mathcal{S}^*}^*) \xrightarrow{d} \mathcal{N}(0, \Sigma)$
\end{frame}

\begin{frame}[t]{Identification via Narrative Shocks}
\textbf{Instrumental Variables Approach}

Structural equation:
\begin{equation}
r_t = \alpha + \beta n_t + \gamma' \bm{x}_t + \epsilon_t
\end{equation}

where $\mathbb{E}[n_t \epsilon_t] \neq 0$ (endogeneity).

\textbf{Instrument:} Unexpected narrative shocks $z_t$ from:
\begin{itemize}
\item Natural disasters mentioning specific narratives
\item Predetermined media coverage cycles
\item Regulatory announcements
\end{itemize}

\textbf{2SLS Estimator:}
\begin{align}
\text{First stage:} \quad & n_t = \pi_0 + \pi_1 z_t + \pi_2' \bm{x}_t + v_t \\
\text{Second stage:} \quad & r_t = \alpha + \beta \hat{n}_t + \gamma' \bm{x}_t + u_t
\end{align}

\textbf{Validity:} $\mathbb{E}[z_t \epsilon_t] = 0$ and $\pi_1 \neq 0$ (relevance)
\end{frame}

\begin{frame}[t]{Difference-in-Differences and Synthetic Control}
\textbf{DiD for Narrative Policy Shocks}

Treatment effect of narrative intervention:
\begin{equation}
Y_{it} = \alpha + \beta(Treat_i \times Post_t) + \gamma_i + \delta_t + \epsilon_{it}
\end{equation}

\textbf{Parallel Trends Assumption:}
\begin{equation}
\mathbb{E}[Y_{i,t+1}^{(0)} - Y_{i,t}^{(0)} | Treat_i = 1] = \mathbb{E}[Y_{i,t+1}^{(0)} - Y_{i,t}^{(0)} | Treat_i = 0]
\end{equation}

\textbf{Synthetic Control Method:}

Find weights $W^* = (w_1,...,w_J)$ minimizing:
\begin{equation}
||X_1 - X_0W||_V = \sqrt{(X_1 - X_0W)'V(X_1 - X_0W)}
\end{equation}

Treatment effect: $\hat{\alpha}_{1t} = Y_{1t} - \sum_{j=2}^{J+1} w_j^* Y_{jt}$

\footnotesize
\textbf{Application:} Evaluate impact of central bank narrative shifts on markets.
\end{frame}

\begin{frame}[t]{Factor Models for Narrative Dynamics}
\textbf{Dynamic Factor Structure}

Observable narratives driven by latent factors:
\begin{equation}
\bm{n}_t = \Lambda \bm{f}_t + \bm{e}_t
\end{equation}

Factor dynamics (VAR):
\begin{equation}
\bm{f}_t = \Phi_1 \bm{f}_{t-1} + ... + \Phi_p \bm{f}_{t-p} + \bm{\eta}_t
\end{equation}

\textbf{Identification via PCA:}
\begin{equation}
\hat{\bm{f}}_t = \sqrt{K} \cdot \text{eigenvectors of } \frac{1}{KT}\sum_{t=1}^T \bm{n}_t \bm{n}_t'
\end{equation}

\textbf{Asymptotic Properties:} As $K, T \rightarrow \infty$ with $\sqrt{K}/T \rightarrow 0$:
\begin{equation}
\frac{1}{T}\sum_{t=1}^T ||\hat{\bm{f}}_t - H\bm{f}_t||^2 = O_p\left(\frac{1}{\min(K,T)}\right)
\end{equation}
\end{frame}

\begin{frame}[t]{State-Space Models with Regime Switching}
\textbf{Markov-Switching Narrative Impact}

State equation:
\begin{equation}
\beta_t^{(s_t)} = \mu^{(s_t)} + \Phi^{(s_t)} \beta_{t-1}^{(s_{t-1})} + \epsilon_t^{(s_t)}
\end{equation}

Observation equation:
\begin{equation}
r_t = \bm{n}_t' \beta_t^{(s_t)} + \eta_t
\end{equation}

Transition probabilities: $P(s_t = j | s_{t-1} = i) = p_{ij}$

\textbf{Hamilton Filter:}
\begin{equation}
P(s_t|Y_t) = \frac{f(y_t|s_t, Y_{t-1}) \sum_{s_{t-1}} p_{s_{t-1},s_t} P(s_{t-1}|Y_{t-1})}{\sum_{s_t} f(y_t|s_t, Y_{t-1}) \sum_{s_{t-1}} p_{s_{t-1},s_t} P(s_{t-1}|Y_{t-1})}
\end{equation}
\end{frame}

% PART III: PORTFOLIO THEORY
\section{Portfolio Theory with Narrative Constraints}

\begin{frame}[t]
\vfill
\centering
\begin{beamercolorbox}[sep=8pt,center]{title}
\usebeamerfont{title}\Large Portfolio Theory with Narrative Constraints\par
\end{beamercolorbox}
\vfill
\end{frame}

\begin{frame}[t]{Stochastic Control for Dynamic Allocation}
\textbf{Hamilton-Jacobi-Bellman Equation}

Value function:
\begin{equation}
V(t, W, N) = \max_{\{\pi_s\}} \mathbb{E}\left[\int_t^T U(W_s)ds + B(W_T) \Big| \mathcal{F}_t\right]
\end{equation}

HJB equation:
\begin{equation}
\frac{\partial V}{\partial t} + \sup_\pi \mathcal{L}^\pi V = 0
\end{equation}

where the generator:
\begin{align}
\mathcal{L}^\pi V = & \pi W(\mu - r) \frac{\partial V}{\partial W} + \kappa(\theta - N)\frac{\partial V}{\partial N} \\
& + \frac{1}{2}\pi^2 W^2 \sigma^2 \frac{\partial^2 V}{\partial W^2} + \frac{1}{2}\sigma_N^2 N \frac{\partial^2 V}{\partial N^2} \\
& + \rho \pi W \sigma \sigma_N \sqrt{N} \frac{\partial^2 V}{\partial W \partial N}
\end{align}
\end{frame}

\begin{frame}[t]{Mean-Variance with Narrative Beta Constraints}
\textbf{Lagrangian Formulation}

\begin{equation}
\mathcal{L} = \bm{w}'\bm{\mu} - \frac{\lambda}{2}\bm{w}'\Sigma\bm{w} - \sum_{j=1}^J \gamma_j(|\bm{w}'\bm{\beta}_j| - \bar{E}_j) - \nu(\bm{w}'\bm{1} - 1)
\end{equation}

\textbf{KKT Conditions:}
\begin{align}
\nabla_w \mathcal{L} &= \bm{\mu} - \lambda\Sigma\bm{w} - \sum_j \gamma_j \text{sgn}(\bm{w}'\bm{\beta}_j)\bm{\beta}_j - \nu\bm{1} = 0 \\
\gamma_j(|\bm{w}'\bm{\beta}_j| - \bar{E}_j) &= 0, \quad \gamma_j \geq 0 \\
|\bm{w}'\bm{\beta}_j| &\leq \bar{E}_j, \quad \bm{w}'\bm{1} = 1
\end{align}

\textbf{Solution:} Iterative quadratic programming with active set method.
\end{frame}

\begin{frame}[t]{Narrative-Based Risk Measures}
\textbf{Conditional Value-at-Risk with Narrative State}

\begin{equation}
CVaR_\alpha(L|N=n) = \mathbb{E}[L | L > VaR_\alpha(L|N=n), N=n]
\end{equation}

\textbf{Coherent Risk Measure Properties:}
\begin{enumerate}
\item Monotonicity: $L_1 \leq L_2 \Rightarrow CVaR(L_1|N) \leq CVaR(L_2|N)$
\item Sub-additivity: $CVaR(L_1 + L_2|N) \leq CVaR(L_1|N) + CVaR(L_2|N)$
\item Positive homogeneity: $CVaR(cL|N) = c \cdot CVaR(L|N)$
\item Translation invariance: $CVaR(L + c|N) = CVaR(L|N) + c$
\end{enumerate}

\textbf{Optimization:}
\begin{equation}
\min_w CVaR_\alpha(\bm{w}'\bm{r}|N) = \min_{w,\xi} \left\{\xi + \frac{1}{(1-\alpha)T}\sum_{t=1}^T [\bm{w}'\bm{r}_t - \xi]^+\right\}
\end{equation}
\end{frame}

\begin{frame}[t]{Copula Models for Narrative Tail Dependencies}
\textbf{Narrative-Return Dependence Structure}

Joint distribution via copula:
\begin{equation}
F(n,r) = C(F_N(n), F_R(r); \theta)
\end{equation}

\textbf{Clayton Copula} (lower tail dependence):
\begin{equation}
C_{\text{Clayton}}(u,v) = (u^{-\theta} + v^{-\theta} - 1)^{-1/\theta}
\end{equation}

Lower tail dependence: $\lambda_L = 2^{-1/\theta}$

\textbf{Gumbel Copula} (upper tail dependence):
\begin{equation}
C_{\text{Gumbel}}(u,v) = \exp\left(-\left[(-\ln u)^\theta + (-\ln v)^\theta\right]^{1/\theta}\right)
\end{equation}

Upper tail dependence: $\lambda_U = 2 - 2^{1/\theta}$

\footnotesize
\textbf{Application:} Model asymmetric dependence during narrative crises vs normal times.
\end{frame}

% PART IV: MACHINE LEARNING THEORY
\section{Statistical Learning Theory for Narratives}

\begin{frame}[t]
\vfill
\centering
\begin{beamercolorbox}[sep=8pt,center]{title}
\usebeamerfont{title}\Large Statistical Learning Theory for Narratives\par
\end{beamercolorbox}
\vfill
\end{frame}

\begin{frame}[t]{Transformer Architecture Mathematics}
\textbf{Multi-Head Attention Mechanism}

Single attention head:
\begin{equation}
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

Multi-head attention:
\begin{align}
\text{MultiHead}(Q,K,V) &= \text{Concat}(\text{head}_1,...,\text{head}_h)W^O \\
\text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{align}

\textbf{Positional Encoding:}
\begin{align}
PE_{(pos,2i)} &= \sin(pos/10000^{2i/d_{model}}) \\
PE_{(pos,2i+1)} &= \cos(pos/10000^{2i/d_{model}})
\end{align}

\vfill
\footnotesize
\textbf{Complexity:} $O(n^2 \cdot d)$ where $n$ = sequence length, $d$ = dimension
\end{frame}

\begin{frame}[t]{Generalization Bounds for Narrative Models}
\textbf{Rademacher Complexity}

For hypothesis class $\mathcal{H}$ of narrative predictors:
\begin{equation}
\mathcal{R}_T(\mathcal{H}) = \mathbb{E}_{\sigma}\left[\sup_{h \in \mathcal{H}} \frac{1}{T}\sum_{t=1}^T \sigma_t h(\bm{n}_t)\right]
\end{equation}

where $\sigma_t \sim \text{Uniform}\{-1,+1\}$.

\textbf{Generalization Bound:}
With probability $1 - \delta$:
\begin{equation}
\mathbb{E}[L(h)] \leq \frac{1}{T}\sum_{t=1}^T L(h,\bm{n}_t,r_t) + 2\mathcal{R}_T(\mathcal{H}) + 3\sqrt{\frac{\log(2/\delta)}{2T}}
\end{equation}

\textbf{For Neural Networks:} If $||W^{(l)}||_F \leq B_l$:
\begin{equation}
\mathcal{R}_T(\mathcal{H}_{NN}) \leq \frac{2^L \prod_{l=1}^L B_l}{\sqrt{T}} \prod_{l=1}^{L-1} \sqrt{d_l}
\end{equation}
\end{frame}

\begin{frame}[t]{LSTM Gradient Flow Analysis}
\textbf{Long Short-Term Memory Dynamics}

Gate equations:
\begin{align}
f_t &= \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \quad \text{(forget)} \\
i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \quad \text{(input)} \\
\tilde{C}_t &= \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) \\
C_t &= f_t * C_{t-1} + i_t * \tilde{C}_t \\
o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \quad \text{(output)} \\
h_t &= o_t * \tanh(C_t)
\end{align}

\textbf{Gradient Flow:}
\begin{equation}
\frac{\partial L}{\partial C_\tau} = \sum_{t=\tau}^T \frac{\partial L}{\partial C_t} \prod_{i=\tau}^{t-1} f_i
\end{equation}

Prevents vanishing gradients when $f_i \approx 1$.
\end{frame}

% PART V: NETWORK THEORY
\section{Network Theory and Narrative Contagion}

\begin{frame}[t]
\vfill
\centering
\begin{beamercolorbox}[sep=8pt,center]{title}
\usebeamerfont{title}\Large Network Theory and Narrative Contagion\par
\end{beamercolorbox}
\vfill
\end{frame}

\begin{frame}[t]{Graph-Theoretic Narrative Models}
\textbf{Spectral Analysis of Narrative Networks}

Adjacency matrix $A$ with eigendecomposition:
\begin{equation}
A = Q\Lambda Q^{-1}
\end{equation}

\textbf{Narrative Centrality:}
\begin{align}
\text{Eigenvector:} \quad & Ax = \lambda_{\max} x \\
\text{PageRank:} \quad & \bm{p} = \alpha A^T\bm{p} + \frac{1-\alpha}{n}\bm{1} \\
\text{Katz:} \quad & \bm{c} = (I - \alpha A^T)^{-1} \bm{1}
\end{align}

\textbf{Diffusion Dynamics:}
\begin{equation}
\frac{d\bm{n}}{dt} = -L\bm{n} + \bm{f}
\end{equation}

where $L = D - A$ is the graph Laplacian.

\footnotesize
\textbf{Theorem:} Convergence rate determined by $\lambda_2(L)$ (algebraic connectivity).
\end{frame}

\begin{frame}[t]{Community Detection in Narrative Space}
\textbf{Modularity Optimization}

Modularity score:
\begin{equation}
Q = \frac{1}{2m}\sum_{i,j}\left(A_{ij} - \frac{k_ik_j}{2m}\right)\delta(c_i, c_j)
\end{equation}

\textbf{Spectral Clustering Algorithm:}
\begin{enumerate}
\item Compute normalized Laplacian: $L_{norm} = I - D^{-1/2}AD^{-1/2}$
\item Find eigenvectors of smallest $k$ eigenvalues
\item Cluster rows using k-means
\end{enumerate}

\textbf{Stochastic Block Model:}
\begin{equation}
P(A_{ij} = 1) = \begin{cases}
p_{in} & \text{if } c_i = c_j \\
p_{out} & \text{if } c_i \neq c_j
\end{cases}
\end{equation}

\footnotesize
\textbf{Application:} Identify clusters of co-occurring narratives (e.g., inflation-rates-Fed).
\end{frame}

% PART VI: ADVANCED TOPICS
\section{Advanced Theoretical Extensions}

\begin{frame}[t]
\vfill
\centering
\begin{beamercolorbox}[sep=8pt,center]{title}
\usebeamerfont{title}\Large Advanced Theoretical Extensions\par
\end{beamercolorbox}
\vfill
\end{frame}

\begin{frame}[t]{Continuous-Time Narrative Finance}
\textbf{Narrative-Driven Asset Pricing}

Price dynamics with narrative feedback:
\begin{equation}
\frac{dS_t}{S_t} = (\mu + \beta N_t)dt + \sigma dW_t^S + \gamma dJ_t
\end{equation}

Narrative evolution:
\begin{equation}
dN_t = \kappa(\theta - N_t)dt + \sigma_N\sqrt{N_t}dW_t^N + \alpha \log(S_t/S_{t-\delta})dt
\end{equation}

\textbf{Equilibrium Condition:}
\begin{equation}
\mu + \beta N^* = r + \lambda_{mkt}\sigma + \lambda_{narr}\beta\sigma_N\sqrt{N^*}
\end{equation}

where $\lambda_{mkt}$, $\lambda_{narr}$ are market prices of risk.

\footnotesize
\textbf{Existence:} Under Lipschitz conditions, unique strong solution exists via Picard iteration.
\end{frame}

\begin{frame}[t]{Optimal Stopping with Narrative Signals}
\textbf{American Option with Narrative State}

Value function:
\begin{equation}
V(t,S,N) = \sup_{\tau \in \mathcal{T}_{[t,T]}} \mathbb{E}\left[e^{-r(\tau-t)} g(S_\tau) \Big| S_t=S, N_t=N\right]
\end{equation}

\textbf{Free Boundary Problem:}
\begin{align}
\mathcal{L}V - rV &= 0 \quad \text{in continuation region} \\
V(t,S,N) &\geq g(S) \quad \text{everywhere} \\
V(t,S,N) &= g(S) \quad \text{on exercise boundary}
\end{align}

where $\mathcal{L}$ is the infinitesimal generator of $(S,N)$.

\textbf{Smooth Pasting:}
\begin{equation}
\frac{\partial V}{\partial S}\Big|_{S=S^*(N)} = \frac{\partial g}{\partial S}\Big|_{S=S^*(N)}
\end{equation}
\end{frame}

\begin{frame}[t]{Game Theory of Narrative Competition}
\textbf{Strategic Narrative Manipulation}

Two-player game with narrative influence:
\begin{align}
\text{Player 1:} \quad & \max_{a_1} \mathbb{E}[U_1(W_1, N(a_1, a_2))] - C_1(a_1) \\
\text{Player 2:} \quad & \max_{a_2} \mathbb{E}[U_2(W_2, N(a_1, a_2))] - C_2(a_2)
\end{align}

\textbf{Nash Equilibrium:}
\begin{equation}
\frac{\partial U_i}{\partial N} \cdot \frac{\partial N}{\partial a_i} = \frac{\partial C_i}{\partial a_i}, \quad i=1,2
\end{equation}

\textbf{Social Welfare Loss:}
\begin{equation}
DWL = W^{social} - W^{Nash} = \int_0^T \left[\sum_i (a_i^{Nash} - a_i^{opt})^2\right]dt
\end{equation}

\footnotesize
\textbf{Extension:} $N$-player game with network effects and strategic complementarities.
\end{frame}

\begin{frame}[t]{Quantum-Inspired Narrative Models}
\textbf{Narrative Superposition and Entanglement}

Narrative state as quantum amplitude:
\begin{equation}
|\psi\rangle = \sum_{n} \alpha_n |n\rangle, \quad \sum_n |\alpha_n|^2 = 1
\end{equation}

\textbf{Measurement (Observation) Operator:}
\begin{equation}
\hat{O} = \sum_n o_n |n\rangle\langle n|, \quad \langle O \rangle = \langle\psi|\hat{O}|\psi\rangle
\end{equation}

\textbf{Entangled Narrative-Market State:}
\begin{equation}
|\Psi\rangle = \sum_{n,m} \beta_{nm} |n\rangle \otimes |m\rangle
\end{equation}

\textbf{Von Neumann Entropy:}
\begin{equation}
S(\rho) = -\text{Tr}(\rho \ln \rho) = -\sum_i \lambda_i \ln \lambda_i
\end{equation}

\footnotesize
\textbf{Interpretation:} Narratives exist in superposition until "measured" by market reaction.
\end{frame}

\begin{frame}[t]{Topological Data Analysis for Narrative Spaces}
\textbf{Persistent Homology of Narrative Evolution}

Filtration of narrative complex: $K_0 \subseteq K_1 \subseteq ... \subseteq K_n$

\textbf{Betti Numbers:}
\begin{itemize}
\item $\beta_0$: Connected components (narrative clusters)
\item $\beta_1$: Loops (cyclic narrative patterns)
\item $\beta_2$: Voids (narrative gaps)
\end{itemize}

\textbf{Persistence Diagram:}
Points $(b_i, d_i)$ where features born at $b_i$, die at $d_i$

\textbf{Wasserstein Distance between Diagrams:}
\begin{equation}
W_p(D_1, D_2) = \left[\inf_{\phi} \sum_{x \in D_1} ||x - \phi(x)||_\infty^p\right]^{1/p}
\end{equation}

\textbf{Mapper Algorithm:}
$\mathcal{M} = \text{nerve}(f^{-1}(\mathcal{U}) \cap \mathcal{C})$

\footnotesize
\textbf{Application:} Detect topological changes in narrative landscape over time.
\end{frame}

\begin{frame}[t]{Mechanism Design for Narrative Aggregation}
\textbf{Truthful Narrative Reporting Mechanism}

Agent $i$ has private signal $s_i$ about narrative $n$:
\begin{equation}
u_i(a, s_i) = v_i(a, s_i) - p_i
\end{equation}

\textbf{VCG Mechanism:}
\begin{equation}
a^* = \arg\max_a \sum_i v_i(a, \hat{s}_i)
\end{equation}

Payment: $p_i = \sum_{j \neq i} v_j(a^{-i}, \hat{s}_j) - \sum_{j \neq i} v_j(a^*, \hat{s}_j)$

\textbf{Incentive Compatibility:}
\begin{equation}
v_i(a^*(s_i, s_{-i}), s_i) - p_i(s_i, s_{-i}) \geq v_i(a^*(\hat{s}_i, s_{-i}), s_i) - p_i(\hat{s}_i, s_{-i})
\end{equation}

\footnotesize
\textbf{Application:} Design prediction markets for narrative intensity aggregation.
\end{frame}

% Conclusions
\section{Theoretical Implications}

\begin{frame}[t]{Key Theoretical Contributions}
\begin{enumerate}
\item \textbf{Contagion Theory:} Narratives spread via epidemic-like dynamics with critical thresholds

\item \textbf{Information Theory:} Transfer entropy quantifies directional causality from narratives to returns

\item \textbf{Stochastic Processes:} Jump-diffusion models capture both gradual and sudden narrative shifts

\item \textbf{Econometric Identification:} IV and high-dimensional methods address endogeneity and selection

\item \textbf{Portfolio Theory:} HJB equations solve dynamic allocation with narrative state variables

\item \textbf{Learning Theory:} Generalization bounds ensure out-of-sample predictability

\item \textbf{Network Analysis:} Graph theory reveals narrative clustering and contagion paths
\end{enumerate}

\vfill
\footnotesize
\textbf{Open Questions:} Non-Markovian narrative memory, quantum narrative superposition, topological narrative spaces.
\end{frame}

\begin{frame}[t]{Thank You}
\centering
\Large Questions and Theoretical Discussion\\
\vspace{20pt}
\normalsize
\textbf{Contact:}\\
Prof. Dr. Joerg Osterrieder\\
\vspace{10pt}
\textbf{References:}\\
Detailed proofs and empirical validation in appendix\\
\vspace{10pt}
\textbf{Code Repository:}\\
Implementations available upon request
\end{frame}

% APPENDIX
\appendix
\section{Appendix: Empirical Results}

\begin{frame}[t]{Empirical Validation}
\centering
\small
\textbf{Key empirical findings moved to appendix for space}

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
Model & Out-of-Sample $R^2$ & Information Ratio \\
\midrule
Baseline & 0.08 & 0.71 \\
With Narratives & 0.18 & 1.26 \\
Machine Learning & 0.22 & 1.44 \\
\bottomrule
\end{tabular}
\end{table}

\begin{itemize}
\item Market Crash narrative: 34\% explanatory power
\item COVID-19 portfolio: 120.74\% return
\item Real-time implementation feasible
\end{itemize}

\footnotesize
Full results available in supplementary materials.
\end{frame}

\begin{frame}[t]{Performance Metrics}
\includegraphics[width=0.9\textwidth]{predictive_power_analysis.pdf}

\footnotesize
Historical backtests and implementation details available separately.
\end{frame}

\end{document}