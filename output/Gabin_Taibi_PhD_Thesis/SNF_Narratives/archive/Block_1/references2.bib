
@article{westerlund_new_2007,
	title = {New {Improved} {Tests} for {Cointegration} with {Structural} {Breaks}},
	volume = {28},
	issn = {1467-9892},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9892.2006.00504.x},
	doi = {10.1111/j.1467-9892.2006.00504.x},
	abstract = {Abstract. This article proposes Lagrange multiplier-based tests for the null hypothesis of no cointegration. The tests are general enough to allow for heteroskedastic and serially correlated errors, deterministic trends, and a structural break of unknown timing in both the intercept and slope. The limiting distributions of the test statistics are derived, and are found to be invariant not only with respect to the trend and structural break, but also with respect to the regressors. A small Monte Carlo study is also conducted to investigate the small-sample properties of the tests. The results reveal that the tests have small size distortions and good power relative to other tests.},
	language = {en},
	number = {2},
	urldate = {2024-11-28},
	journal = {Journal of Time Series Analysis},
	author = {Westerlund, Joakim and Edgerton, David L.},
	year = {2007},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9892.2006.00504.x},
	keywords = {C12, C32, C33, Cointegration test, Lagrange multiplier principle, deterministic trend, structural break},
	pages = {188--224},
}

@article{johansen_cointegration_2000,
	title = {Cointegration analysis in the presence of structural breaks in the deterministic trend},
	volume = {3},
	copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
	issn = {1368-4221, 1368-423X},
	url = {https://academic.oup.com/ectj/article/3/2/216-249/5071736},
	doi = {10.1111/1368-423X.00047},
	abstract = {When analysing macroeconomic data it is often of relevance to allow for structural breaks in the statistical analysis. In particular, cointegration analysis in the presence of structural breaks could be of interest. We propose a cointegration model with piecewise linear trend and known break points. Within this model it is possible to test cointegration rank, restrictions on the cointegrating vector as well as restrictions on the slopes of the broken linear trend.},
	language = {en},
	number = {2},
	urldate = {2024-11-28},
	journal = {The Econometrics Journal},
	author = {Johansen, Søren and Mosconi, Rocco and Nielsen, Bent},
	month = dec,
	year = {2000},
	pages = {216--249},
}

@article{dufays_relevant_2020,
	title = {Relevant parameter changes in structural break models},
	volume = {217},
	issn = {03044076},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304407619302441},
	doi = {10.1016/j.jeconom.2019.10.008},
	abstract = {Structural break time series models, which are commonly used in macroeconomics and finance, capture unknown structural changes by allowing for abrupt changes to model parameters. However, many specifications suffer from an over-parametrization issue, since typically all parameters have to change when a break occurs. We introduce a sparse change-point model to detect which parameters change over time. We propose a shrinkage prior distribution, which controls model parsimony by limiting the number of parameters that change from one structural break to another. We develop a Bayesian sampler for inference on the sparse change-point model. An extensive simulation study based on AR, ARMA and GARCH processes highlights the excellent performance of the sampler. We provide several empirical applications including an out-of-sample forecasting exercise showing that the Sparse change-point framework compares favorably with other recent time-varying parameter processes.},
	language = {en},
	number = {1},
	urldate = {2024-11-28},
	journal = {Journal of Econometrics},
	author = {Dufays, Arnaud and Rombouts, Jeroen V.K.},
	month = jul,
	year = {2020},
	pages = {46--78},
}

@misc{andreou_structural_2006,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Structural {Breaks} in {Financial} {Time} {Series}},
	url = {https://papers.ssrn.com/abstract=935971},
	doi = {10.2139/ssrn.935971},
	abstract = {This paper reviews the literature on structural breaks in financial time series. First we discuss the implications of structural breaks in financial time series for statistical inference purposes. In the second section we discuss the relevant asymptotic results and issues involved in general classifications of change-point tests in financial time series such historical versus sequential tests, parametric versus nonparametric tests and single versus multiple break tests. The third section reviews a number of structural change tests by focusing on certain characteristics or moments of financial time series such as structural break tests in the financial asset returns and volatility, long memory, tails and distribution. In addition, we review changepoint tests for the co-dependence between financial asset returns processes in the context of multivariate volatility models, copulae and last but not least asset pricing. In concluding we provide some areas of future research in the subject.},
	language = {en},
	urldate = {2024-11-28},
	publisher = {Social Science Research Network},
	author = {Andreou, Elena and Ghysels, Eric},
	month = oct,
	year = {2006},
	keywords = {Structural change, historical tests, sequential tests},
}

@article{ngure_detection_2023,
	title = {Detection and {Estimation} of {Change} {Point} in {Volatility} {Function} of {Foreign} {Exchange} {Rate} {Returns}},
	volume = {9},
	copyright = {2023 The Author(s)},
	issn = {2575-1891},
	url = {https://www.sciencepg.com/article/10.11648/j.ijdsa.20230901.11},
	doi = {10.11648/j.ijdsa.20230901.11},
	abstract = {This work aims at detection and estimation of a change point in conditional variance function of a Nonparametric Auto-Regressive Conditional Heteroscedastic model. The conditional mean and conditional variance functions are not specified a priori but estimated using Nadaraya Watson kernel. This is because inferences based on nonparametric approaches are robust against misspecification of the conditional mean function and the conditional variance function of returns. The squared residuals obtained after estimating the regression function of the returns are used in estimating the conditional variance function. Further, the squared residuals are used in developing a test statistic for unknown abrupt change point in volatility of the exchange rate returns. The test statistic takes into consideration the conditional heteroskedasticity of the disturbances, dependence of the returns, heterogeneity and fourth moment of returns. This does not require prior knowledge of the marginal or the conditional densities of the returns as opposed to maximum likelihood estimation methods. The estimator for change point is considered as the augmented maximum of the test statistic. The consistency of the estimator is stated as a theorem. The asymptotic distribution associated with the test for unknown break points is the Bessel process distribution. The Bessel process distributions have no known simple closed-form expression for the distribution function which makes it difficult to compute exact p-values. Also, the Bessel process distributions depend on two parameters which makes it hard to tabulate the critical values hence one needs to simulate them. After simulating the critical values, hypothesis testing is done in the presence and absence of a change point in volatility of a simulated time series and the test is shown to reject the null hypothesis in the presence of a change point at alpha level of significance. Further, the test fails to reject the null hypothesis in the absence of a change point at alpha level of significance. An application to United States Dollar/Kenya Shilling historical exchange rates returns is made from 1$^{\textrm{st}}$ January 2010 to 27$^{\textrm{th}}$ November 2020 where the sample size n = 2839 is done. Through binary segmentation method, three change points are detected, estimated and accounted for. A significant improvement in describing a time series is expected if a point in time for volatility change has been detected and estimated.},
	language = {En},
	number = {1},
	urldate = {2024-11-28},
	journal = {International Journal of Data Science and Analysis},
	author = {Ngure, Josephine Njeri and Waititu, Anthony Gichuhi and Mundia, Simon Maina},
	month = apr,
	year = {2023},
	note = {Number: 1
Publisher: Science Publishing Group},
	pages = {1--12},
}

@article{cho_multiple-change-point_2015,
	title = {Multiple-{Change}-{Point} {Detection} for {High} {Dimensional} {Time} {Series} via {Sparsified} {Binary} {Segmentation}},
	volume = {77},
	issn = {1369-7412},
	url = {https://doi.org/10.1111/rssb.12079},
	doi = {10.1111/rssb.12079},
	abstract = {Time series segmentation, which is also known as multiple-change-point detection, is a well-established problem. However, few solutions have been designed specifically for high dimensional situations. Our interest is in segmenting the second-order structure of a high dimensional time series. In a generic step of a binary segmentation algorithm for multivariate time series, one natural solution is to combine cumulative sum statistics obtained from local periodograms and cross-periodograms of the components of the input time series. However, the standard ‘maximum’ and ‘average’ methods for doing so often fail in high dimensions when, for example, the change points are sparse across the panel or the cumulative sum statistics are spuriously large. We propose the sparsified binary segmentation algorithm which aggregates the cumulative sum statistics by adding only those that pass a certain threshold. This ‘sparsifying’ step reduces the influence of irrelevant noisy contributions, which is particularly beneficial in high dimensions. To show the consistency of sparsified binary segmentation, we introduce the multivariate locally stationary wavelet model for time series, which is a separate contribution of this work.},
	number = {2},
	urldate = {2024-11-28},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Cho, Haeran and Fryzlewicz, Piotr},
	month = mar,
	year = {2015},
	pages = {475--507},
}

@article{haynes_computationally_2017,
	title = {A computationally efficient nonparametric approach for changepoint detection},
	volume = {27},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-016-9687-5},
	doi = {10.1007/s11222-016-9687-5},
	abstract = {In this paper we build on an approach proposed by Zou et al. (2014) for nonparametric changepoint detection. This approach defines the best segmentation for a data set as the one which minimises a penalised cost function, with the cost function defined in term of minus a non-parametric log-likelihood for data within each segment. Minimising this cost function is possible using dynamic programming, but their algorithm had a computational cost that is cubic in the length of the data set. To speed up computation, Zou et al. (2014) resorted to a screening procedure which means that the estimated segmentation is no longer guaranteed to be the global minimum of the cost function. We show that the screening procedure adversely affects the accuracy of the changepoint detection method, and show how a faster dynamic programming algorithm, pruned exact linear time (PELT) (Killick et al. 2012), can be used to find the optimal segmentation with a computational cost that can be close to linear in the amount of data. PELT requires a penalty to avoid under/over-fitting the model which can have a detrimental effect on the quality of the detected changepoints. To overcome this issue we use a relatively new method, changepoints over a range of penalties (Haynes et al. 2016), which finds all of the optimal segmentations for multiple penalty values over a continuous range. We apply our method to detect changes in heart-rate during physical activity.},
	language = {en},
	number = {5},
	urldate = {2024-11-28},
	journal = {Statistics and Computing},
	author = {Haynes, Kaylea and Fearnhead, Paul and Eckley, Idris A.},
	month = sep,
	year = {2017},
	keywords = {Activity tracking, Artificial Intelligence, CROPS, Nonparametric maximum likelihood, PELT},
	pages = {1293--1305},
}

@article{fearnhead_changepoint_2019,
	title = {Changepoint {Detection} in the {Presence} of {Outliers}},
	volume = {114},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2017.1385466},
	doi = {10.1080/01621459.2017.1385466},
	abstract = {Many traditional methods for identifying changepoints can struggle in the presence of outliers, or when the noise is heavy-tailed. Often they will infer additional changepoints to fit the outliers. To overcome this problem, data often needs to be preprocessed to remove outliers, though this is difficult for applications where the data needs to be analyzed online. We present an approach to changepoint detection that is robust to the presence of outliers. The idea is to adapt existing penalized cost approaches for detecting changes so that they use loss functions that are less sensitive to outliers. We argue that loss functions that are bounded, such as the classical biweight loss, are particularly suitable—as we show that only bounded loss functions are robust to arbitrarily extreme outliers. We present an efficient dynamic programming algorithm that can find the optimal segmentation under our penalized cost criteria. Importantly, this algorithm can be used in settings where the data needs to be analyzed online. We show that we can consistently estimate the number of changepoints, and accurately estimate their locations, using the biweight loss function. We demonstrate the usefulness of our approach for applications such as analyzing well-log data, detecting copy number variation, and detecting tampering of wireless devices. Supplementary materials for this article are available online.},
	number = {525},
	urldate = {2024-11-28},
	journal = {Journal of the American Statistical Association},
	author = {Fearnhead, Paul and Rigaill, Guillem},
	month = jan,
	year = {2019},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1080/01621459.2017.1385466},
	pages = {169--183},
}

@article{xie_change-point_2013,
	title = {Change-{Point} {Detection} for {High}-{Dimensional} {Time} {Series} {With} {Missing} {Data}},
	volume = {7},
	issn = {1941-0484},
	url = {https://ieeexplore.ieee.org/document/6381435},
	doi = {10.1109/JSTSP.2012.2234082},
	abstract = {This paper describes a novel approach to change-point detection when the observed high-dimensional data may have missing elements. The performance of classical methods for change-point detection typically scales poorly with the dimensionality of the data, so that a large number of observations are collected after the true change-point before it can be reliably detected. Furthermore, missing components in the observed data handicap conventional approaches. The proposed method addresses these challenges by modeling the dynamic distribution underlying the data as lying close to a time-varying low-dimensional submanifold embedded within the ambient observation space. Specifically, streaming data is used to track a submanifold approximation, measure deviations from this approximation, and calculate a series of statistics of the deviations for detecting when the underlying manifold has changed in a sharp or unexpected manner. The approach described in this paper leverages several recent results in the field of high-dimensional data analysis, including subspace tracking with missing data, multiscale analysis techniques for point clouds, online optimization, and change-point detection performance analysis. Simulations and experiments highlight the robustness and efficacy of the proposed approach in detecting an abrupt change in an otherwise slowly varying low-dimensional manifold.},
	number = {1},
	urldate = {2024-11-28},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Xie, Yao and Huang, Jiaji and Willett, Rebecca},
	month = feb,
	year = {2013},
	note = {Conference Name: IEEE Journal of Selected Topics in Signal Processing},
	keywords = {Approximation methods, Computational modeling, Data models, Estimation, Kernel, Manifolds, Statistical learning, Streaming media, change detection algorithms, signal detection, signal processing algorithms},
	pages = {12--27},
}

@misc{adams_bayesian_2007,
	title = {Bayesian {Online} {Changepoint} {Detection}},
	url = {http://arxiv.org/abs/0710.3742},
	doi = {10.48550/arXiv.0710.3742},
	abstract = {Changepoints are abrupt variations in the generative parameters of a data sequence. Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics. While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem. Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint. We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. Our implementation is highly modular so that the algorithm may be applied to a variety of types of data. We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Adams, Ryan Prescott and MacKay, David J. C.},
	month = oct,
	year = {2007},
	note = {arXiv:0710.3742},
	keywords = {Statistics - Machine Learning},
}

@misc{burg_evaluation_2022,
	title = {An {Evaluation} of {Change} {Point} {Detection} {Algorithms}},
	url = {http://arxiv.org/abs/2003.06222},
	doi = {10.48550/arXiv.2003.06222},
	abstract = {Change point detection is an important part of time series analysis, as the presence of a change point indicates an abrupt and significant change in the data generating process. While many algorithms for change point detection have been proposed, comparatively little attention has been paid to evaluating their performance on real-world time series. Algorithms are typically evaluated on simulated data and a small number of commonly-used series with unreliable ground truth. Clearly this does not provide sufficient insight into the comparative performance of these algorithms. Therefore, instead of developing yet another change point detection method, we consider it vastly more important to properly evaluate existing algorithms on real-world data. To achieve this, we present a data set specifically designed for the evaluation of change point detection algorithms that consists of 37 time series from various application domains. Each series was annotated by five human annotators to provide ground truth on the presence and location of change points. We analyze the consistency of the human annotators, and describe evaluation metrics that can be used to measure algorithm performance in the presence of multiple ground truth annotations. Next, we present a benchmark study where 14 algorithms are evaluated on each of the time series in the data set. Our aim is that this data set will serve as a proving ground in the development of novel change point detection algorithms.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Burg, Gerrit J. J. van den and Williams, Christopher K. I.},
	month = feb,
	year = {2022},
	note = {arXiv:2003.06222},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{truong_selective_2020,
	title = {Selective review of offline change point detection methods},
	url = {http://arxiv.org/abs/1801.00718},
	doi = {10.48550/arXiv.1801.00718},
	abstract = {This article presents a selective survey of algorithms for the offline detection of multiple change points in multivariate time series. A general yet structuring methodological strategy is adopted to organize this vast body of work. More precisely, detection algorithms considered in this review are characterized by three elements: a cost function, a search method and a constraint on the number of changes. Each of those elements is described, reviewed and discussed separately. Implementations of the main algorithms described in this article are provided within a Python package called ruptures.},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Truong, Charles and Oudre, Laurent and Vayatis, Nicolas},
	month = mar,
	year = {2020},
	note = {arXiv:1801.00718},
	keywords = {Computer Science - Computational Engineering, Finance, and Science, Statistics - Computation, Statistics - Methodology},
}

@article{aminikhanghahi_survey_2017,
	title = {A survey of methods for time series change point detection},
	volume = {51},
	issn = {0219-3116},
	url = {https://doi.org/10.1007/s10115-016-0987-z},
	doi = {10.1007/s10115-016-0987-z},
	abstract = {Change points are abrupt variations in time series data. Such abrupt changes may represent transitions that occur between states. Detection of change points is useful in modelling and prediction of time series and is found in application areas such as medical condition monitoring, climate change detection, speech and image analysis, and human activity analysis. This survey article enumerates, categorizes, and compares many of the methods that have been proposed to detect change points in time series. The methods examined include both supervised and unsupervised algorithms that have been introduced and evaluated. We introduce several criteria to compare the algorithms. Finally, we present some grand challenges for the community to consider.},
	language = {en},
	number = {2},
	urldate = {2024-11-28},
	journal = {Knowledge and Information Systems},
	author = {Aminikhanghahi, Samaneh and Cook, Diane J.},
	month = may,
	year = {2017},
	keywords = {Change point detection, Data mining, Machine learning, Segmentation, Time series data},
	pages = {339--367},
}

@article{urich_identification_nodate,
	title = {{IDENTIFICATION} {AND} {FORECASTS} {OF} {FINANCIAL} {BUBBLES}},
	language = {en},
	author = {Urich, Eth Z},
}

@misc{saha_io_2024,
	title = {The {I}/{O} {Complexity} of {Attention}, or {How} {Optimal} is {Flash} {Attention}?},
	url = {http://arxiv.org/abs/2402.07443},
	abstract = {Self-attention is at the heart of the popular Transformer architecture, yet suffers from quadratic time and memory complexity. The breakthrough FlashAttention algorithm revealed I/O complexity as the true bottleneck in scaling Transformers. Given two levels of memory hierarchy, a fast cache (e.g. GPU on-chip SRAM) and a slow memory (e.g. GPU high-bandwidth memory), the I/O complexity measures the number of accesses to memory. FlashAttention computes attention using \${\textbackslash}frac\{N{\textasciicircum}2d{\textasciicircum}2\}\{M\}\$ I/O operations where \$N\$ is the dimension of the attention matrix, \$d\$ the head-dimension and \$M\$ the cache size. However, is this I/O complexity optimal? The known lower bound only rules out an I/O complexity of \$o(Nd)\$ when \$M={\textbackslash}Theta(Nd)\$, since the output that needs to be written to slow memory is \${\textbackslash}Omega(Nd)\$. This leads to the main question of our work: Is FlashAttention I/O optimal for all values of \$M\$? We resolve the above question in its full generality by showing an I/O complexity lower bound that matches the upper bound provided by FlashAttention for any values of \$M {\textbackslash}geq d{\textasciicircum}2\$ within any constant factors. Further, we give a better algorithm with lower I/O complexity for \$M {\textless} d{\textasciicircum}2\$, and show that it is optimal as well. Moreover, our lower bounds do not rely on using combinatorial matrix multiplication for computing the attention matrix. We show even if one uses fast matrix multiplication, the above I/O complexity bounds cannot be improved. We do so by introducing a new communication complexity protocol for matrix compression, and connecting communication complexity to I/O complexity. To the best of our knowledge, this is the first work to establish a connection between communication complexity and I/O complexity, and we believe this connection could be of independent interest and will find many more applications in proving I/O complexity lower bounds in the future.},
	urldate = {2024-11-18},
	publisher = {arXiv},
	author = {Saha, Barna and Ye, Christopher},
	month = feb,
	year = {2024},
	note = {arXiv:2402.07443},
	keywords = {Computer Science - Computational Complexity, Computer Science - Data Structures and Algorithms, Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Information Theory},
}

@article{dao_transformers_nodate,
	title = {Transformers are {SSMs}: {Generalized} {Models} and {Efficient} {Algorithms} {Through}  {Structured} {State} {Space} {Duality}},
	abstract = {While Transformers have been the main architecture behind deep learning’s success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a wellstudied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba’s selective SSM that is 2-8× faster, while continuing to be competitive with Transformers on language modeling.},
	language = {en},
	author = {Dao, Tri and Gu, Albert},
}

@article{bhargava_quantifying_nodate,
	title = {Quantifying {Narratives} and their {Impact} on {Financial} {Markets}},
	abstract = {This paper introduces a media-coverage-based approach to quantify narratives and develops methodologies to explain the extent to which narratives drive financial markets and returns of investment portfolios. We show that media-derived narratives may contain predictive information for market returns beyond traditional macro indicators. Finally, we demonstrate that narrative indicators can be used to enhance asset allocation strategies and to gain or hedge exposure to narratives by constructing portfolios of narrative-sensitive assets.},
	language = {en},
	author = {Bhargava, Rajeev and Lou, Xiaoxia and Ozik, Gideon and Sadka, Ronnie and Whitmore, Travis},
}

@misc{wu_fastopic_2024,
	title = {{FASTopic}: {Pretrained} {Transformer} is a {Fast}, {Adaptive}, {Stable}, and {Transferable} {Topic} {Model}},
	url = {http://arxiv.org/abs/2405.17978},
	doi = {10.48550/arXiv.2405.17978},
	abstract = {Topic models have been evolving rapidly over the years, from conventional to recent neural models. However, existing topic models generally struggle with either effectiveness, efficiency, or stability, highly impeding their practical applications. In this paper, we propose FASTopic, a fast, adaptive, stable, and transferable topic model. FASTopic follows a new paradigm: Dual Semantic-relation Reconstruction (DSR). Instead of previous conventional, VAE-based, or clustering-based methods, DSR directly models the semantic relations among document embeddings from a pretrained Transformer and learnable topic and word embeddings. By reconstructing through these semantic relations, DSR discovers latent topics. This brings about a neat and efficient topic modeling framework. We further propose a novel Embedding Transport Plan (ETP) method. Rather than early straightforward approaches, ETP explicitly regularizes the semantic relations as optimal transport plans. This addresses the relation bias issue and thus leads to effective topic modeling. Extensive experiments on benchmark datasets demonstrate that our FASTopic shows superior effectiveness, efficiency, adaptivity, stability, and transferability, compared to state-of-the-art baselines across various scenarios.},
	urldate = {2024-11-04},
	publisher = {arXiv},
	author = {Wu, Xiaobao and Nguyen, Thong and Zhang, Delvin Ce and Wang, William Yang and Luu, Anh Tuan},
	month = oct,
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{biagini_detecting_2024,
	title = {Detecting asset price bubbles using deep learning},
	volume = {n/a},
	issn = {1467-9965},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/mafi.12443},
	doi = {10.1111/mafi.12443},
	abstract = {In this paper, we employ deep learning techniques to detect financial asset bubbles by using observed call option prices. The proposed algorithm is widely applicable and model-independent. We test the accuracy of our methodology in numerical experiments within a wide range of models and apply it to market data of tech stocks in order to assess if asset price bubbles are present. Under a given condition on the pricing of call options under asset price bubbles, we are able to provide a theoretical foundation of our approach for positive and continuous stochastic asset price processes. When such a condition is not satisfied, we focus on local volatility models. To this purpose, we give a new necessary and sufficient condition for a process with time-dependent local volatility function to be a strict local martingale.},
	language = {en},
	number = {n/a},
	urldate = {2024-09-04},
	journal = {Mathematical Finance},
	author = {Biagini, Francesca and Gonon, Lukas and Mazzon, Andrea and Meyer-Brandis, Thilo},
	year = {2024},
}

@article{huang_finbert_2023,
	title = {{FinBERT}: {A} {Large} {Language} {Model} for {Extracting} {Information} from {Financial} {Text}},
	volume = {40},
	issn = {1911-3846},
	shorttitle = {{FinBERT}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1911-3846.12832},
	doi = {10.1111/1911-3846.12832},
	abstract = {We develop FinBERT, a state-of-the-art large language model that adapts to the finance domain. We show that FinBERT incorporates finance knowledge and can better summarize contextual information in financial texts. Using a sample of researcher-labeled sentences from analyst reports, we document that FinBERT substantially outperforms the Loughran and McDonald dictionary and other machine learning algorithms, including naïve Bayes, support vector machine, random forest, convolutional neural network, and long short-term memory, in sentiment classification. Our results show that FinBERT excels in identifying the positive or negative sentiment of sentences that other algorithms mislabel as neutral, likely because it uses contextual information in financial text. We find that FinBERT's advantage over other algorithms, and Google's original bidirectional encoder representations from transformers model, is especially salient when the training sample size is small and in texts containing financial words not frequently used in general texts. FinBERT also outperforms other models in identifying discussions related to environment, social, and governance issues. Last, we show that other approaches underestimate the textual informativeness of earnings conference calls by at least 18\% compared to FinBERT. Our results have implications for academic researchers, investment professionals, and financial market regulators.},
	language = {en},
	number = {2},
	urldate = {2024-08-21},
	journal = {Contemporary Accounting Research},
	author = {Huang, Allen H. and Wang, Hui and Yang, Yi},
	year = {2023},
	keywords = {and governance (ESG), apprentissage automatique interprétable, apprentissage par transfert, apprentissage profond, classification des sentiments, deep learning, environment, environnement, grand modèle de langage, interpretable machine learning, large language model, sentiment classification, social, social et gouvernance (ESG), transfer learning},
	pages = {806--841},
}

@techreport{araci_finbert_2019,
	title = {{FinBERT}: {Financial} {Sentiment} {Analysis} with {Pre}-trained {Language} {Models}},
	shorttitle = {{FinBERT}},
	url = {http://arxiv.org/abs/1908.10063},
	abstract = {Financial sentiment analysis is a challenging task due to the specialized language and lack of labeled data in that domain. General-purpose models are not effective enough because of the specialized language used in a financial context. We hypothesize that pre-trained language models can help with this problem because they require fewer labeled examples and they can be further trained on domain-specific corpora. We introduce FinBERT, a language model based on BERT, to tackle NLP tasks in the financial domain. Our results show improvement in every measured metric on current state-of-the-art results for two financial sentiment analysis datasets. We find that even with a smaller training set and fine-tuning only a part of the model, FinBERT outperforms state-of-the-art machine learning methods.},
	urldate = {2024-08-20},
	institution = {arXiv},
	author = {Araci, Dogu},
	month = aug,
	year = {2019},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{mccrae_introduction_1992,
	title = {An {Introduction} to the {Five}-{Factor} {Model} and {Its} {Applications}},
	volume = {60},
	issn = {1467-6494},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-6494.1992.tb00970.x},
	doi = {10.1111/j.1467-6494.1992.tb00970.x},
	abstract = {The five-factor model of personality is a hierarchical organization of personality traits in terms of five basic dimensions: Extraversion, Agreeableness, Conscientiousness, Neuroticism, and Openness to Experience. Research using both natural language adjectives and theoretically based personality questionnaires supports the comprehensiveness of the model and its applicability across observers and cultures. This article summarizes the history of the model and its supporting evidence; discusses conceptions of the nature of the factors; and outlines an agenda for theorizing about the origins and operation of the factors. We argue that the model should prove useful both for individual assessment and for the elucidation of a number of topics of interest to personality psychologists.},
	language = {en},
	number = {2},
	urldate = {2024-11-06},
	journal = {Journal of Personality},
	author = {McCrae, Robert R. and John, Oliver P.},
	year = {1992},
	pages = {175--215},
}

@book{lopez_de_prado_advances_2018,
	title = {Advances in {Financial} {Machine} {Learning} {\textbar} {Wiley}},
	isbn = {978-1-119-48208-6},
	abstract = {{\textless}p{\textgreater}\textbf{Learn to understand and implement the latest machine learning innovations to improve your investment performance}{\textless}/p{\textgreater} {\textless}p{\textgreater}Machine learning (ML) is changing virtually every aspect of our lives. Today, ML algorithms accomplish tasks that \&ndash; until recently \&ndash; only expert humans could perform. And finance is ripe for disruptive innovations that will transform how the following generations understand money and invest.{\textless}/p{\textgreater} {\textless}p{\textgreater}In the book, readers will learn how to:{\textless}/p{\textgreater} {\textless}ul{\textgreater} {\textless}li{\textgreater}Structure big data in a way that is amenable to ML algorithms{\textless}/li{\textgreater} {\textless}li{\textgreater}Conduct research with ML algorithms on big data{\textless}/li{\textgreater} {\textless}li{\textgreater}Use supercomputing methods and back test their discoveries while avoiding false positives{\textless}/li{\textgreater} {\textless}/ul{\textgreater} {\textless}p{\textgreater}\textit{Advances in Financial Machine Learning} addresses real life problems faced by practitioners every day, and explains scientifically sound solutions using math, supported by code and examples. Readers become active users who can test the proposed solutions in their individual setting.{\textless}/p{\textgreater} {\textless}p{\textgreater}Written by a recognized expert and portfolio manager, this book will equip investment professionals with the groundbreaking tools needed to succeed in modern finance.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2024-11-07},
	publisher = {Wiley},
	author = {Lopez de Prado, Marcos},
	year = {2018},
}

@misc{minej_minejbert-base-personality_2024,
	title = {Minej/bert-base-personality · {Hugging} {Face}},
	url = {https://huggingface.co/Minej/bert-base-personality},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-11-06},
	author = {Minej},
	month = nov,
	year = {2024},
}

@misc{nogueira_investigating_2021,
	title = {Investigating the {Limitations} of {Transformers} with {Simple} {Arithmetic} {Tasks}},
	url = {http://arxiv.org/abs/2102.13019},
	doi = {10.48550/arXiv.2102.13019},
	abstract = {The ability to perform arithmetic tasks is a remarkable trait of human intelligence and might form a critical component of more complex reasoning tasks. In this work, we investigate if the surface form of a number has any influence on how sequence-to-sequence language models learn simple arithmetic tasks such as addition and subtraction across a wide range of values. We find that how a number is represented in its surface form has a strong influence on the model's accuracy. In particular, the model fails to learn addition of five-digit numbers when using subwords (e.g., "32"), and it struggles to learn with character-level representations (e.g., "3 2"). By introducing position tokens (e.g., "3 10e1 2"), the model learns to accurately add and subtract numbers up to 60 digits. We conclude that modern pretrained language models can easily learn arithmetic from very few examples, as long as we use the proper surface representation. This result bolsters evidence that subword tokenizers and positional encodings are components in current transformer designs that might need improvement. Moreover, we show that regardless of the number of parameters and training examples, models cannot learn addition rules that are independent of the length of the numbers seen during training. Code to reproduce our experiments is available at https://github.com/castorini/transformers-arithmetic},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Nogueira, Rodrigo and Jiang, Zhiying and Lin, Jimmy},
	month = apr,
	year = {2021},
	note = {arXiv:2102.13019},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{merrill_parallelism_2023,
	title = {The {Parallelism} {Tradeoff}: {Limitations} of {Log}-{Precision} {Transformers}},
	volume = {11},
	issn = {2307-387X},
	shorttitle = {The {Parallelism} {Tradeoff}},
	url = {https://doi.org/10.1162/tacl_a_00562},
	doi = {10.1162/tacl_a_00562},
	abstract = {Despite their omnipresence in modern NLP, characterizing the computational power of transformer neural nets remains an interesting open question. We prove that transformers whose arithmetic precision is logarithmic in the number of input tokens (and whose feedforward nets are computable using space linear in their input) can be simulated by constant-depth logspace-uniform threshold circuits. This provides insight on the power of transformers using known results in complexity theory. For example, if L≠P (i.e., not all poly-time problems can be solved using logarithmic space), then transformers cannot even accurately solve linear equalities or check membership in an arbitrary context-free grammar with empty productions. Our result intuitively emerges from the transformer architecture’s high parallelizability. We thus speculatively introduce the idea of a fundamental parallelism tradeoff: any model architecture as parallelizable as the transformer will obey limitations similar to it. Since parallelism is key to training models at massive scale, this suggests a potential inherent weakness of the scaling paradigm.},
	urldate = {2024-11-05},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Merrill, William and Sabharwal, Ashish},
	month = jun,
	year = {2023},
	pages = {531--545},
}

@misc{sanford_representational_2023,
	title = {Representational {Strengths} and {Limitations} of {Transformers}},
	url = {http://arxiv.org/abs/2306.02896},
	doi = {10.48550/arXiv.2306.02896},
	abstract = {Attention layers, as commonly used in transformers, form the backbone of modern deep learning, yet there is no mathematical description of their benefits and deficiencies as compared with other architectures. In this work we establish both positive and negative results on the representation power of attention layers, with a focus on intrinsic complexity parameters such as width, depth, and embedding dimension. On the positive side, we present a sparse averaging task, where recurrent networks and feedforward networks all have complexity scaling polynomially in the input size, whereas transformers scale merely logarithmically in the input size; furthermore, we use the same construction to show the necessity and role of a large embedding dimension in a transformer. On the negative side, we present a triple detection task, where attention layers in turn have complexity scaling linearly in the input size; as this scenario seems rare in practice, we also present natural variants that can be efficiently solved by attention layers. The proof techniques emphasize the value of communication complexity in the analysis of transformers and related models, and the role of sparse averaging as a prototypical attention task, which even finds use in the analysis of triple detection.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Sanford, Clayton and Hsu, Daniel and Telgarsky, Matus},
	month = nov,
	year = {2023},
	note = {arXiv:2306.02896},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{madusanka_identifying_2023,
	address = {Dubrovnik, Croatia},
	title = {Identifying the limits of transformers when performing model-checking with natural language},
	url = {https://aclanthology.org/2023.eacl-main.257},
	doi = {10.18653/v1/2023.eacl-main.257},
	abstract = {Can transformers learn to comprehend logical semantics in natural language? Although many strands of work on natural language inference have focussed on transformer models' ability to perform reasoning on text, the above question has not been answered adequately. This is primarily because the logical problems that have been studied in the context of natural language inference have their computational complexity vary with the logical and grammatical constructs within the sentences. As such, it is difficult to access whether the difference in accuracy is due to logical semantics or the difference in computational complexity. A problem that is much suited to address this issue is that of the model-checking problem, whose computational complexity remains constant (for fragments derived from first-order logic). However, the model-checking problem remains untouched in natural language inference research. Thus, we investigated the problem of model-checking with natural language to adequately answer the question of how the logical semantics of natural language affects transformers' performance. Our results imply that the language fragment has a significant impact on the performance of transformer models. Furthermore, we hypothesise that a transformer model can at least partially understand the logical semantics in natural language but can not completely learn the rules governing the model-checking algorithm.},
	urldate = {2024-11-05},
	booktitle = {Proceedings of the 17th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Madusanka, Tharindu and Batista-navarro, Riza and Pratt-hartmann, Ian},
	editor = {Vlachos, Andreas and Augenstein, Isabelle},
	month = may,
	year = {2023},
	pages = {3539--3550},
}

@misc{ansar_survey_2024,
	title = {A {Survey} on {Transformers} in {NLP} with {Focus} on {Efficiency}},
	url = {http://arxiv.org/abs/2406.16893},
	doi = {10.48550/arXiv.2406.16893},
	abstract = {The advent of transformers with attention mechanisms and associated pre-trained models have revolutionized the field of Natural Language Processing (NLP). However, such models are resource-intensive due to highly complex architecture. This limits their application to resource-constrained environments. While choosing an appropriate NLP model, a major trade-off exists over choosing accuracy over efficiency and vice versa. This paper presents a commentary on the evolution of NLP and its applications with emphasis on their accuracy as-well-as efficiency. Following this, a survey of research contributions towards enhancing the efficiency of transformer-based models at various stages of model development along with hardware considerations has been conducted. The goal of this survey is to determine how current NLP techniques contribute towards a sustainable society and to establish a foundation for future research.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Ansar, Wazib and Goswami, Saptarsi and Chakrabarti, Amlan},
	month = may,
	year = {2024},
	note = {arXiv:2406.16893},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{masson_evaluating_2024,
	address = {Torino, Italia},
	title = {Evaluating {Topic} {Model} on {Asymmetric} and {Multi}-{Domain} {Financial} {Corpus}},
	url = {https://aclanthology.org/2024.lrec-main.578},
	abstract = {Multiple recent research works in Finance try to quantify the exposure of market assets to various risks from text and how assets react if the risk materialize itself. We consider risk sections from french Financial Corporate Annual Reports, which are regulated documents with a mandatory section containing important risks the company is facing, to extract an accurate risk profile and exposure of companies. We identify multiple pitfalls of topic models when applied to corporate filing financial domain data for unsupervised risk distribution extraction which has not yet been studied on this domain. We propose two new metrics to evaluate the behavior of different types of topic models with respect to pitfalls previously mentioned about document risk distribution extraction. Our evaluation will focus on three aspects: regularizations, down-sampling and data augmentation. In our experiments, we found that classic Topic Models require down-sampling to obtain unbiased risks, while Topic Models using metadata and in-domain pre-trained word-embeddings partially correct the coherence imbalance per subdomain and remove sector's specific language from the detected themes. We then demonstrate the relevance and usefulness of the extracted information with visualizations that help to understand the content of such corpus and its evolution along the years.},
	urldate = {2024-11-05},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Masson, Corentin and Paroubek, Patrick},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	pages = {6515--6529},
}

@misc{scherrmann_multi-label_2023,
	title = {Multi-{Label} {Topic} {Model} for {Financial} {Textual} {Data}},
	url = {http://arxiv.org/abs/2311.07598},
	doi = {10.48550/arXiv.2311.07598},
	abstract = {This paper presents a multi-label topic model for financial texts like ad-hoc announcements, 8-K filings, finance related news or annual reports. I train the model on a new financial multi-label database consisting of 3,044 German ad-hoc announcements that are labeled manually using 20 predefined, economically motivated topics. The best model achieves a macro F1 score of more than 85\%. Translating the data results in an English version of the model with similar performance. As application of the model, I investigate differences in stock market reactions across topics. I find evidence for strong positive or negative market reactions for some topics, like announcements of new Large Scale Projects or Bankruptcy Filings, while I do not observe significant price effects for some other topics. Furthermore, in contrast to previous studies, the multi-label structure of the model allows to analyze the effects of co-occurring topics on stock market reactions. For many cases, the reaction to a specific topic depends heavily on the co-occurrence with other topics. For example, if allocated capital from a Seasoned Equity Offering (SEO) is used for restructuring a company in the course of a Bankruptcy Proceeding, the market reacts positively on average. However, if that capital is used for covering unexpected, additional costs from the development of new drugs, the SEO implies negative reactions on average.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Scherrmann, Moritz},
	month = nov,
	year = {2023},
	note = {arXiv:2311.07598},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Quantitative Finance - Statistical Finance},
}

@article{so_assessing_2022,
	title = {Assessing systemic risk in financial markets using dynamic topic networks},
	volume = {12},
	copyright = {2022 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-06399-x},
	doi = {10.1038/s41598-022-06399-x},
	abstract = {Systemic risk in financial markets refers to the breakdown of a financial system due to global events, catastrophes, or extreme incidents, leading to huge financial instability and losses. This study proposes a dynamic topic network (DTN) approach that combines topic modelling and network analysis to assess systemic risk in financial markets. We make use of Latent Dirichlet Allocation (LDA) to semantically analyse news articles, and the extracted topics then serve as input to construct topic similarity networks over time. Our results indicate how connected the topics are so that we can correlate any abnormal behaviours with volatility in the financial markets. With the 2015–2016 stock market selloff and COVID-19 as use cases, our results also suggest that the proposed DTN approach can provide an indication of (a) abnormal movement in the Dow Jones Industrial Average and (b) when the market would gradually begin to recover from such an event. From a practical risk management point of view, this analysis can be carried out on a daily basis when new data come in so that we can make use of the calculated metrics to predict real-time systemic risk in financial markets.},
	language = {en},
	number = {1},
	urldate = {2024-11-05},
	journal = {Scientific Reports},
	author = {So, Mike K. P. and Mak, Anson S. W. and Chu, Amanda M. Y.},
	month = feb,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Applied mathematics, Applied physics, Statistical physics, thermodynamics and nonlinear dynamics},
	pages = {2668},
}

@inproceedings{zhu_firm_2016,
	title = {Firm risk identification through topic analysis of textual financial disclosures},
	url = {https://ieeexplore.ieee.org/document/7850005},
	doi = {10.1109/SSCI.2016.7850005},
	abstract = {Corporate risk disclosures as part of U.S. public companies' financial reports are mandated by the Securities and Exchange Commission (SEC) since 2005. It provides forward-looking information about companies' future business and potential risks. This study analyzes risk types revealed in these risk disclosures and examines their potential implications on stock returns. Using 16,110 risk disclosures submitted to the SEC from 2011 to 2015, we apply Sentence Latent Dirichlet Allocation (Sent-LDA) model to infer risk types and propose a novel algorithm to match new factors with existing risk types which generates 90\% correct matches. We then quantify the impact of different risk factors on the distribution of stock returns using different time windows. We find that common risk factors, such as accounting risk and acquisition risk, have significant effects on both long-term and short-term stock returns. Some other factors only have short-term or long-term effects on stock returns. These findings provide evidence that the companies' self-disclosed risk factors have significant impacts on subsequent stock return volatility and such impacts can be used to predict potential stock change after the public release of the financial risk disclosures.},
	urldate = {2024-11-05},
	booktitle = {2016 {IEEE} {Symposium} {Series} on {Computational} {Intelligence} ({SSCI})},
	author = {Zhu, Xiaodi and Yang, Steve Y. and Moazeni, Somayeh},
	month = dec,
	year = {2016},
	keywords = {Analytical models, Companies, Inference algorithms, Portfolios, Probability distribution, Resource management, Security},
	pages = {1--8},
}

@article{garcia-mendez_automatic_2023,
	title = {Automatic detection of relevant information, predictions and forecasts in financial news through topic modelling with {Latent} {Dirichlet} {Allocation}},
	volume = {53},
	issn = {1573-7497},
	url = {https://doi.org/10.1007/s10489-023-04452-4},
	doi = {10.1007/s10489-023-04452-4},
	abstract = {Financial news items are unstructured sources of information that can be mined to extract knowledge for market screening applications. They are typically written by market experts who describe stock market events within the context of social, economic and political change. Manual extraction of relevant information from the continuous stream of finance-related news is cumbersome and beyond the skills of many investors, who, at most, can follow a few sources and authors. Accordingly, we focus on the analysis of financial news to identify relevant text and, within that text, forecasts and predictions. We propose a novel Natural Language Processing (nlp) system to assist investors in the detection of relevant financial events in unstructured textual sources by considering both relevance and temporality at the discursive level. Firstly, we segment the text to group together closely related text. Secondly, we apply co-reference resolution to discover internal dependencies within segments. Finally, we perform relevant topic modelling with Latent Dirichlet Allocation (lda) to separate relevant from less relevant text and then analyse the relevant text using a Machine Learning-oriented temporal approach to identify predictions and speculative statements. Our solution outperformed a rule-based baseline system. We created an experimental data set composed of 2,158 financial news items that were manually labelled by nlp researchers to evaluate our solution. Inter-agreement Alpha-reliability and accuracy values, and rouge-l results endorse its potential as a valuable tool for busy investors. The rouge-l values for the identification of relevant text and predictions/forecasts were 0.662 and 0.982, respectively. To our knowledge, this is the first work to jointly consider relevance and temporality at the discursive level. It contributes to the transfer of human associative discourse capabilities to expert systems through the combination of multi-paragraph topic segmentation and co-reference resolution to separate author expression patterns, topic modelling with lda to detect relevant text, and discursive temporality analysis to identify forecasts and predictions within this text. Our solution may have compelling applications in the financial field, including the possibility of extracting relevant statements on investment strategies to analyse authors’ reputations.},
	language = {en},
	number = {16},
	urldate = {2024-11-05},
	journal = {Applied Intelligence},
	author = {García-Méndez, Silvia and de Arriba-Pérez, Francisco and Barros-Vila, Ana and González-Castaño, Francisco J. and Costa-Montenegro, Enrique},
	month = aug,
	year = {2023},
	keywords = {Artificial Intelligence, Financial news analysis, Knowledge extraction, Latent Dirichlet Allocation, Natural language processing, Personal finance management, Temporality analysis},
	pages = {19610--19628},
}

@article{morimoto_forecasting_2017,
	title = {Forecasting {Financial} {Market} {Volatility} {Using} a {Dynamic} {Topic} {Model}},
	volume = {24},
	issn = {1573-6946},
	url = {https://doi.org/10.1007/s10690-017-9228-z},
	doi = {10.1007/s10690-017-9228-z},
	abstract = {This study employs big data and text data mining techniques to forecast financial market volatility. We incorporate financial information from online news sources into time series volatility models. We categorize a topic for each news article using time stamps and analyze the chronological evolution of the topic in the set of articles using a dynamic topic model. After calculating a topic score, we develop time series models that incorporate the score to estimate and forecast realized volatility. The results of our empirical analysis suggest that the proposed models can contribute to improving forecasting accuracy.},
	language = {en},
	number = {3},
	urldate = {2024-11-05},
	journal = {Asia-Pacific Financial Markets},
	author = {Morimoto, Takayuki and Kawasaki, Yoshinori},
	month = sep,
	year = {2017},
	keywords = {Big data, C10, C80, Dynamic topic model, Forecasting, G17, Online news, Realized volatility, Topic score},
	pages = {149--167},
}

@inproceedings{chen_comparative_2017,
	title = {Comparative text analytics via topic modeling in banking},
	url = {https://ieeexplore.ieee.org/document/8280945},
	doi = {10.1109/SSCI.2017.8280945},
	abstract = {In this paper, we compare and evaluate multiple topic modeling approaches and their effectiveness in analyzing a large set of SEC filings by US public banks. More specifically, we apply four major topic modeling methods to a corpus of 8-K and 10-K filings, from the years 2005-2016, of 578 bank holding companies. These methods include Principal Component Analysis, Non-negative Matrix Factorization, Latent Dirichlet Allocation and KATE, a novel k-competitive autoencoder for text documents. Separately for 8-K and 10-K, the usefulness and effectiveness of these methods is evaluated by comparing their performances on two classification tasks: (i) predicting which section each document corresponds to, where we consider each section within an 8-K or 10-K filing as an individual document, and (ii) detecting text from a bank's year of failure, a task for which we use bank failure data from the 2008 financial crisis. In addition, we qualitatively compare the topics discovered by the different methods. We conclude that topic modeling can be an effective tool in financial decision making and risk management.},
	urldate = {2024-11-05},
	booktitle = {2017 {IEEE} {Symposium} {Series} on {Computational} {Intelligence} ({SSCI})},
	author = {Chen, Yu and Rabbani, Rhaad M. and Gupta, Aparna and Zaki, Mohammed J.},
	month = nov,
	year = {2017},
	keywords = {Analytical models, Companies, Computational modeling, Feature extraction, Neurons, Principal component analysis, Vocabulary},
	pages = {1--8},
}

@article{aziz_machine_2022,
	title = {Machine learning in finance: {A} topic modeling approach},
	volume = {28},
	copyright = {© 2021 John Wiley \& Sons Ltd.},
	issn = {1468-036X},
	shorttitle = {Machine learning in finance},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/eufm.12326},
	doi = {10.1111/eufm.12326},
	abstract = {We identify the core topics of research applying machine learning to finance. We use a probabilistic topic modeling approach to make sense of this diverse body of research spanning across multiple disciplines. Through a latent Dirichlet allocation topic modeling technique, we extract 15 coherent research topics that are the focus of 5942 academic studies from 1990 to 2020. We find that these topics can be grouped into four categories: Price-forecasting techniques, financial markets analysis, risk forecasting and financial perspectives. We first describe and structure these topics and then further show how the topic focus has evolved over the last three decades. A notable trend we find is the emergence of text-based machine learning, for example, for sentiment analysis, in recent years. Our study thus provides a structured topography for finance researchers seeking to integrate machine learning research approaches in their exploration of finance phenomena. We also showcase the benefits to finance researchers of the method of probabilistic modeling of topics for deep comprehension of a body of literature.},
	language = {en},
	number = {3},
	urldate = {2024-11-05},
	journal = {European Financial Management},
	author = {Aziz, Saqib and Dowling, Michael and Hammami, Helmi and Piepenbrink, Anke},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/eufm.12326},
	keywords = {finance, latent dirichlet allocation, machine learning, textual analysis, topic modeling},
	pages = {744--770},
}

@misc{httpswwwkagglecom_kaggle_2024,
	title = {kaggle: {Your} {Machine} {Learning} and {Data} {Science} {Community}},
	shorttitle = {kaggle.com},
	url = {https://www.kaggle.com/},
	abstract = {Kaggle is the world’s largest data science community with powerful tools and resources to help you achieve your data science goals.},
	language = {en},
	urldate = {2024-11-05},
	author = {https://www.kaggle.com},
	month = nov,
	year = {2024},
}

@misc{nyt_scraping_2024,
	title = {Scraping {New} {York} {Times} {Articles} ({Daily} {Updated})},
	url = {https://kaggle.com/code/aryansingh0909/scraping-new-york-times-articles-daily-updated},
	abstract = {Explore and run machine learning code with Kaggle Notebooks {\textbar} Using data from NYT Articles: 2.1M+ (2000-Present) Daily Updated},
	language = {en},
	urldate = {2024-11-05},
	author = {NYT},
	month = nov,
	year = {2024},
}

@misc{lowe_samloweroberta-base-go_emotions_2024,
	title = {{SamLowe}/roberta-base-go\_emotions · {Hugging} {Face}},
	url = {https://huggingface.co/SamLowe/roberta-base-go_emotions},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-11-05},
	author = {Lowe, Sam},
	month = nov,
	year = {2024},
}

@misc{goemotions_goemotions_2024,
	title = {{GoEmotions}: {A} {Dataset} for {Fine}-{Grained} {Emotion} {Classification}},
	shorttitle = {{GoEmotions}},
	url = {http://research.google/blog/goemotions-a-dataset-for-fine-grained-emotion-classification/},
	abstract = {Posted by Dana Alon and Jeongwoo Ko, Software Engineers, Google Research Emotions are a key aspect of social interactions, influencing the way peop...},
	language = {en},
	urldate = {2024-11-05},
	author = {GoEmotions, Google},
	month = nov,
	year = {2024},
}

@article{li_news_2014,
	title = {News impact on stock price return via sentiment analysis},
	volume = {69},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705114001440},
	doi = {10.1016/j.knosys.2014.04.022},
	abstract = {Financial news articles are believed to have impacts on stock price return. Previous works model news pieces in bag-of-words space, which analyzes the latent relationship between word statistical patterns and stock price movements. However, news sentiment, which is an important ring on the chain of mapping from the word patterns to the price movements, is rarely touched. In this paper, we ﬁrst implement a generic stock price prediction framework, and plug in six different models with different analyzing approaches. To take one step further, we use Harvard psychological dictionary and Loughran–McDonald ﬁnancial sentiment dictionary to construct a sentiment space. Textual news articles are then quantitatively measured and projected onto the sentiment space. Instance labeling method is rigorously discussed and tested. We evaluate the models’ prediction accuracy and empirically compare their performance at different market classiﬁcation levels. Experiments are conducted on ﬁve years historical Hong Kong Stock Exchange prices and news articles. Results show that (1) at individual stock, sector and index levels, the models with sentiment analysis outperform the bag-of-words model in both validation set and independent testing set; (2) the models which use sentiment polarity cannot provide useful predictions; (3) there is a minor difference between the models using two different sentiment dictionaries.},
	language = {en},
	urldate = {2024-11-04},
	journal = {Knowledge-Based Systems},
	author = {Li, Xiaodong and Xie, Haoran and Chen, Li and Wang, Jianping and Deng, Xiaotie},
	month = oct,
	year = {2014},
	pages = {14--23},
}

@article{yadav_sentiment_2020,
	title = {Sentiment analysis of financial news using unsupervised approach},
	volume = {167},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050920307912},
	doi = {10.1016/j.procs.2020.03.325},
	abstract = {Sentiment analysis aims to determine the sentiment strength from a textual source for good decision making. This work focuses on application of sentiment analysis in financial news. The semantic orientation of documents is first calculated by tuning the existing technique for financial domain. The existing technique is found to have limitations in identifying representative phrases that effectively capture the sentiment of the text. Two alternative techniques - one using Noun-verb combinations and the other a hybrid one, are evaluated. Noun-verb approach yields best results in the experiment conducted.},
	language = {en},
	urldate = {2024-11-04},
	journal = {Procedia Computer Science},
	author = {Yadav, Anita and Jha, C K and Sharan, Aditi and Vaish, Vikrant},
	year = {2020},
	pages = {589--598},
}

@article{loughran_when_2011,
	title = {When {Is} a {Liability} {Not} a {Liability}? {Textual} {Analysis}, {Dictionaries}, and 10‐{Ks}},
	volume = {66},
	issn = {0022-1082, 1540-6261},
	shorttitle = {When {Is} a {Liability} {Not} a {Liability}?},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2010.01625.x},
	doi = {10.1111/j.1540-6261.2010.01625.x},
	abstract = {Previous research uses negative word counts to measure the tone of a text. We show that word lists developed for other disciplines misclassify common words in ﬁnancial text. In a large sample of 10-Ks during 1994 to 2008, almost three-fourths of the words identiﬁed as negative by the widely used Harvard Dictionary are words typically not considered negative in ﬁnancial contexts. We develop an alternative negative word list, along with ﬁve other word lists, that better reﬂect tone in ﬁnancial text. We link the word lists to 10-K ﬁling returns, trading volume, return volatility, fraud, material weakness, and unexpected earnings.},
	language = {en},
	number = {1},
	urldate = {2024-11-04},
	journal = {The Journal of Finance},
	author = {Loughran, Tim and Mcdonald, Bill},
	month = feb,
	year = {2011},
	pages = {35--65},
}

@misc{grootendorst_bertopic_2022,
	title = {{BERTopic}: {Neural} topic modeling with a class-based {TF}-{IDF} procedure},
	url = {http://arxiv.org/abs/2203.05794},
	abstract = {Topic models can be useful tools to discover latent topics in collections of documents. Recent studies have shown the feasibility of approach topic modeling as a clustering task. We present BERTopic, a topic model that extends this process by extracting coherent topic representation through the development of a class-based variation of TF-IDF. More specifically, BERTopic generates document embedding with pre-trained transformer-based language models, clusters these embeddings, and finally, generates topic representations with the class-based TF-IDF procedure. BERTopic generates coherent topics and remains competitive across a variety of benchmarks involving classical models and those that follow the more recent clustering approach of topic modeling.},
	urldate = {2024-11-04},
	publisher = {arXiv},
	author = {Grootendorst, Maarten},
	month = mar,
	year = {2022},
	note = {Number: arXiv:2203.05794
arXiv:2203.05794},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{ertopcu_new_2017,
	title = {A new approach for named entity recognition},
	url = {https://ieeexplore.ieee.org/document/8093439},
	doi = {10.1109/UBMK.2017.8093439},
	abstract = {Many sentences create certain impressions on people. These impressions help the reader to have an insight about the sentence via some entities. In NLP, this process corresponds to Named Entity Recognition (NER). NLP algorithms can trace a lot of entities in the sentence like person, location, date, time or money. One of the major problems in these operations are confusions about whether the word denotes the name of a person, a location or an organisation, or whether an integer stands for a date, time or money. In this study, we design a new model for NER algorithms. We train this model in our predefined dataset and compare the results with other models. In the end we get considerable outcomes in a dataset containing 1400 sentences.},
	urldate = {2024-11-04},
	booktitle = {2017 {International} {Conference} on {Computer} {Science} and {Engineering} ({UBMK})},
	author = {Ertopçu, Burak and Kanburoğlu, Ali Buğra and Topsakal, Ozan and Açıkgöz, Onur and Gürkan, Ali Tunca and Özenç, Berke and Çam, İlker and Avar, Begüm and Ercan, Gökhan and Yıldız, Olcay Taner},
	month = oct,
	year = {2017},
	keywords = {Algorithm design and analysis, Companies, Information Extraction, Named Entity Recognition, Natural Language Processing, Tagging},
	pages = {474--479},
}

@inproceedings{kang_deep_2022,
	title = {Deep {Learning}-{Based} {Named} {Entity} {Recognition} and {Knowledge} {Graph} for {Accidents} of {Commercial} {Bank}},
	url = {https://ieeexplore.ieee.org/document/9983563},
	doi = {10.1109/ICKII55100.2022.9983563},
	abstract = {With the diversified development of business, the construction of the banking system has become increasingly complex, which is prone to accidents. Since system accidents are the result of the combined action of various risk factors, accident management requires comprehensive knowledge support. Although bank accident management has accumulated a large amount of data, there is still a lack of effective solutions to obtain the required knowledge from big data quickly and accurately when faced with a specific accident. To solve the above problems, we developed bank accident management from the perspective of knowledge support to introduce relevant methods and technologies in the field of artificial intelligence. Then, accident management based on named entity recognition and knowledge graph can be developed. The entity annotation corpus in banking accidents is constructed. For the context of each bank accident, key information (four types of entities: time, accident name, loss amount, and reason) is automatically extracted by the BERT-BiLSTM-CRF model. Various entities and relational knowledge elements in the knowledge graph are retained in the graph database Neo4j to form a knowledge graph in the field of banking accidents. We provide important references for the bank's accident analysis, cause investigation, resource allocation, and management decision-making.},
	urldate = {2024-11-04},
	booktitle = {2022 {IEEE} 5th {International} {Conference} on {Knowledge} {Innovation} and {Invention} ({ICKII} )},
	author = {Kang, Wenhao and Cheung, Chi Fai},
	month = jul,
	year = {2022},
	note = {ISSN: 2770-4785},
	keywords = {Accident, Banking, Commercial Bank, Data mining, Deep Learning, Knowledge Graph, Knowledge Management, Knowledge engineering, Named Entity Recognition, Semantics, Soft sensors, Technological innovation, Text recognition},
	pages = {103--107},
}

@article{jehangir_survey_2023,
	title = {A survey on {Named} {Entity} {Recognition} — datasets, tools, and methodologies},
	volume = {3},
	issn = {29497191},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2949719123000146},
	doi = {10.1016/j.nlp.2023.100017},
	abstract = {Natural language processing (NLP) is crucial in the current processing of data because it takes into account many sources, formats, and purposes of data as well as information from various sectors of our economy, government, and private and public lives. We perform a variety of NLP operations on the text in order to complete certain tasks. One of them is NER (Named Entity Recognition). An act of recognizing and categorizing named entities that are presented in a text document is known as named entity recognition. The purpose of NER is to find references of rigid designators in the text which belong to established semantic kinds like a person, place, organization, etc. It acts as a cornerstone for many Information Extraction-related activities. In this work, we present a thorough analysis of several methodologies for NER ranging from unsupervised learning, rule-based, supervised learning, and various Deep Learning based approaches. We examine the most relevant datasets, tools, and deep learning approaches like Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Bidirectional Long Short Term Memory, Transfer learning approaches, and numerous other approaches currently being used in present-day NER problem environments and their applications. Finally, we outline the difficulties NER systems encounter and future directions.},
	language = {en},
	urldate = {2024-11-04},
	journal = {Natural Language Processing Journal},
	author = {Jehangir, Basra and Radhakrishnan, Saravanan and Agarwal, Rahul},
	month = jun,
	year = {2023},
	pages = {100017},
}

@inproceedings{konstantinidis_comparative_2023,
	title = {A comparative study on {ML}-based approaches for {Main} {Entity} {Detection} in {Financial} {Reports}},
	url = {https://ieeexplore.ieee.org/document/10167951},
	doi = {10.1109/DSP58604.2023.10167951},
	abstract = {Modern AI technologies which exploit the classification and/or prediction capacities of Deep Neural Architectures demonstrate superior performance to traditional approaches in most cases. However, they come with the unavoidable shortcoming of lack of transparency in their outcomes. This attribute renders them unsuitable for big industrial sectors, such as finance, investment management, etc. Specifically, their "black-box" nature makes them unattractive in cases where human understanding in the decision making process is required and may be legally mandatory. In such cases, traditional (i.e., non-deep learning) ML approaches are still preferred, to minimize for example the presence of false positives. In this context, this paper introduces an unsupervised, trustful, bottom-up probabilistic approach for Named Entity Recognition (NER) in financial reports, while in parallel it provides a comparative study on well-known ML-approaches in terms of their performance. The proposed approach builds on the probability of appearance of representative tokens within the given reports and utilizes Kronecker’s Delta and the Total Probability Theorem to construct a probabilistic model that estimates the overall classification probability of a document.},
	urldate = {2024-11-04},
	booktitle = {2023 24th {International} {Conference} on {Digital} {Signal} {Processing} ({DSP})},
	author = {Konstantinidis, Thanos and Xu, Yao Lei and Constantinides, Tony G. and Mandic, Danilo P.},
	month = jun,
	year = {2023},
	note = {ISSN: 2165-3577},
	keywords = {Artificial intelligence, Closed box, Decision making, Digital signal processing, Finance, Investment, Probabilistic logic},
	pages = {1--5},
}

@book{shiller_irrational_2009,
	title = {Irrational {Exuberance}: ({Second} {Edition})},
	isbn = {978-1-4008-2436-6},
	shorttitle = {Irrational {Exuberance}},
	abstract = {This first edition of this book was a broad study, drawing on a wide range of published research and historical evidence, of the enormous stock market boom that started around 1982 and picked up incredible speed after 1995. Although it took as its specific starting point this ongoing boom, it placed it in the context of stock market booms generally, and it also made concrete suggestions regarding policy changes that should be initiated in response to this and other such booms. The book argued that the boom represents a speculative bubble, not grounded in sensible economic fundamentals. Part one of the book considered structural factors behind the boom. A list of twelve precipitating factors that appear to be its ultimate causes was given. Amplification mechanisms, naturally-occurring Ponzi processes, that enlarge the effects of these precipitating factors, were described. Part Two discussed cultural factors, the effects of the news media, and of "new era" economic thinking. Part Three discussed psychological factors, psychological anchors for the market and herd behavior. Part Four discussed attempts to rationalize exuberance: efficient markets theory and theories that investors are learning. Part Five presented policy options and actions that should be taken.  The second edition, 2005, added an analysis of the real estate bubble as similar to the stock market bubble that preceded it, and warned that "Significant further rises in these markets could lead, eventually, to even more significant declines. The bad outcome could be that eventual declines would result in a substantial increase in the rate of personal bankruptcies, which could lead to a secondary string of bankruptcies of financial institutions as well. Another long-run consequence could be a decline in consumer and business confidence, and another, possibly worldwide, recession." Thus, the second edition of this book was among the first to warn of the global financial crisis that began with the subprime mortgage debacle in 2007},
	language = {en},
	publisher = {Princeton University Press},
	author = {Shiller, Robert J.},
	month = feb,
	year = {2009},
	keywords = {Business \& Economics / Economics / Theory, Business \& Economics / Investments \& Securities / General, Business \& Economics / Real Estate / General},
}

@book{kindleberger_manias_2005,
	address = {London},
	title = {Manias, {Panics} and {Crashes}},
	copyright = {http://www.springer.com/tdm},
	url = {http://link.springer.com/10.1057/9780230628045},
	language = {en},
	urldate = {2024-11-04},
	publisher = {Palgrave Macmillan UK},
	author = {Kindleberger, Charles P. and Aliber, Robert Z.},
	year = {2005},
	doi = {10.1057/9780230628045},
	keywords = {Expansion, crisis, money},
}

@misc{msca_msca_2024,
	title = {{MSCA} {Digital} {Finance}},
	url = {https://www.digital-finance-msca.com/individual-research-projects},
	urldate = {2024-11-04},
	journal = {Digital Finance MSCA},
	author = {MSCA},
	month = nov,
	year = {2024},
}

@misc{cost_action_2024,
	title = {Action {CA19130}},
	url = {https://www.cost.eu/actions/CA19130},
	urldate = {2024-11-04},
	journal = {COST},
	author = {COST},
	month = nov,
	year = {2024},
}

@misc{snsf_snsf_2024,
	title = {{SNSF} {Narrative} {Digital} {Finance} - {EU} {COST} {Fin}-{AI}},
	url = {https://wiki.fin-ai.eu/index.php/SNSF_Narrative_Digital_Finance},
	urldate = {2024-11-04},
	author = {SNSF},
	month = nov,
	year = {2024},
}

@techreport{ke_predicting_2021,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Predicting {Returns} with {Text} {Data}},
	url = {https://papers.ssrn.com/abstract=3389884},
	abstract = {We introduce a new text-mining methodology that extracts information from news articles to predict asset returns. Unlike more common sentiment scores used for stock return prediction (e.g., those sold by commercial vendors or built with dictionary-based methods), our supervised learning framework constructs a score that is specifically adapted to the problem of return prediction. Our method proceeds in three steps: 1) isolating a list of terms via predictive screening, 2) assigning prediction weights to these words via topic modeling, and 3) aggregating terms into an article-level predictive score via penalized likelihood. We derive theoretical guarantees on the accuracy of estimates from our model with minimal assumptions. In our empirical analysis, we study one of the most actively monitored streams of news articles in the financial system--the Dow Jones Newswires--and show that our supervised text model excels at extracting return-predictive signals in this context. Information in newswires is assimilated into prices with an ineffcient delay that is broadly consistent with limits-to-arbitrage (i.e., more severe for smaller and more volatile firms) yet can be exploited in a real-time trading strategy with reasonable turnover and net of transaction costs.},
	language = {en},
	number = {3389884},
	urldate = {2024-09-04},
	institution = {Social Science Research Network},
	author = {Ke, Zheng Tracy and Kelly, Bryan T. and Xiu, Dacheng},
	month = aug,
	year = {2021},
	doi = {10.2139/ssrn.3389884},
	keywords = {Machine Learning, Penalized Likelihood, Return Predictability, Screening, Sentiment Analysis, Text Mining, Topic Modeling},
}

@inproceedings{zmandar_comparative_2023,
	address = {Cham},
	title = {A {Comparative} {Study} of {Evaluation} {Metrics} for {Long}-{Document} {Financial} {Narrative} {Summarization} with {Transformers}},
	isbn = {978-3-031-35320-8},
	doi = {10.1007/978-3-031-35320-8_28},
	abstract = {There are more than 2,000 listed companies on the UK’s London Stock Exchange, divided into 11 sectors who are required to communicate their financial results at least twice in a single financial year. UK annual reports are very lengthy documents with around 80 pages on average. In this study, we aim to benchmark a variety of summarisation methods on a set of different pre-trained transformers with different extraction techniques. In addition, we considered multiple evaluation metrics in order to investigate their differing behaviour and applicability on a dataset from the Financial Narrative Summarisation (FNS 2020) shared task, which is composed of annual reports published by firms listed on the London Stock Exchange and their corresponding summaries. We hypothesise that some evaluation metrics do not reflect true summarisation ability and propose a novel BRUGEscore metric, as the harmonic mean of ROUGE-2 and BERTscore. Finally, we perform a statistical significance test on our results to verify whether they are statistically robust, alongside an adversarial analysis task with three different corruption methods.},
	language = {en},
	booktitle = {Natural {Language} {Processing} and {Information} {Systems}},
	publisher = {Springer Nature Switzerland},
	author = {Zmandar, Nadhem and El-Haj, Mahmoud and Rayson, Paul},
	editor = {Métais, Elisabeth and Meziane, Farid and Sugumaran, Vijayan and Manning, Warren and Reiff-Marganiec, Stephan},
	year = {2023},
	pages = {391--403},
}

@inproceedings{wu_attending_2019,
	title = {Attending to {Emotional} {Narratives}},
	url = {https://ieeexplore.ieee.org/document/8925497},
	doi = {10.1109/ACII.2019.8925497},
	abstract = {Attention mechanisms in deep neural networks have achieved excellent performance on sequence-prediction tasks. Here, we show that these recently-proposed attention-based mechanisms-in particular, the Transformer with its parallelizable self-attention layers, and the Memory Fusion Network with attention across modalities and time-also generalize well to multimodal time-series emotion recognition. Using a recently-introduced dataset of emotional autobiographical narratives, we adapt and apply these two attention mechanisms to predict emotional valence over time. Our models perform extremely well, in some cases reaching a performance comparable with human raters. We end with a discussion of the implications of attention mechanisms to affective computing.},
	urldate = {2024-11-01},
	booktitle = {2019 8th {International} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction} ({ACII})},
	author = {Wu, Zhengxuan and Zhang, Xiyu and Zhi-Xuan, Tan and Zaki, Jamil and Ong, Desmond C.},
	month = sep,
	year = {2019},
	note = {ISSN: 2156-8111},
	keywords = {Acoustics, Attention, Computational modeling, Deep Learning, Emotion recognition, Linguistics, Multimodal Emotion Recognition, Neural networks, Task analysis, Time-series Emotion Recognition, Visualization},
	pages = {648--654},
}

@misc{christ_modeling_2024,
	title = {Modeling {Emotional} {Trajectories} in {Written} {Stories} {Utilizing} {Transformers} and {Weakly}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/2406.02251},
	doi = {10.48550/arXiv.2406.02251},
	abstract = {Telling stories is an integral part of human communication which can evoke emotions and influence the affective states of the audience. Automatically modeling emotional trajectories in stories has thus attracted considerable scholarly interest. However, as most existing works have been limited to unsupervised dictionary-based approaches, there is no benchmark for this task. We address this gap by introducing continuous valence and arousal labels for an existing dataset of children's stories originally annotated with discrete emotion categories. We collect additional annotations for this data and map the categorical labels to the continuous valence and arousal space. For predicting the thus obtained emotionality signals, we fine-tune a DeBERTa model and improve upon this baseline via a weakly supervised learning approach. The best configuration achieves a Concordance Correlation Coefficient (CCC) of \$.8221\$ for valence and \$.7125\$ for arousal on the test set, demonstrating the efficacy of our proposed approach. A detailed analysis shows the extent to which the results vary depending on factors such as the author, the individual story, or the section within the story. In addition, we uncover the weaknesses of our approach by investigating examples that prove to be difficult to predict.},
	urldate = {2024-11-01},
	publisher = {arXiv},
	author = {Christ, Lukas and Amiriparian, Shahin and Milling, Manuel and Aslan, Ilhan and Schuller, Björn W.},
	month = jun,
	year = {2024},
	note = {arXiv:2406.02251},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{sornette_nurturing_2008,
	title = {Nurturing breakthroughs: lessons from complexity theory},
	volume = {3},
	issn = {1860-7128},
	shorttitle = {Nurturing breakthroughs},
	url = {https://doi.org/10.1007/s11403-008-0040-8},
	doi = {10.1007/s11403-008-0040-8},
	abstract = {A general theory of innovation and progress in human society is outlined, based on the combat between two opposite forces (conservatism/inertia and speculative herding “bubble” behavior). We contend that human affairs are characterized by ubiquitous “bubbles”, which involve huge risks which would not otherwise be taken using standard cost/benefit analysis. Bubbles result from self-reinforcing positive feedbacks. This leads to explore uncharted territories and niches whose rare successes lead to extraordinary discoveries and provide the base for the observed accelerating development of technology and of the economy. But the returns are very heterogeneous and very risky. In other words, bubbles, which are characteristic definitions of human activity, allow huge risks to get huge returns over large scales. We refer to and summarize a large bibliography covering our research efforts in the last decade, which present the relevant underlying mathematical tools and a few results involving positive feedbacks, emergence, heavy-tailed power laws, outliers/kings, the problem of predictability and the illusion of control.},
	language = {en},
	number = {2},
	urldate = {2024-11-01},
	journal = {Journal of Economic Interaction and Coordination},
	author = {Sornette, D.},
	month = dec,
	year = {2008},
	keywords = {Commercial Sale, Complexity Theory, Exogenous Shock, Human Affair, Social Unrest},
	pages = {165--181},
}

@article{gisler_innovation_2011,
	title = {Innovation as a social bubble: {The} example of the {Human} {Genome} {Project}},
	volume = {40},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00487333},
	shorttitle = {Innovation as a social bubble},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0048733311001004},
	doi = {10.1016/j.respol.2011.05.019},
	language = {en},
	number = {10},
	urldate = {2024-11-01},
	journal = {Research Policy},
	author = {Gisler, Monika and Sornette, Didier and Woodard, Ryan},
	month = dec,
	year = {2011},
	pages = {1412--1425},
}

@misc{sornette_financial_2014,
	title = {Financial bubbles: mechanisms and diagnostics},
	url = {http://arxiv.org/abs/1404.2140},
	abstract = {We define a financial bubble as a period of unsustainable growth, when the price of an asset increases ever more quickly, in a series of accelerating phases of corrections and rebounds. More technically, during a bubble phase, the price follows a faster-than-exponential power law growth process, often accompanied by log-periodic oscillations. This dynamic ends abruptly in a change of regime that may be a crash or a substantial correction. Because they leave such specific traces, bubbles may be recognised in advance, that is, before they burst. In this paper, we will explain the mechanism behind financial bubbles in an intuitive way. We will show how the log-periodic power law emerges spontaneously from the complex system that financial markets are, as a consequence of feedback mechanisms, hierarchical structure and specific trading dynamics and investment styles. We argue that the risk of a major correction, or even a crash, becomes substantial when a bubble develops towards maturity, and that it is therefore very important to find evidence of bubbles and to follow their development from as early a stage as possible. The tools that are explained in this paper actually serve that purpose. They are at the core of the Financial Crisis Observatory at the ETH Zurich, where tens of thousands of assets are monitored on a daily basis. This allow us to have a continuous overview of emerging bubbles in the global financial markets. The companion report available as part of the Notenstein white paper series (2014) with the title ``Financial bubbles: mechanism, diagnostic and state of the World (Feb. 2014)'' presents a practical application of the methodology outlines in this article and describes our view of the status concerning positive and negative bubbles in the financial markets, as of the end of January 2014.},
	urldate = {2024-11-01},
	publisher = {arXiv},
	author = {Sornette, Didier and Cauwels, Peter},
	month = apr,
	year = {2014},
	note = {Number: arXiv:1404.2140
arXiv:1404.2140},
	keywords = {Quantitative Finance - General Finance, Quantitative Finance - Risk Management},
}

@misc{mersha_semantic-driven_2024,
	title = {Semantic-{Driven} {Topic} {Modeling} {Using} {Transformer}-{Based} {Embeddings} and {Clustering} {Algorithms}},
	url = {http://arxiv.org/abs/2410.00134},
	doi = {10.48550/arXiv.2410.00134},
	abstract = {Topic modeling is a powerful technique to discover hidden topics and patterns within a collection of documents without prior knowledge. Traditional topic modeling and clustering-based techniques encounter challenges in capturing contextual semantic information. This study introduces an innovative end-to-end semantic-driven topic modeling technique for the topic extraction process, utilizing advanced word and document embeddings combined with a powerful clustering algorithm. This semantic-driven approach represents a significant advancement in topic modeling methodologies. It leverages contextual semantic information to extract coherent and meaningful topics. Specifically, our model generates document embeddings using pre-trained transformer-based language models, reduces the dimensions of the embeddings, clusters the embeddings based on semantic similarity, and generates coherent topics for each cluster. Compared to ChatGPT and traditional topic modeling algorithms, our model provides more coherent and meaningful topics.},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Mersha, Melkamu Abay and yigezu, Mesay Gemeda and Kalita, Jugal},
	month = sep,
	year = {2024},
	note = {arXiv:2410.00134},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{galli_topic_2025,
	title = {Topic {Modeling} for {Faster} {Literature} {Screening} {Using} {Transformer}-{Based} {Embeddings}},
	volume = {1},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {3042-5042},
	url = {https://www.mdpi.com/3042-5042/1/1/2},
	doi = {10.3390/metrics1010002},
	abstract = {Systematic reviews are a powerful tool to summarize the existing evidence in medical literature. However, identifying relevant articles is difficult, and this typically involves structured searches with keyword-based strategies, followed by the painstaking manual selection of relevant evidence. A.I. may help investigators, for example, through topic modeling, i.e., algorithms that can understand the content of a text. We applied BERTopic, a transformer-based topic-modeling algorithm, to two datasets consisting of 6137 and 5309 articles, respectively, used in recently published systematic reviews on peri-implantitis and bone regeneration. We extracted the title of each article, encoded it into embeddings, and input it into BERTopic, which then rapidly identified 14 and 22 topic clusters, respectively, and it automatically created labels describing the content of these groups based on their semantics. For both datasets, BERTopic uncovered a variable number of articles unrelated to the query, which accounted for up to 30\% of the dataset—achieving a sensitivity of up to 0.79 and a specificity of at least 0.99. These articles could have been discarded from the screening, reducing the workload of investigators. Our results suggest that adding a topic-modeling step to the screening process could potentially save working hours for researchers involved in systematic reviews of the literature.},
	language = {en},
	number = {1},
	urldate = {2024-10-30},
	journal = {Metrics},
	author = {Galli, Carlo and Cusano, Claudio and Meleti, Marco and Donos, Nikolaos and Calciolari, Elena},
	month = jun,
	year = {2025},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {embedding, systematic reviews, topic modeling},
	pages = {2},
}

@inproceedings{basmatkar_overview_2022,
	address = {Singapore},
	title = {An {Overview} of {Contextual} {Topic} {Modeling} {Using} {Bidirectional} {Encoder} {Representations} from {Transformers}},
	isbn = {9789811688621},
	doi = {10.1007/978-981-16-8862-1_32},
	abstract = {Topic modeling refers to a range of algorithms in natural language processing that gives us an insight into the ‘latent’ semantic topics or patterns in a collection of documents. These patterns of word co-occurrence are used to determine the hidden ‘topics’ which are present in the corpus. Topic modeling has been used successfully for information retrieval, classifying documents, summarizing them and for exploratory analysis of large corpora of texts. This survey studies various algorithms that have been used for topic modeling over time including TF-IDF, latent Dirichlet algorithm (LDA), clustering on sentence-level BERT embeddings and a newer hybrid approach of generating contextual topics using a combination of LDA and BERT vectors. This survey will analyze the advantages and limitations of these algorithms.},
	language = {en},
	booktitle = {Proceedings of {Third} {International} {Conference} on {Communication}, {Computing} and {Electronics} {Systems}},
	publisher = {Springer},
	author = {Basmatkar, Pranjali and Maurya, Mahesh},
	editor = {Bindhu, V. and Tavares, João Manuel R. S. and Du, Ke-Lin},
	year = {2022},
	keywords = {Contextual topic analysis, Latent Dirichlet algorithm, Natural language processing, Sentence-BERT, TF-IDF, Topic modeling},
	pages = {489--504},
}

@misc{alcoforado_zeroberto_2022,
	title = {{ZeroBERTo}: {Leveraging} {Zero}-{Shot} {Text} {Classification} by {Topic} {Modeling}},
	shorttitle = {{ZeroBERTo}},
	url = {http://arxiv.org/abs/2201.01337},
	doi = {10.48550/arXiv.2201.01337},
	abstract = {Traditional text classification approaches often require a good amount of labeled data, which is difficult to obtain, especially in restricted domains or less widespread languages. This lack of labeled data has led to the rise of low-resource methods, that assume low data availability in natural language processing. Among them, zero-shot learning stands out, which consists of learning a classifier without any previously labeled data. The best results reported with this approach use language models such as Transformers, but fall into two problems: high execution time and inability to handle long texts as input. This paper proposes a new model, ZeroBERTo, which leverages an unsupervised clustering step to obtain a compressed data representation before the classification task. We show that ZeroBERTo has better performance for long inputs and shorter execution time, outperforming XLM-R by about 12\% in the F1 score in the FolhaUOL dataset. Keywords: Low-Resource NLP, Unlabeled data, Zero-Shot Learning, Topic Modeling, Transformers.},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Alcoforado, Alexandre and Ferraz, Thomas Palmeira and Gerber, Rodrigo and Bustos, Enzo and Oliveira, André Seidel and Veloso, Bruno Miguel and Siqueira, Fabio Levy and Costa, Anna Helena Reali},
	month = jun,
	year = {2022},
	note = {arXiv:2201.01337},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{reuter_probabilistic_2024,
	title = {Probabilistic {Topic} {Modelling} with {Transformer} {Representations}},
	url = {http://arxiv.org/abs/2403.03737},
	doi = {10.48550/arXiv.2403.03737},
	abstract = {Topic modelling was mostly dominated by Bayesian graphical models during the last decade. With the rise of transformers in Natural Language Processing, however, several successful models that rely on straightforward clustering approaches in transformer-based embedding spaces have emerged and consolidated the notion of topics as clusters of embedding vectors. We propose the Transformer-Representation Neural Topic Model (TNTM), which combines the benefits of topic representations in transformer-based embedding spaces and probabilistic modelling. Therefore, this approach unifies the powerful and versatile notion of topics based on transformer embeddings with fully probabilistic modelling, as in models such as Latent Dirichlet Allocation (LDA). We utilize the variational autoencoder (VAE) framework for improved inference speed and modelling flexibility. Experimental results show that our proposed model achieves results on par with various state-of-the-art approaches in terms of embedding coherence while maintaining almost perfect topic diversity. The corresponding source code is available at https://github.com/ArikReuter/TNTM.},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Reuter, Arik and Thielmann, Anton and Weisser, Christoph and Säfken, Benjamin and Kneib, Thomas},
	month = mar,
	year = {2024},
	note = {arXiv:2403.03737},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@incollection{nguyen_hybrid_2023,
	address = {Cham},
	title = {Hybrid {Method} for {Short} {Text} {Topic} {Modeling}},
	volume = {1863},
	isbn = {978-3-031-42429-8 978-3-031-42430-4},
	url = {https://link.springer.com/10.1007/978-3-031-42430-4_13},
	abstract = {The rise in social media’s popularity has led to a signiﬁcant increase in user-generated content across various topics. Extracting information from these data can be valuable for diﬀerent domains, however, due to the nature of the vast volume it is not possible to extract information manually. Diﬀerent aspects of information extraction have been introduced in literature including identifying what topic is discussed in the text. The challenge becomes even bigger when the text is short, such as found in social media. Various methods for topic modeling have been proposed in the literature that could be generally categorized as unsupervised and supervised learning. However, unsupervised topic modeling methods have some shortcomings, such as semantic loss and poor explanation, and are sensitive to the choice of parameters, such as the number of topics. While supervised machine learning methods based on deep learning can achieve high accuracy they need data annotated by humans, which is time-consuming and costly. To overcome the above mentioned disadvantages this work proposes a hybrid topic modeling method that combines the advantages of both unsupervised and supervised methods. We built a hybrid model by combining Latent Dirichlet Allocation (LDA) and deep learning built on top of the Bidirectional Encoder Representations from the Transformers (BERT) model. LDA is used to identify the optimal number of topics and topic-relevant keywords where the only need for human input, with the aid of ChatGPT, is to identify associated topics based on topic-speciﬁc keywords. This annotation is used to train and ﬁne-tune the BERT model. In the experimental evaluation of posts related to climate change, we show that the proposed concept is applicable for predicting topics from short text without the need for lengthy and costly annotation.},
	language = {en},
	urldate = {2024-10-29},
	booktitle = {Recent {Challenges} in {Intelligent} {Information} and {Database} {Systems}},
	publisher = {Springer Nature Switzerland},
	author = {Chen, Jinyuan and Stantic, Bela},
	editor = {Nguyen, Ngoc Thanh and Boonsang, Siridech and Fujita, Hamido and Hnatkowska, Bogumiła and Hong, Tzung-Pei and Pasupa, Kitsuchart and Selamat, Ali},
	year = {2023},
	doi = {10.1007/978-3-031-42430-4_13},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {157--168},
}

@article{ha_topic_nodate,
	title = {Topic classification of electric vehicle consumer experiences with transformer-based deep learning},
	abstract = {The transportation sector is a major contributor to greenhouse gas (GHG) emissions and is a driver of adverse health effects globally. Increasingly, government policies have promoted the adoption of electric vehicles (EVs) as a solution to mitigate GHG emissions. However, government analysts have failed to fully utilize consumer data in decisions related to charging infrastructure. This is because a large share of EV data is unstructured text, which presents challenges for data discovery. In this article, we deploy advances in transformer-based deep learning to discover topics of attention in a nationally representative sample of user reviews. We report classiﬁcation accuracies greater than 91\% (F1 scores of 0.83), outperforming previously leading algorithms in this domain. We describe applications of these deep learning models for public policy analysis and large-scale implementation. This capability can boost intelligence for the EV charging market, which is expected to grow to US\$27.6 billion by 2027.},
	language = {en},
	journal = {OPEN ACCESS},
	author = {Ha, Sooji},
}

@misc{sia_tired_2020,
	title = {Tired of {Topic} {Models}? {Clusters} of {Pretrained} {Word} {Embeddings} {Make} for {Fast} and {Good} {Topics} too!},
	shorttitle = {Tired of {Topic} {Models}?},
	url = {http://arxiv.org/abs/2004.14914},
	abstract = {Topic models are a useful analysis tool to uncover the underlying themes within document collections. The dominant approach is to use probabilistic topic models that posit a generative story, but in this paper we propose an alternative way to obtain topics: clustering pre-trained word embeddings while incorporating document information for weighted clustering and reranking top words. We provide benchmarks for the combination of different word embeddings and clustering algorithms, and analyse their performance under dimensionality reduction with PCA. The best performing combination for our approach performs as well as classical topic models, but with lower runtime and computational complexity.},
	urldate = {2024-10-29},
	publisher = {arXiv},
	author = {Sia, Suzanna and Dalmia, Ayush and Mielke, Sabrina J.},
	month = oct,
	year = {2020},
	note = {arXiv:2004.14914},
	keywords = {Computer Science - Computation and Language},
}

@article{uthirapathy_topic_nodate,
	title = {Topic {Modelling} and {Opinion} {Analysis} {On} {Climate} {Change} {Twitter} {Data} {Using} {LDA} {And} {BERT} {Model}.},
	abstract = {Nowadays, Climate change is an important environmental factor that affects every living thing on the earth. It is very essential to study the public perceptions regarding the disaster events frequently happening due to climate change. In today's digital era individuals are using social network platforms namely Twitter, Facebook, and Weibo now and then to express their views about any events. In this paper, the climate change Twitter data set was considered for analyzing the topics and the opinions discussed by the public regarding climate change. The Latent Dirichlet Allocation(LDA) method was used to list out the various topics present in the data set and the Bidirectional Encoder Representation from Transformers(BERT uncased) is an efficient deep learning technique used to classify the sentiments present in the data set. Here the sentiments were labelled as pro news, support, neutral and anti. The performance of the proposed topic modelling and sentiment classification model was compared using the precision, recall, and accuracy measures. The BERT uncased model with has shown the best results such as precision of 91.35\%, recall of 89.65\%, and accuracy of 93.50\% compared to other methods.},
	language = {en},
	author = {Uthirapathy, Samson Ebenezar and Sandanam, Domnic},
}

@article{gurusamy_hybrid_2023,
	title = {A hybrid approach for text summarization using semantic latent {Dirichlet} allocation and sentence concept mapping with transformer},
	volume = {13},
	doi = {10.11591/ijece.v13i6.pp6663-6672},
	abstract = {Automatic text summarization generates a summary that contains sentences reflecting the essential and relevant information of the original documents. Extractive summarization requires semantic understanding, while abstractive summarization requires a better intermediate text representation. This paper proposes a hybrid approach for generating text summaries that combine extractive and abstractive methods. To improve the semantic understanding of the model, we propose two novel extractive methods: semantic latent Dirichlet allocation (semantic LDA) and sentence concept mapping. We then generate an intermediate summary by applying our proposed sentence ranking algorithm over the sentence concept mapping. This intermediate summary is input to a transformer-based abstractive model fine-tuned with a multi-head attention mechanism. Our experimental results demonstrate that the proposed hybrid model generates coherent summaries using the intermediate extractive summary covering semantics. As we increase the concepts and number of words in the summary the rouge scores are improved for precision and F1 scores in our proposed model. This is an open access article under the CC BY-SA license.},
	journal = {International Journal of Electrical and Computer Engineering (IJECE)},
	author = {Gurusamy, Bharathi Mohan and Rangarajan, Prasanna Kumar and Srinivasan, Partha},
	month = dec,
	year = {2023},
	pages = {6663--6672},
}

@misc{zaratiana_gliner_2023,
	title = {{GLiNER}: {Generalist} {Model} for {Named} {Entity} {Recognition} using {Bidirectional} {Transformer}},
	shorttitle = {{GLiNER}},
	url = {http://arxiv.org/abs/2311.08526},
	abstract = {Named Entity Recognition (NER) is essential in various Natural Language Processing (NLP) applications. Traditional NER models are effective but limited to a set of predefined entity types. In contrast, Large Language Models (LLMs) can extract arbitrary entities through natural language instructions, offering greater flexibility. However, their size and cost, particularly for those accessed via APIs like ChatGPT, make them impractical in resource-limited scenarios. In this paper, we introduce a compact NER model trained to identify any type of entity. Leveraging a bidirectional transformer encoder, our model, GLiNER, facilitates parallel entity extraction, an advantage over the slow sequential token generation of LLMs. Through comprehensive testing, GLiNER demonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs in zero-shot evaluations on various NER benchmarks.},
	urldate = {2024-10-29},
	publisher = {arXiv},
	author = {Zaratiana, Urchade and Tomeh, Nadi and Holat, Pierre and Charnois, Thierry},
	month = nov,
	year = {2023},
	note = {arXiv:2311.08526},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{marcinczuk_transformer-based_2024,
	title = {Transformer-based {Named} {Entity} {Recognition} with {Combined} {Data} {Representation}},
	url = {http://arxiv.org/abs/2406.17474},
	abstract = {This study examines transformer-based models and their effectiveness in named entity recognition tasks. The study investigates data representation strategies, including single, merged, and context, which respectively use one sentence, multiple sentences, and sentences joined with attention to context per vector. Analysis shows that training models with a single strategy may lead to poor performance on different data representations. To address this limitation, the study proposes a combined training procedure that utilizes all three strategies to improve model stability and adaptability. The results of this approach are presented and discussed for four languages (English, Polish, Czech, and German) across various datasets, demonstrating the effectiveness of the combined strategy.},
	urldate = {2024-10-29},
	publisher = {arXiv},
	author = {Marcińczuk, Michał},
	month = jun,
	year = {2024},
	note = {arXiv:2406.17474},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{yan_tener_2019,
	title = {{TENER}: {Adapting} {Transformer} {Encoder} for {Named} {Entity} {Recognition}},
	shorttitle = {{TENER}},
	url = {http://arxiv.org/abs/1911.04474},
	abstract = {The Bidirectional long short-term memory networks (BiLSTM) have been widely used as an encoder in models solving the named entity recognition (NER) task. Recently, the Transformer is broadly adopted in various Natural Language Processing (NLP) tasks owing to its parallelism and advantageous performance. Nevertheless, the performance of the Transformer in NER is not as good as it is in other NLP tasks. In this paper, we propose TENER, a NER architecture adopting adapted Transformer Encoder to model the character-level features and word-level features. By incorporating the direction and relative distance aware attention and the un-scaled attention, we prove the Transformer-like encoder is just as effective for NER as other NLP tasks.},
	urldate = {2024-10-29},
	publisher = {arXiv},
	author = {Yan, Hang and Deng, Bocao and Li, Xiaonan and Qiu, Xipeng},
	month = dec,
	year = {2019},
	note = {arXiv:1911.04474},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{shishehgarkhaneh_transformer-based_2024,
	title = {Transformer-{Based} {Named} {Entity} {Recognition} in {Construction} {Supply} {Chain} {Risk} {Management} in {Australia}},
	volume = {12},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10472528/?arnumber=10472528},
	doi = {10.1109/ACCESS.2024.3377232},
	abstract = {In the Australian construction industry, effective supply chain risk management (SCRM) is critical due to its complex networks and susceptibility to various risks. This study explores the application of transformer models like BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA for Named Entity Recognition (NER) in this context. Utilizing these models, we analyzed news articles to identify and classify entities related to supply chain risks, providing insights into the vulnerabilities within this sector. Among the evaluated models, RoBERTa achieved the highest average F1 score of 0.8580, demonstrating its superior balance in precision and recall for NER in the Australian construction supply chain context. Our findings highlight the potential of NLP-driven solutions to revolutionize SCRM, particularly in geo-specific settings.},
	urldate = {2024-10-29},
	journal = {IEEE Access},
	author = {Shishehgarkhaneh, Milad Baghalzadeh and Moehler, Robert C. and Fang, Yihai and Hijazi, Amer A. and Aboutorab, Hamed},
	year = {2024},
	note = {Conference Name: IEEE Access},
	keywords = {BERT, Biological system modeling, Construction industry, Construction supply chain risk management, Data mining, Data models, Geophysical measurements, Natural language processing, Risk management, Supply chain management, Transformers, named entity recognition, natural language processing, transformers},
	pages = {41829--41851},
}

@inproceedings{ushio_t-ner_2021,
	address = {Online},
	title = {T-{NER}: {An} {All}-{Round} {Python} {Library} for {Transformer}-based {Named} {Entity} {Recognition}},
	shorttitle = {T-{NER}},
	url = {https://aclanthology.org/2021.eacl-demos.7},
	doi = {10.18653/v1/2021.eacl-demos.7},
	abstract = {Language model (LM) pretraining has led to consistent improvements in many NLP downstream tasks, including named entity recognition (NER). In this paper, we present T-NER (Transformer-based Named Entity Recognition), a Python library for NER LM finetuning. In addition to its practical utility, T-NER facilitates the study and investigation of the cross-domain and cross-lingual generalization ability of LMs finetuned on NER. Our library also provides a web app where users can get model predictions interactively for arbitrary text, which facilitates qualitative model evaluation for non-expert programmers. We show the potential of the library by compiling nine public NER datasets into a unified format and evaluating the cross-domain and cross- lingual performance across the datasets. The results from our initial experiments show that in-domain performance is generally competitive across datasets. However, cross-domain generalization is challenging even with a large pretrained LM, which has nevertheless capacity to learn domain-specific features if fine- tuned on a combined dataset. To facilitate future research, we also release all our LM checkpoints via the Hugging Face model hub.},
	urldate = {2024-10-29},
	booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Ushio, Asahi and Camacho-Collados, Jose},
	editor = {Gkatzia, Dimitra and Seddah, Djamé},
	month = apr,
	year = {2021},
	pages = {53--62},
}

@article{berragan_transformer_2023,
	title = {Transformer based named entity recognition for place name extraction from unstructured text},
	volume = {37},
	issn = {1365-8816},
	url = {https://doi.org/10.1080/13658816.2022.2133125},
	doi = {10.1080/13658816.2022.2133125},
	abstract = {Place names embedded in online natural language text present a useful source of geographic information. Despite this, many methods for the extraction of place names from text use pre-trained models that were not explicitly designed for this task. Our paper builds five custom-built Named Entity Recognition (NER) models and evaluates them against three popular pre-built models for place name extraction. The models are evaluated using a set of manually annotated Wikipedia articles with reference to the F1 score metric. Our best performing model achieves an F1 score of 0.939 compared with 0.730 for the best performing pre-built model. Our model is then used to extract all place names from Wikipedia articles in Great Britain, demonstrating the ability to more accurately capture unknown place names from volunteered sources of online geographic information.},
	number = {4},
	urldate = {2024-10-29},
	journal = {International Journal of Geographical Information Science},
	author = {Berragan, Cillian and Singleton, Alex and Calafiore, Alessia and Morley, Jeremy},
	month = apr,
	year = {2023},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/13658816.2022.2133125},
	keywords = {Named entity recognition, natural language processing, place name extraction, volunteered geographic information},
	pages = {747--766},
}

@article{martinez-beneito_bayesian_2008,
	title = {Bayesian {Markov} switching models for the early detection of influenza epidemics},
	volume = {27},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3320},
	doi = {10.1002/sim.3320},
	abstract = {The early detection of outbreaks of diseases is one of the most challenging objectives of epidemiological surveillance systems. In this paper, a Markov switching model is introduced to determine the epidemic and non-epidemic periods from influenza surveillance data: the process of differenced incidence rates is modelled either with a first-order autoregressive process or with a Gaussian white-noise process depending on whether the system is in an epidemic or in a non-epidemic phase. The transition between phases of the disease is modelled as a Markovian process. Bayesian inference is carried out on the former model to detect influenza epidemics at the very moment of their onset. Moreover, the proposal provides the probability of being in an epidemic state at any given moment. In order to validate the methodology, a comparison of its performance with other alternatives has been made using influenza illness data obtained from the Sanitary Sentinel Network of the Comunitat Valenciana, one of the 17 autonomous regions in Spain. Copyright © 2008 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {22},
	urldate = {2024-10-27},
	journal = {Statistics in Medicine},
	author = {Martínez-Beneito, Miguel A. and Conesa, David and López-Quílez, Antonio and López-Maside, Aurora},
	year = {2008},
	keywords = {autoregressive modelling, epidemiological surveillance, hidden Markov models},
	pages = {4455--4468},
}

@article{blei_latent_2003,
	title = {Latent {Dirichlet} {Allocation}},
	volume = {3},
	abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a ﬁnite mixture over an underlying set of topics. Each topic is, in turn, modeled as an inﬁnite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efﬁcient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classiﬁcation, and collaborative ﬁltering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
	language = {en},
	number = {2003},
	journal = {Journal of Machine Learning Research},
	author = {Blei, David M},
	year = {2003},
	pages = {993--1022},
}

@article{jarrow_asset_2010,
	title = {Asset price bubbles in incomplete markets},
	volume = {20},
	copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
	issn = {09601627, 14679965},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9965.2010.00394.x},
	doi = {10.1111/j.1467-9965.2010.00394.x},
	abstract = {This paper studies asset price bubbles in a continuous time model using the local martingale framework. Providing careful deﬁnitions of the asset’s market and fundamental price, we characterize all possible price bubbles in an incomplete market satisfying the “no free lunch with vanishing risk (NFLVR)” and “no dominance” assumptions. We show that the two leading models for bubbles as either charges or as strict local martingales, respectively, are equivalent. We propose a new theory for bubble birth that involves a nontrivial modiﬁcation of the classical martingale pricing framework. This modiﬁcation involves the market exhibiting different local martingale measures across time—a possibility not previously explored within the classical theory. Finally, we investigate the pricing of derivative securities in the presence of asset price bubbles, and we show that: (i) European put options can have no bubbles; (ii) European call options and discounted forward prices have bubbles whose magnitudes are related to the asset’s price bubble; (iii) with no dividends, American call options are not exercised early; (iv) European put-call parity in market prices must always hold, regardless of bubbles; and (v) futures price bubbles can exist and they are independent of the underlying asset’s price bubble. Many of these results stand in contrast to those of the classical theory. We propose, but do not implement, some new tests for the existence of asset price bubbles using derivative securities.},
	language = {en},
	number = {2},
	urldate = {2024-09-04},
	journal = {Mathematical Finance},
	author = {Jarrow, Robert A. and Protter, Philip and Shimbo, Kazuhiro},
	month = apr,
	year = {2010},
	pages = {145--185},
}

@article{phillips_testing_2015,
	title = {Testing for multiple bubbles: {Historical} episodes of exuberance and collapse in the {S}\&{P} 500},
	volume = {56},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	issn = {0020-6598, 1468-2354},
	shorttitle = {{TESTING} {FOR} {MULTIPLE} {BUBBLES}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/iere.12132},
	doi = {10.1111/iere.12132},
	abstract = {Recent work on econometric detection mechanisms has shown the effectiveness of recursive procedures in identifying and dating financial bubbles in real time. These procedures are useful as warning alerts in surveillance strategies conducted by central banks and fiscal regulators with real‐time data. Use of these methods over long historical periods presents a more serious econometric challenge due to the complexity of the nonlinear structure and break mechanisms that are inherent in multiple‐bubble phenomena within the same sample period. To meet this challenge, this article develops a new recursive flexible window method that is better suited for practical implementation with long historical time series. The method is a generalized version of the sup augmented Dickey–Fuller (ADF) test of Phillips et al. (“Explosive behavior in the 1990s NASDAQ: When did exuberance escalate asset values?”
              International Economic Review
              52 (2011), 201–26; PWY) and delivers a consistent real‐time date‐stamping strategy for the origination and termination of multiple bubbles. Simulations show that the test significantly improves discriminatory power and leads to distinct power gains when multiple bubbles occur. An empirical application of the methodology is conducted on S\&P 500 stock market data over a long historical period from January 1871 to December 2010. The new approach successfully identifies the well‐known historical episodes of exuberance and collapses over this period, whereas the strategy of PWY and a related cumulative sum (CUSUM) dating procedure locate far fewer episodes in the same sample range.},
	language = {en},
	number = {4},
	urldate = {2024-09-04},
	journal = {International Economic Review},
	author = {Phillips, Peter C. B. and Shi, Shuping and Yu, Jun},
	month = nov,
	year = {2015},
	pages = {1043--1078},
}

@article{kermack_contribution_1927,
	title = {A {Contribution} to the {Mathematical} {Theory} of {Epidemics}},
	volume = {115},
	url = {http://www.jstor.org/stable/94815},
	abstract = {The various possible mechanisms for the production of ammonia in a nitrogen hydrogen mixture by means of thermions have been investigated in detail. It is shown that synthesis can occur due to the following reactionsN2 + H at the surface of platinum or nickel. N2 + H' in the bulk at 13 volts. The following molecular species are shown to be chemically reactiveN2+ in the bulk at 17 volts, N+ in the bulk at 23 volts, and possible modes of mechanism involving N2' and H' are elaborated. Our thanks are due to Prof. T. M. Lowry, F.R.S., who communicated this paper, and to Messrs. Brunner Mond and Co., for providing a grant to defray part of the cost of the apparatus employed.},
	language = {en},
	number = {772},
	journal = {Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character},
	author = {Kermack, W. O. and McKendrick, A. G.},
	year = {1927},
	pages = {700--721},
}

@article{dalio_principles_2018,
	title = {Principles for {Navigating} {Big} {Debt} {Crises}},
	url = {https://www.bridgewater.com/big-debt-crises/principles-for-navigating-big-debt-crises-by-ray-dalio.pdf},
	language = {en},
	journal = {Principles for Navigating Big Debt Crises},
	author = {Dalio, Ray},
	year = {2018},
}

@article{du_financial_2024,
	title = {Financial {Sentiment} {Analysis}: {Techniques} and {Applications}},
	volume = {56},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Financial {Sentiment} {Analysis}},
	url = {https://dl.acm.org/doi/10.1145/3649451},
	doi = {10.1145/3649451},
	abstract = {Financial Sentiment Analysis (FSA) is an important domain application of sentiment analysis that has gained increasing attention in the past decade. FSA research falls into two main streams. The
              first stream
              focuses on defining tasks and developing techniques for FSA, and its main objective is to improve the performances of various FSA tasks by advancing methods and using/curating human-annotated datasets. The
              second stream
              of research focuses on using financial sentiment, implicitly or explicitly, for downstream applications on financial markets, which has received more research efforts. The main objective is to discover appropriate market applications for existing techniques. More specifically, the application of FSA mainly includes hypothesis testing and predictive modeling in financial markets. This survey conducts a comprehensive review of FSA research in both the technique and application areas and proposes several frameworks to help understand the two areas’ interactive relationship. This article defines a clearer scope for FSA studies and conceptualizes the FSA-investor sentiment-market sentiment relationship. Major findings, challenges, and future research directions for both FSA techniques and applications have also been summarized and discussed.},
	language = {en},
	number = {9},
	urldate = {2024-10-23},
	journal = {ACM Computing Surveys},
	author = {Du, Kelvin and Xing, Frank and Mao, Rui and Cambria, Erik},
	month = oct,
	year = {2024},
	pages = {1--42},
}

@article{cambria_jumping_2014,
	title = {Jumping {NLP} {Curves}: {A} {Review} of {Natural} {Language} {Processing} {Research} [{Review} {Article}]},
	volume = {9},
	issn = {1556-6048},
	shorttitle = {Jumping {NLP} {Curves}},
	url = {https://ieeexplore.ieee.org/document/6786458},
	doi = {10.1109/MCI.2014.2307227},
	abstract = {Natural language processing (NLP) is a theory-motivated range of computational techniques for the automatic analysis and representation of human language. NLP research has evolved from the era of punch cards and batch processing (in which the analysis of a sentence could take up to 7 minutes) to the era of Google and the likes of it (in which millions of webpages can be processed in less than a second). This review paper draws on recent developments in NLP research to look at the past, present, and future of NLP technology in a new light. Borrowing the paradigm of `jumping curves' from the field of business management and marketing prediction, this survey article reinterprets the evolution of NLP research as the intersection of three overlapping curves-namely Syntactics, Semantics, and Pragmatics Curveswhich will eventually lead NLP research to evolve into natural language understanding.},
	number = {2},
	urldate = {2024-10-23},
	journal = {IEEE Computational Intelligence Magazine},
	author = {Cambria, Erik and White, Bebo},
	month = may,
	year = {2014},
	note = {Conference Name: IEEE Computational Intelligence Magazine},
	keywords = {Knowledge based systems, Natural language processing, Pragmatics, Semantics, Syntactics},
	pages = {48--57},
}

@article{olaniyan_applied_nodate,
	title = {Applied {Natural} {Language} {Processing} and {Machine} {Learning} in {Algorithmic} {Trading}},
	language = {en},
	author = {Olaniyan, Rapheal},
}

@misc{yang_measuring_2023,
	title = {Measuring {Consistency} in {Text}-based {Financial} {Forecasting} {Models}},
	url = {http://arxiv.org/abs/2305.08524},
	abstract = {Financial forecasting has been an important and active area of machine learning research, as even the most modest advantage in predictive accuracy can be parlayed into significant financial gains. Recent advances in natural language processing (NLP) bring the opportunity to leverage textual data, such as earnings reports of publicly traded companies, to predict the return rate for an asset. However, when dealing with such a sensitive task, the consistency of models -- their invariance under meaning-preserving alternations in input -- is a crucial property for building user trust. Despite this, current financial forecasting methods do not consider consistency. To address this problem, we propose FinTrust, an evaluation tool that assesses logical consistency in financial text. Using FinTrust, we show that the consistency of state-of-the-art NLP models for financial forecasting is poor. Our analysis of the performance degradation caused by meaning-preserving alternations suggests that current text-based methods are not suitable for robustly predicting market information. All resources are available at https://github.com/yingpengma/fintrust.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Yang, Linyi and Ma, Yingpeng and Zhang, Yue},
	month = jun,
	year = {2023},
	note = {arXiv:2305.08524},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Economics - General Economics, Quantitative Finance - Economics},
}

@article{hirano_stbm_2022,
	title = {{STBM}+: {Advanced} {Stochastic} {Trading} {Behavior} {Model} for {Financial} {Markets} using {Residual} {Blocks} or {Transformers}},
	volume = {40},
	issn = {1882-7055},
	shorttitle = {{STBM}+},
	url = {https://doi.org/10.1007/s00354-021-00145-z},
	doi = {10.1007/s00354-021-00145-z},
	abstract = {This study proposes a new model to reverse engineer and predict traders’ behavior for the financial market. This trial is essential to build a more reliable simulation because the reliability of models is a fundamental issue in the increasing use of simulations. Thus, we tried to build a behavior model of financial traders through the traders’ future action predicting using the actual order data. This study focused on one category of traders employing high-frequency market-making (HFT-MM) trading in financial markets. In our experiments, we build models for predicting the next actions of each trader and evaluate how correctly these models successfully predict trades’ future actions in the next one minutes. Although the task is the same as previous work, this study newly used an architecture based on the transformer and residual block, and a loss function based on the Kullback-Leibler divergence (KLD). In addition, we established a new evaluation metric. Consequently, our new models, both transformer-based and residual-block-based models, outperformed the previous model based on LSTM in terms of both old and new evaluation metrics. These results suggested that transformer and residual block are effective in capturing traders’ behaviors. In addition, the KLD-based new loss function also showed better results than the previous MSE-based loss function. We assumed it is because the KLD-based loss function has a better fitting to this task due to its mathematical form.},
	language = {en},
	number = {1},
	urldate = {2024-10-23},
	journal = {New Generation Computing},
	author = {Hirano, Masanori and Izumi, Kiyoshi and Sakaji, Hiroki},
	month = apr,
	year = {2022},
	keywords = {91C99, 91G15, Artificial Intelligence, Behavior Modeling, Financial Market, High-frequency Trading, Trader Model},
	pages = {7--24},
}

@article{mandal_tweets_2023,
	title = {Tweets {Topic} {Classification} and {Sentiment} {Analysis} {Based} on {Transformer}-{Based} {Language} {Models}},
	volume = {10},
	issn = {2196-8888},
	url = {https://www.worldscientific.com/doi/10.1142/S2196888822500269},
	doi = {10.1142/S2196888822500269},
	abstract = {People provide information on their thoughts, perceptions, and activities through a wide range of channels, including social media. The wide acceptance of social media results in vast volume of valuable data, in variety of format as well as veracity. Analysis of such ‘big data’ allows organizations and analysts to make better and faster decisions. However, this data had to be quantified and information has to be extracted, which can be very challenging because of possible data ambiguity and complexity. To address information extraction, many analytic techniques, such as text mining, machine learning, predictive analytics, and diverse natural language processing, have been proposed in the literature. Recent advances in Natural Language Understanding-based techniques more specifically transformer-based architectures can solve sequence-to-sequence modeling tasks while handling long-range dependencies efficiently. In this work, we applied transformer-based sequence modeling on short texts’ topic classification and sentiment analysis from user-posted tweets. Applicability of models is investigated on posts from the Great Barrier Reef tweet dataset and obtained findings are encouraging providing insight that can be valuable for researchers working on classification of large datasets as well as large number of target classes.},
	number = {02},
	urldate = {2024-10-23},
	journal = {Vietnam Journal of Computer Science},
	author = {Mandal, Ranju and Chen, Jinyan and Becken, Susanne and Stantic, Bela},
	month = may,
	year = {2023},
	note = {Publisher: World Scientific Publishing Co.},
	keywords = {Transformer, deep learning, natural language processing, target classification, topic classification},
	pages = {117--134},
}

@misc{debelak_embeddings_2024,
	title = {From {Embeddings} to {Explainability}: {A} {Tutorial} on {Transformer}-{Based} {Text} {Analysis} for {Social} and {Behavioral} {Scientists}},
	shorttitle = {From {Embeddings} to {Explainability}},
	url = {https://osf.io/bc56a},
	doi = {10.31234/osf.io/bc56a},
	abstract = {Large language models and their use for text analysis have had a significant impact on psychology and the social and behavioral sciences in general. Key applications include the analysis of texts, such as social media posts, to infer psychological characteristics, as well as survey and interview analysis. In this tutorial paper, we demonstrate the use of the Python-based natural language processing software package transformers (and related modules from the Hugging Face Ecosystem) that allow for the automated classification of text inputs in a practical exercise. In doing so, we rely on pretrained transformer models which can be fine-tuned to a specific task and domain. The first proposed application of this model class is to use it as a feature extractor, allowing for the transformation of written text into real-valued numerical vectors (called "embeddings") that capture a text's semantic meaning. These vectors can, in turn, be used as input for a subsequent machine-learning model. The second presented application of transformer models is the end-to-end training (so-called "fine-tuning") of the model. This results in a direct prediction of the label within the same model that directly maps the text to the embeddings. While in the second case, results are usually better and training works more seamlessly, the model itself is often not directly interpretable. We showcase an alleviation of this issue via the application of post-hoc interpretability methods by calculating SHAP values and applying local interpretable model-agnostic explanations (LIME) in an attempt to explain the model's inner workings.},
	language = {en-us},
	urldate = {2024-10-23},
	publisher = {OSF},
	author = {Debelak, Rudolf and Koch, Timo and Aßenmacher, Matthias and Stachl, Clemens},
	month = may,
	year = {2024},
	keywords = {Automated Text Scoring, Deep Learning, Machine Learning, Natural Language Processing, Python},
}

@misc{li_findkg_2024,
	title = {{FinDKG}: {Dynamic} {Knowledge} {Graphs} with {Large} {Language} {Models} for {Detecting} {Global} {Trends} in {Financial} {Markets}},
	shorttitle = {{FinDKG}},
	url = {http://arxiv.org/abs/2407.10909},
	abstract = {Dynamic knowledge graphs (DKGs) are popular structures to express different types of connections between objects over time. They can also serve as an efficient mathematical tool to represent information extracted from complex unstructured data sources, such as text or images. Within financial applications, DKGs could be used to detect trends for strategic thematic investing, based on information obtained from financial news articles. In this work, we explore the properties of large language models (LLMs) as dynamic knowledge graph generators, proposing a novel open-source fine-tuned LLM for this purpose, called the Integrated Contextual Knowledge Graph Generator (ICKG). We use ICKG to produce a novel open-source DKG from a corpus of financial news articles, called FinDKG, and we propose an attention-based GNN architecture for analysing it, called KGTransformer. We test the performance of the proposed model on benchmark datasets and FinDKG, demonstrating superior performance on link prediction tasks. Additionally, we evaluate the performance of the KGTransformer on FinDKG for thematic investing, showing it can outperform existing thematic ETFs.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Li, Xiaohui Victor and Passino, Francesco Sanna},
	month = oct,
	year = {2024},
	note = {arXiv:2407.10909},
	keywords = {Quantitative Finance - Computational Finance},
}

@article{xiao_automatic_2024,
	title = {An {Automatic} {Sentiment} {Analysis} {Method} for {Short} {Texts} {Based} on {Transformer}-{BERT} {Hybrid} {Model}},
	volume = {12},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10580959/?arnumber=10580959},
	doi = {10.1109/ACCESS.2024.3422268},
	abstract = {Sentiment analysis towards short texts is always facing challenges, because short texts only contain limited semantic characteristics. As a result, this paper constructs a specific large language structure to deal with this issue. In all, a novel automatic sentiment analysis method for short texts based on Transformer-BERT hybrid model is proposed by this paper. Firstly, BERT structure is utilized to extract word vectors, and is integrated with topic vectors to improve textual feature expression ability. Then, the fused word vectors are input into a Bidirectional Gated Recurrent Unit (Bi-GRU) structure to learn contextual features. In this part, a Transformer structure is applied behind the Bi-GRU and combined with the previous module to output sentiment analysis results. In addition, Accuracy, Precision, Recall and F1 indexes were collected from real-world Twitter datasets and shopping data to evaluate the performance of the proposed method. The experimental results show that the method performs well in many indexes. Compared with traditional method, it has achieved remarkable performance improvement, and this method achieves higher accuracy and efficiency in sentiment analysis of short texts, and has good generalization ability.},
	urldate = {2024-10-23},
	journal = {IEEE Access},
	author = {Xiao, Haiyan and Luo, Linghua},
	year = {2024},
	note = {Conference Name: IEEE Access},
	keywords = {Analytical models, Bidirectional control, Encoding, Large language models, Semantics, Sentiment analysis, Text processing, Transformers, Vectors, large language model, semantic comprehension, short texts},
	pages = {93305--93317},
}

@misc{xiao_introduction_2023,
	title = {Introduction to {Transformers}: an {NLP} {Perspective}},
	shorttitle = {Introduction to {Transformers}},
	url = {http://arxiv.org/abs/2311.17633},
	abstract = {Transformers have dominated empirical machine learning models of natural language processing. In this paper, we introduce basic concepts of Transformers and present key techniques that form the recent advances of these models. This includes a description of the standard Transformer architecture, a series of model refinements, and common applications. Given that Transformers and related deep learning techniques might be evolving in ways we have never seen, we cannot dive into all the model details or cover all the technical areas. Instead, we focus on just those concepts that are helpful for gaining a good understanding of Transformers and their variants. We also summarize the key ideas that impact this field, thereby yielding some insights into the strengths and limitations of these models.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Xiao, Tong and Zhu, Jingbo},
	month = nov,
	year = {2023},
	note = {arXiv:2311.17633},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{rau_role_2022,
	title = {The {Role} of {Complex} {NLP} in {Transformers} for {Text} {Ranking}?},
	url = {http://arxiv.org/abs/2207.02522},
	abstract = {Even though term-based methods such as BM25 provide strong baselines in ranking, under certain conditions they are dominated by large pre-trained masked language models (MLMs) such as BERT. To date, the source of their effectiveness remains unclear. Is it their ability to truly understand the meaning through modeling syntactic aspects? We answer this by manipulating the input order and position information in a way that destroys the natural sequence order of query and passage and shows that the model still achieves comparable performance. Overall, our results highlight that syntactic aspects do not play a critical role in the effectiveness of re-ranking with BERT. We point to other mechanisms such as query-passage cross-attention and richer embeddings that capture word meanings based on aggregated context regardless of the word order for being the main attributions for its superior performance.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Rau, David and Kamps, Jaap},
	month = jul,
	year = {2022},
	note = {arXiv:2207.02522},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{tabinda_kokab_transformer-based_2022,
	title = {Transformer-based deep learning models for the sentiment analysis of social media data},
	volume = {14},
	issn = {25900056},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2590005622000224},
	doi = {10.1016/j.array.2022.100157},
	abstract = {Sentiment analysis (SA) is a widely used contextual mining technique for extracting useful and subjective information from text-based data. It applies on Natural Language Processing (NLP), text analysis, biometrics, and computational linguistics to identify, analyse, and extract responses, states, or emotions from the data. The features analysis technique plays a significant role in the development and improvement of a SA model. Recently, GloVe and Word2vec embedding models have been widely used for feature extractions. However, they overlook sentimental and contextual information of the text and need a large corpus of text data for training and generating exact vectors. These techniques generate vectors for just those words that are included in their vocabulary and ignore Out of Vocabulary Words (OOV), which can lead to information loss. Another challenge for the classification of sentiments is that of the lack of readily available annotated data. Sometimes, there is a contradiction between the review and their label that may cause misclassification. The aim of this paper is to propose a generalized SA model that can handle noisy data, OOV words, sentimental and contextual loss of reviews data. In this research, an effective Bi-directional Encoder Representation from Transformers (BERT) based Convolution Bi-directional Recurrent Neural Network (CBRNN) model is proposed with for exploring the syntactic and semantic information along with the sentimental and contextual analysis of the data. Initially, the zero-shot classification is used for labelling the reviews by calculating their polarity scores. After that, a pre-trained BERT model is employed for obtaining sentence-level semantics and contextual features from that data and generate embeddings. The obtained contextual embedded vectors were then passed to the neural network, comprised of dilated convolution and Bi-LSTM. The proposed model uses dilated convolution instead of classical convolution to extract local and global contextual semantic features from the embedded data. Bi-directional Long Short-Term Memory (Bi-LSTM) is used for the entire sequencing of the sentences. The CBRNN model is evaluated across four diverse domain text datasets based on accuracy, precision, recall, f1-score and AUC values. Thus, CBRNN can be efficiently used for performing SA tasks on social media reviews, without any information loss.},
	language = {en},
	urldate = {2024-10-23},
	journal = {Array},
	author = {Tabinda Kokab, Sayyida and Asghar, Sohail and Naz, Shehneela},
	month = jul,
	year = {2022},
	pages = {100157},
}

@misc{bilokon_transformers_2023,
	title = {Transformers versus {LSTMs} for electronic trading},
	url = {http://arxiv.org/abs/2309.11400},
	abstract = {With the rapid development of artificial intelligence, long short term memory (LSTM), one kind of recurrent neural network (RNN), has been widely applied in time series prediction. Like RNN, Transformer is designed to handle the sequential data. As Transformer achieved great success in Natural Language Processing (NLP), researchers got interested in Transformer's performance on time series prediction, and plenty of Transformer-based solutions on long time series forecasting have come out recently. However, when it comes to financial time series prediction, LSTM is still a dominant architecture. Therefore, the question this study wants to answer is: whether the Transformer-based model can be applied in financial time series prediction and beat LSTM. To answer this question, various LSTM-based and Transformer-based models are compared on multiple financial prediction tasks based on high-frequency limit order book data. A new LSTM-based model called DLSTM is built and new architecture for the Transformer-based model is designed to adapt for financial prediction. The experiment result reflects that the Transformer-based model only has the limited advantage in absolute price sequence prediction. The LSTM-based models show better and more robust performance on difference sequence prediction, such as price difference and price movement.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Bilokon, Paul and Qiu, Yitao},
	month = sep,
	year = {2023},
	note = {arXiv:2309.11400},
	keywords = {Computer Science - Machine Learning, Economics - Econometrics, Quantitative Finance - Statistical Finance, Quantitative Finance - Trading and Market Microstructure},
}

@article{rodrawangpai_improving_2022,
	title = {Improving text classification with transformers and layer normalization},
	volume = {10},
	issn = {26668270},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666827022000792},
	doi = {10.1016/j.mlwa.2022.100403},
	abstract = {More than 25,000 injuries and 25 fatalities occur each year due to unstable furniture tip-over incidents. Classifying these furniture tip-over incidents is an essential task for understanding incident patterns and building safer products. For example, this classification can help standards development organizations (SDOs) and policy makers discover hidden insights, which can be used to develop standards and regulations that help improve furniture and make homes safer. Since 2000, the U.S. Consumer Product Safety Commission (CPSC) has published data related to consumer product injuries. The amount of data has grown rapidly, and the process of manually reviewing and classifying individual incidents has correspondingly become very resource intensive. This paper proposes an improved method that employs a combination of natural language processing (NLP) techniques and machine learning (ML) algorithms to classify textual data. Machine learning models can help reduce time and effort by streamlining incident narrative classification for determining whether incidents are related to furniture tip-overs. Challenges often presented by real-world data sets (such as the CPSC data used in our experiment) include imbalanced target classes and narratives requiring domain knowledge, since the data sets contain abbreviations and jargon. Using out-of-the-box, default classification models such as bidirectional encoder representations from transformers (BERT) might not yield adequate results. Our proposed method adds layer normalization and dropout layers to a transformer-based language model, which achieves better classification results than using a transformer-based language alone with imbalanced classes. We carefully measure the impact of hidden layers in order to fine-tune the model.},
	language = {en},
	urldate = {2024-10-23},
	journal = {Machine Learning with Applications},
	author = {Rodrawangpai, Ben and Daungjaiboon, Witawat},
	month = dec,
	year = {2022},
	pages = {100403},
}

@article{zhao_prediction_2024,
	title = {Prediction of {Currency} {Exchange} {Rate} {Based} on {Transformers}},
	volume = {17},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1911-8074},
	url = {https://www.mdpi.com/1911-8074/17/8/332},
	doi = {10.3390/jrfm17080332},
	abstract = {The currency exchange rate is a crucial link between all countries related to economic and trade activities. With increasing volatility, exchange rate fluctuations have become frequent under the combined effects of global economic uncertainty and political risks. Consequently, accurate exchange rate prediction is significant in managing financial risks and economic instability. In recent years, the Transformer models have attracted attention in the field of time series analysis. Transformer models, such as Informer and TFT (Temporal Fusion Transformer), have also been extensively studied. In this paper, we evaluate the performance of the Transformer, Informer, and TFT models based on four exchange rate datasets: NZD/USD, NZD/CNY, NZD/GBP, and NZD/AUD. The results indicate that the TFT model has achieved the highest accuracy in exchange rate prediction, with an R2 value of up to 0.94 and the lowest RMSE and MAE errors. However, the Informer model offers faster training and convergence speeds than the TFT and Transformer, making it more efficient. Furthermore, our experiments on the TFT model demonstrate that integrating the VIX index can enhance the accuracy of exchange rate predictions.},
	language = {en},
	number = {8},
	urldate = {2024-10-23},
	journal = {Journal of Risk and Financial Management},
	author = {Zhao, Lu and Yan, Wei Qi},
	month = aug,
	year = {2024},
	pages = {332},
}

@article{mishra_volatility_2024,
	title = {Volatility forecasting and assessing risk of financial markets using multi-transformer neural network based architecture},
	volume = {133},
	issn = {09521976},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0952197624003816},
	doi = {10.1016/j.engappai.2024.108223},
	abstract = {This research introduces a more reliable model for predicting market volatility. The model incorporates Transformer and Multi-transformer layers with the GARCH and LSTM models and compares their performance to classic GARCH-type models. Euro-US Dollar FX Spot Rate (EURO-USD), Australian-US Dollar FX Spot Rate (AUDUSD), S\&P 500 Index, FTSE 100 Index, Reliance Industries Ltd., and Samsung Electronics Co Ltd. are analyzed. The timeframe examined is January 2005 to December 2021, with training from 2005 to 2016 and testing from 2017 to 2021. Empirical evidence suggests that hybrid Neural-network models, notably Transformer-based models, outperform individual transformers, deep learning, neural networks, and traditional GARCH-type models, even in unpredictable conditions like the COVID-19 pandemic. Within the hybrid Neural-network models, MT-GARCH and MTL-GARCH showed lower validation error (RMSE) than the other models, showing that the bagging mechanism added to the attention mechanism of the Multi-Transformer architecture helped to lower the error in the variance in the noisy data of the daily returns of the assets, reducing the RMSE of the hybrid multi-Transformer models. In addition, three to four of the five hybrid neural-network models showed appropriate risk estimates for the entire test period, as observed from the Christoffersen test. Moreover, the lengthy sample period helps test whether hybrid models perform better than classic GARCH-type models in volatile conditions like the COVID-19 pandemic.},
	language = {en},
	urldate = {2024-10-23},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Mishra, Aswini Kumar and Renganathan, Jayashree and Gupta, Aaryaman},
	month = jul,
	year = {2024},
	pages = {108223},
}

@misc{yang_generating_2020,
	title = {Generating {Plausible} {Counterfactual} {Explanations} for {Deep} {Transformers} in {Financial} {Text} {Classification}},
	url = {http://arxiv.org/abs/2010.12512},
	abstract = {Corporate mergers and acquisitions (M\&A) account for billions of dollars of investment globally every year, and offer an interesting and challenging domain for artificial intelligence. However, in these highly sensitive domains, it is crucial to not only have a highly robust and accurate model, but be able to generate useful explanations to garner a user's trust in the automated system. Regrettably, the recent research regarding eXplainable AI (XAI) in financial text classification has received little to no attention, and many current methods for generating textual-based explanations result in highly implausible explanations, which damage a user's trust in the system. To address these issues, this paper proposes a novel methodology for producing plausible counterfactual explanations, whilst exploring the regularization benefits of adversarial training on language models in the domain of FinTech. Exhaustive quantitative experiments demonstrate that not only does this approach improve the model accuracy when compared to the current state-of-the-art and human performance, but it also generates counterfactual explanations which are significantly more plausible based on human trials.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Yang, Linyi and Kenny, Eoin M. and Ng, Tin Lok James and Yang, Yi and Smyth, Barry and Dong, Ruihai},
	month = oct,
	year = {2020},
	note = {arXiv:2010.12512},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{hendrycks_pretrained_2020,
	address = {Online},
	title = {Pretrained {Transformers} {Improve} {Out}-of-{Distribution} {Robustness}},
	url = {https://aclanthology.org/2020.acl-main.244},
	doi = {10.18653/v1/2020.acl-main.244},
	abstract = {Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers' performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.},
	urldate = {2024-10-23},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Hendrycks, Dan and Liu, Xiaoyuan and Wallace, Eric and Dziedzic, Adam and Krishnan, Rishabh and Song, Dawn},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {2744--2751},
}

@article{wang_stock_2022,
	title = {Stock market index prediction using deep {Transformer} model},
	volume = {208},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417422013100},
	doi = {10.1016/j.eswa.2022.118128},
	abstract = {Applications of deep learning in financial market prediction have attracted widespread attention from investors and scholars. From convolutional neural networks to recurrent neural networks, deep learning methods exhibit superior ability to capture the non-linear characteristics of stock markets and, accordingly, achieve a high performance on stock market index prediction. In this paper, we utilize the latest deep learning framework, Transformer, to predict the stock market index. Transformer was initially developed for the natural language processing problem, and has recently been applied to time series forecasting. Through the encoder–decoder architecture and the multi-head attention mechanism, Transformer can better characterize the underlying rules of stock market dynamics. We implement several back-testing experiments on the main stock market indices worldwide, including CSI 300, S\&P 500, Hang Seng Index, and Nikkei 225. All the experiments demonstrate that Transformer outperforms other classic methods significantly and can gain excess earnings for investors.},
	language = {en},
	urldate = {2024-10-23},
	journal = {Expert Systems with Applications},
	author = {Wang, Chaojie and Chen, Yuanyuan and Zhang, Shuqi and Zhang, Qiuhui},
	month = dec,
	year = {2022},
	pages = {118128},
}

@article{casola_pre-trained_2022,
	title = {Pre-trained transformers: an empirical comparison},
	volume = {9},
	issn = {26668270},
	shorttitle = {Pre-trained transformers},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666827022000445},
	doi = {10.1016/j.mlwa.2022.100334},
	abstract = {Pre-trained transformers have rapidly become very popular in the Natural Language Processing (NLP) community, surpassing the previous state of the art in a wide variety of tasks. While their effectiveness is indisputable, these methods are expensive to fine-tune on the target domain due to the high number of hyper-parameters; this aspect significantly affects the model selection phase and the reliability of the experimental assessment. This paper serves a double purpose: we first describe five popular transformer models and survey their typical use in previous literature, focusing on reproducibility; then, we perform comparisons in a controlled environment over a wide range of NLP tasks. Our analysis reveals that only a minority of recent NLP papers that use pre-trained transformers reported multiple runs (20\%), standard deviation or statistical significance (10\%), and other crucial information, seriously hurting replicability and reproducibility. Through a vast empirical comparison on real-world datasets and benchmarks, we also show how the hyper-parameters and the initial seed impact results, and highlight the low models’ robustness.},
	language = {en},
	urldate = {2024-10-23},
	journal = {Machine Learning with Applications},
	author = {Casola, Silvia and Lauriola, Ivano and Lavelli, Alberto},
	month = sep,
	year = {2022},
	pages = {100334},
}

@article{zhang_survey_2024,
	title = {Survey of transformers and towards ensemble learning using transformers for natural language processing},
	volume = {11},
	issn = {2196-1115},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00842-0},
	doi = {10.1186/s40537-023-00842-0},
	abstract = {The transformer model is a famous natural language processing model proposed by Google in 2017. Now, with the extensive development of deep learning, many natural language processing tasks can be solved by deep learning methods. After the BERT model was proposed, many pre-trained models such as the XLNet model, the RoBERTa model, and the ALBERT model were also proposed in the research community. These models perform very well in various natural language processing tasks. In this paper, we describe and compare these well-known models. In addition, we also apply several types of existing and well-known models which are the BERT model, the XLNet model, the RoBERTa model, the GPT2 model, and the ALBERT model to different existing and well-known natural language processing tasks, and analyze each model based on their performance. There are a few papers that comprehensively compare various transformer models. In our paper, we use six types of well-known tasks, such as sentiment analysis, question answering, text generation, text summarization, name entity recognition, and topic modeling tasks to compare the performance of various transformer models. In addition, using the existing models, we also propose ensemble learning models for the different natural language processing tasks. The results show that our ensemble learning models perform better than a single classifier on specific tasks.},
	language = {en},
	number = {1},
	urldate = {2024-10-23},
	journal = {Journal of Big Data},
	author = {Zhang, Hongzhi and Shafiq, M. Omair},
	month = feb,
	year = {2024},
	pages = {25},
}

@misc{k_temporal_2020,
	title = {Temporal {Embeddings} and {Transformer} {Models} for {Narrative} {Text} {Understanding}},
	url = {http://arxiv.org/abs/2003.08811},
	abstract = {We present two deep learning approaches to narrative text understanding for character relationship modelling. The temporal evolution of these relations is described by dynamic word embeddings, that are designed to learn semantic changes over time. An empirical analysis of the corresponding character trajectories shows that such approaches are effective in depicting dynamic evolution. A supervised learning approach based on the state-of-the-art transformer model BERT is used instead to detect static relations between characters. The empirical validation shows that such events (e.g., two characters belonging to the same family) might be spotted with good accuracy, even when using automatically annotated data. This provides a deeper understanding of narrative plots based on the identification of key facts. Standard clustering techniques are finally used for character de-aliasing, a necessary pre-processing step for both approaches. Overall, deep learning models appear to be suitable for narrative text understanding, while also providing a challenging and unexploited benchmark for general natural language understanding.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {K, Vani and Mellace, Simone and Antonucci, Alessandro},
	month = mar,
	year = {2020},
	note = {arXiv:2003.08811},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{chernyavskiy_transformers_2021,
	title = {Transformers: "{The} {End} of {History}" for {NLP}?},
	shorttitle = {Transformers},
	url = {http://arxiv.org/abs/2105.00813},
	abstract = {Recent advances in neural architectures, such as the Transformer, coupled with the emergence of large-scale pre-trained models such as BERT, have revolutionized the field of Natural Language Processing (NLP), pushing the state of the art for a number of NLP tasks. A rich family of variations of these models has been proposed, such as RoBERTa, ALBERT, and XLNet, but fundamentally, they all remain limited in their ability to model certain kinds of information, and they cannot cope with certain information sources, which was easy for pre-existing models. Thus, here we aim to shed light on some important theoretical limitations of pre-trained BERT-style models that are inherent in the general Transformer architecture. First, we demonstrate in practice on two general types of tasks -- segmentation and segment labeling -- and on four datasets that these limitations are indeed harmful and that addressing them, even in some very simple and naive ways, can yield sizable improvements over vanilla RoBERTa and XLNet models. Then, we offer a more general discussion on desiderata for future additions to the Transformer architecture that would increase its expressiveness, which we hope could help in the design of the next generation of deep NLP architectures.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Chernyavskiy, Anton and Ilvovsky, Dmitry and Nakov, Preslav},
	month = sep,
	year = {2021},
	note = {arXiv:2105.00813},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@inproceedings{ilgun_sentiment_2021,
	title = {Sentiment {Analysis} using {Transformers} and {Machine} {Learning} {Models}},
	url = {https://ieeexplore.ieee.org/document/9558931/?arnumber=9558931},
	doi = {10.1109/UBMK52708.2021.9558931},
	abstract = {Socia1 media is an online network environment where internet users express their positive or negative opinions about any subject, business, product, or situation. It is an environment where users can view and share relevant content, articles, news, thoughts, daily events and all kinds of visual and audio materials thanks to fast access. Social media sentiment analysis is a popular field for various industries and academic studies. These studies, also known as idea mining, are carried out to classify the general feeling in a text. Studies have been conducted on this subject with machine learning models in big data and natural language processing. In our research, sentiment analysis will be shown on text data sets with machine learning models and transformers.},
	urldate = {2024-10-23},
	booktitle = {2021 6th {International} {Conference} on {Computer} {Science} and {Engineering} ({UBMK})},
	author = {İlgün, Hüs Eyin and Kılıç, Erdal},
	month = sep,
	year = {2021},
	note = {ISSN: 2521-1641},
	keywords = {Analytical models, Computational modeling, Deep learning, Logistic Regression, Machine Learning, Sentiment analysis, Social networking (online), Support Vector Machines, Support vector machines, Transformers, Visualization},
	pages = {42--45},
}

@article{bacco_extractive_nodate,
	title = {Extractive {Summarization} for {Explainable} {Sentiment} {Analysis} using {Transformers}},
	abstract = {In recent years, the paradigm of eXplainable Artificial Intelligence (XAI) systems has gained wide research interest and beyond. The Natural Language Processing (NLP) community is also approaching this new way of understanding AI applications: building a suite of models that provide an explanation for the decision, without affecting performance. This is certainly not an easy task, considering the wide use of very poorly interpretable models such as Transformers, which in recent years are found to be almost ubiquitous in the NLP literature because of the great strides they have allowed. Here we propose two different methodologies to exploit the performance of these models in a task of sentiment analysis and, in the meantime, to generate a summary that serves as an explanation of the decision taken by the system. To compare the classification performance of the two methodologies, we used the IMDB dataset while, to assess the explainability performance, we annotated some samples of this dataset to retrieve human extractive summaries, benchmarking them with the summaries generated by the systems.},
	language = {en},
	author = {Bacco, Luca and Cimino, Andrea and Dell’Orletta, Felice and Merone, Mario},
}

@article{t_transformers_2023,
	title = {Transformers in {Machine} {Learning}: {Literature} {Review}},
	volume = {9},
	copyright = {Copyright (c) 2023 Thoyyibah T, Wasis  Haryono, Achmad Udin  Zailani, Yan Mitha  Djaksana, Neny  Rosmawarni, Nunik Destria  Arianti},
	issn = {2407-795X},
	shorttitle = {Transformers in {Machine} {Learning}},
	url = {https://jppipa.unram.ac.id/index.php/jppipa/article/view/5040},
	doi = {10.29303/jppipa.v9i9.5040},
	abstract = {In this study, the researcher presents an approach regarding methods in Transformer Machine Learning. Initially, transformers are neural network architectures that are considered as inputs. Transformers are widely used in various studies with various objects. The transformer is one of the deep learning architectures that can be modified. Transformers are also mechanisms that study contextual relationships between words. Transformers are used for text compression in readings. Transformers are used to recognize chemical images with an accuracy rate of 96\%. Transformers are used to detect a person's emotions. Transformer to detect emotions in social media conversations, for example, on Facebook with happy, sad, and angry categories. Figure 1 illustrates the encoder and decoder process through the input process and produces output. the purpose of this study is to only review literature from various journals that discuss transformers. This explanation is also done by presenting the subject or dataset, data analysis method, year, and accuracy achieved. By using the methods presented, researchers can conclude results in search of the highest accuracy and opportunities for further research.},
	language = {en},
	number = {9},
	urldate = {2024-10-23},
	journal = {Jurnal Penelitian Pendidikan IPA},
	author = {T, Thoyyibah and Haryono, Wasis and Zailani, Achmad Udin and Djaksana, Yan Mitha and Rosmawarni, Neny and Arianti, Nunik Destria},
	month = sep,
	year = {2023},
	note = {Number: 9},
	keywords = {Accuracy, Machine learning, Transformer},
	pages = {604--610},
}

@article{tucudean_natural_2024,
	title = {Natural language processing with transformers: a review},
	volume = {10},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2376-5992},
	shorttitle = {Natural language processing with transformers},
	url = {https://peerj.com/articles/cs-2222},
	doi = {10.7717/peerj-cs.2222},
	abstract = {Natural language processing (NLP) tasks can be addressed with several deep learning architectures, and many different approaches have proven to be efﬁcient. This study aims to brieﬂy summarize the use cases for NLP tasks along with the main architectures. This research presents transformer-based solutions for NLP tasks such as Bidirectional Encoder Representations from Transformers (BERT), and Generative Pre-Training (GPT) architectures. To achieve that, we conducted a stepby-step process in the review strategy: identify the recent studies that include Transformers, apply ﬁlters to extract the most consistent studies, identify and deﬁne inclusion and exclusion criteria, assess the strategy proposed in each study, and ﬁnally discuss the methods and architectures presented in the resulting articles. These steps facilitated the systematic summarization and comparative analysis of NLP applications based on Transformer architectures. The primary focus is the current state of the NLP domain, particularly regarding its applications, language models, and data set types. The results provide insights into the challenges encountered in this research domain.},
	language = {en},
	urldate = {2024-10-23},
	journal = {PeerJ Computer Science},
	author = {Tucudean, Georgiana and Bucos, Marian and Dragulescu, Bogdan and Caleanu, Catalin Daniel},
	month = aug,
	year = {2024},
	pages = {e2222},
}

@article{rizinski_sentiment_2024,
	title = {Sentiment {Analysis} in {Finance}: {From} {Transformers} {Back} to {eXplainable} {Lexicons} ({XLex})},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {2169-3536},
	shorttitle = {Sentiment {Analysis} in {Finance}},
	url = {https://ieeexplore.ieee.org/document/10380556/},
	doi = {10.1109/ACCESS.2024.3349970},
	abstract = {Lexicon-based sentiment analysis in finance leverages specialized, manually annotated lexicons created by human experts to effectively extract sentiment from financial texts. Although lexiconbased methods are simple to implement and fast to operate on textual data, they require considerable manual annotation efforts to create, maintain, and update the lexicons. These methods are also considered inferior to the deep learning-based approaches, such as transformer models, which have become dominant in various natural language processing (NLP) tasks due to their remarkable performance. However, their efficacy comes at a cost: these models require extensive data and computational resources for both training and testing. Additionally, they involve significant prediction times, making them unsuitable for real-time production environments or systems with limited processing capabilities. In this paper, we introduce a novel methodology named eXplainable Lexicons (XLex) that combines the advantages of both lexicon-based methods and transformer models. We propose an approach that utilizes transformers and SHapley Additive exPlanations (SHAP) for explainability to automatically learn financial lexicons. Our study presents four main contributions. Firstly, we demonstrate that transformer-aided explainable lexicons can enhance the vocabulary coverage of the benchmark Loughran-McDonald (LM) lexicon. This enhancement leads to a significant reduction in the need for human involvement in the process of annotating, maintaining, and updating the lexicons. Secondly, we show that the resulting lexicon outperforms the standard LM lexicon in sentiment analysis of financial datasets. Our experiments show that XLex outperforms LM when applied to general financial texts, resulting in enhanced word coverage and an overall increase in classification accuracy by 0.431. Furthermore, by employing XLex to extend LM, we create a combined dictionary, XLex+LM, which achieves an even higher accuracy improvement of 0.450. Thirdly, we illustrate that the lexicon-based approach is significantly more efficient in terms of model speed and size compared to transformers. Lastly, the proposed XLex approach is inherently more interpretable than transformer models. This interpretability is advantageous as lexicon models rely on predefined rules, unlike transformers, which have complex inner workings. The interpretability of the models allows for better understanding and insights into the results of sentiment analysis, making the XLex approach a valuable tool for financial decision-making.},
	language = {en},
	urldate = {2024-10-23},
	journal = {IEEE Access},
	author = {Rizinski, Maryan and Peshov, Hristijan and Mishev, Kostadin and Jovanovik, Milos and Trajanov, Dimitar},
	year = {2024},
	pages = {7170--7198},
}

@inproceedings{v_ganesan_empirical_2021,
	address = {Online},
	title = {Empirical {Evaluation} of {Pre}-trained {Transformers} for {Human}-{Level} {NLP}: {The} {Role} of {Sample} {Size} and {Dimensionality}},
	shorttitle = {Empirical {Evaluation} of {Pre}-trained {Transformers} for {Human}-{Level} {NLP}},
	url = {https://aclanthology.org/2021.naacl-main.357},
	doi = {10.18653/v1/2021.naacl-main.357},
	abstract = {In human-level NLP tasks, such as predicting mental health, personality, or demographics, the number of observations is often smaller than the standard 768+ hidden state sizes of each layer within modern transformer-based language models, limiting the ability to effectively leverage transformers. Here, we provide a systematic study on the role of dimension reduction methods (principal components analysis, factorization techniques, or multi-layer auto-encoders) as well as the dimensionality of embedding vectors and sample sizes as a function of predictive performance. We first find that fine-tuning large models with a limited amount of data pose a significant difficulty which can be overcome with a pre-trained dimension reduction regime. RoBERTa consistently achieves top performance in human-level tasks, with PCA giving benefit over other reduction methods in better handling users that write longer texts. Finally, we observe that a majority of the tasks achieve results comparable to the best performance with just 1/12 of the embedding dimensions.},
	urldate = {2024-10-23},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {V Ganesan, Adithya and Matero, Matthew and Ravula, Aravind Reddy and Vu, Huy and Schwartz, H. Andrew},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	pages = {4515--4532},
}

@inproceedings{gillioz_overview_2020,
	title = {Overview of the {Transformer}-based {Models} for {NLP} {Tasks}},
	url = {https://annals-csis.org/Volume_21/drp/20.html},
	doi = {10.15439/2020F20},
	abstract = {In 2017, Vaswani et al. proposed a new neural network architecture named Transformer. That modern architecture quickly revolutionized the natural language processing world. Models like GPT and BERT relying on this Transformer architecture have fully outperformed the previous state-of-theart networks. It surpassed the earlier approaches by such a wide margin that all the recent cutting edge models seem to rely on these Transformer-based architectures.},
	language = {en},
	urldate = {2024-10-23},
	author = {Gillioz, Anthony and Casas, Jacky and Mugellini, Elena and Khaled, Omar Abou},
	month = sep,
	year = {2020},
	pages = {179--183},
}

@misc{sanh_distilbert_2020,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	shorttitle = {{DistilBERT}, a distilled version of {BERT}},
	url = {http://arxiv.org/abs/1910.01108},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	month = mar,
	year = {2020},
	note = {arXiv:1910.01108},
	keywords = {Computer Science - Computation and Language},
}

@misc{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv:1907.11692},
	keywords = {Computer Science - Computation and Language},
}

@misc{lan_albert_2020,
	title = {{ALBERT}: {A} {Lite} {BERT} for {Self}-supervised {Learning} of {Language} {Representations}},
	shorttitle = {{ALBERT}},
	url = {http://arxiv.org/abs/1909.11942},
	abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and {\textbackslash}squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
	month = feb,
	year = {2020},
	note = {arXiv:1909.11942},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2024-10-23},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	pages = {4171--4186},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 
version: 5},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@incollection{brunnermeier_chapter_2013,
	title = {Chapter 18 - {Bubbles}, {Financial} {Crises}, and {Systemic} {Risk}},
	volume = {2},
	url = {https://www.sciencedirect.com/science/article/pii/B9780444594068000184},
	abstract = {This chapter surveys the literature on bubbles, financial crises, and systemic risk. The first part of the chapter provides a brief historical account of bubbles and financial crisis. The second part of the chapter gives a structured overview of the literature on financial bubbles. The third part of the chapter discusses the literatures on financial crises and systemic risk, with particular emphasis on amplification and propagation mechanisms during financial crises, and the measurement of systemic risk. Finally, we point toward some questions for future research.},
	urldate = {2024-10-23},
	booktitle = {Handbook of the {Economics} of {Finance}},
	publisher = {Elsevier},
	author = {Brunnermeier, Markus K. and Oehmke, Martin},
	editor = {Constantinides, George M. and Harris, Milton and Stulz, Rene M.},
	month = jan,
	year = {2013},
	doi = {10.1016/B978-0-44-459406-8.00018-4},
	keywords = {Bubbles, Crashes, Financial crises, Systemic risk},
	pages = {1221--1288},
}

@article{hirano_bubble_2024,
	title = {Bubble economics},
	volume = {111},
	issn = {03044068},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304406824000065},
	doi = {10.1016/j.jmateco.2024.102944},
	abstract = {This article provides a self-contained overview of the theory of rational asset price bubbles. We cover topics from basic definitions, properties, and classical results to frontier research, with an emphasis on bubbles attached to real assets such as stocks, housing, and land. The main message is that bubbles attached to real assets are fundamentally nonstationary phenomena related to unbalanced growth. We present a bare-bones model and draw three new insights: (i) the emergence of asset price bubbles is a necessity, instead of a possibility; (ii) asset pricing implications are markedly different between balanced growth of stationary nature and unbalanced growth of nonstationary nature; and (iii) asset price bubbles occur within larger historical trends involving shifts in industrial structure driven by technological innovation, including the transition from the Malthusian economy to the modern economy.},
	language = {en},
	urldate = {2024-10-22},
	journal = {Journal of Mathematical Economics},
	author = {Hirano, Tomohiro and Toda, Alexis Akira},
	month = apr,
	year = {2024},
	pages = {102944},
}

@article{siegel_what_2003,
	title = {What {Is} an {Asset} {Price} {Bubble}? {An} {Operational} {Definition}},
	volume = {9},
	issn = {1354-7798, 1468-036X},
	shorttitle = {What {Is} an {Asset} {Price} {Bubble}?},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/1468-036X.00206},
	doi = {10.1111/1468-036X.00206},
	abstract = {This paper reviews and analyses the current definitions of bubbles in asset prices. It makes the case that one cannot identify a bubble immediately, but one has to wait a sufficient amount of time to determine whether the previous prices can be justified by subsequent cash flows. The paper proposes an operational definition of a bubble as any time the realised asset return over given future period is more than two standard deviations from its expected return. Using this framework, the paper shows how the great crash of 1929 and 1987— both periods generally characterised as bubbles —prove not to be bubbles but the low point in stock prices in 1932 is a ‘negative bubble.’ The paper then extends this analysis to the internet stocks and concludes that it is virtually certain that it is a bubble.},
	language = {en},
	number = {1},
	urldate = {2024-10-20},
	journal = {European Financial Management},
	author = {Siegel, Jeremy J.},
	month = mar,
	year = {2003},
	pages = {11--24},
}

@article{khadjeh_nassirtoussi_text_2014,
	title = {Text mining for market prediction: {A} systematic review},
	volume = {41},
	issn = {09574174},
	shorttitle = {Text mining for market prediction},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417414003455},
	doi = {10.1016/j.eswa.2014.06.009},
	abstract = {The quality of the interpretation of the sentiment in the online buzz in the social media and the online news can determine the predictability of ﬁnancial markets and cause huge gains or losses. That is why a number of researchers have turned their full attention to the different aspects of this problem lately. However, there is no well-rounded theoretical and technical framework for approaching the problem to the best of our knowledge. We believe the existing lack of such clarity on the topic is due to its interdisciplinary nature that involves at its core both behavioral-economic topics as well as artiﬁcial intelligence. We dive deeper into the interdisciplinary nature and contribute to the formation of a clear frame of discussion. We review the related works that are about market prediction based on onlinetext-mining and produce a picture of the generic components that they all have. We, furthermore, compare each system with the rest and identify their main differentiating factors. Our comparative analysis of the systems expands onto the theoretical and technical foundations behind each. This work should help the research community to structure this emerging ﬁeld and identify the exact aspects which require further research and are of special signiﬁcance.},
	language = {en},
	number = {16},
	urldate = {2024-10-09},
	journal = {Expert Systems with Applications},
	author = {Khadjeh Nassirtoussi, Arman and Aghabozorgi, Saeed and Ying Wah, Teh and Ngo, David Chek Ling},
	month = nov,
	year = {2014},
	pages = {7653--7670},
}

@article{kearns_machine_nodate,
	title = {Machine {Learning} for {Market} {Microstructure} and {High} {Frequency} {Trading}},
	language = {en},
	author = {Kearns, Michael and Nevmyvaka, Yuriy},
}

@misc{barez_exploring_2023,
	title = {Exploring the {Advantages} of {Transformers} for {High}-{Frequency} {Trading}},
	url = {http://arxiv.org/abs/2302.13850},
	abstract = {This paper explores the novel deep learning Transformers architectures for high-frequency Bitcoin-USDT log-return forecasting and compares them to the traditional Long Short-Term Memory models. A hybrid Transformer model, called HFformer, is then introduced for time series forecasting which incorporates a Transformer encoder, linear decoder, spiking activations, and quantile loss function, and does not use position encoding. Furthermore, possible high-frequency trading strategies for use with the HFformer model are discussed, including trade sizing, trading signal aggregation, and minimal trading threshold. Ultimately, the performance of the HFformer and Long Short-Term Memory models are assessed and results indicate that the HFformer achieves a higher cumulative PnL than the LSTM when trading with multiple signals during backtesting1.},
	language = {en},
	urldate = {2024-10-09},
	publisher = {arXiv},
	author = {Barez, Fazl and Bilokon, Paul and Gervais, Arthur and Lisitsyn, Nikita},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13850 [cs, q-fin]},
	keywords = {Computer Science - Machine Learning, Quantitative Finance - Statistical Finance},
}

@misc{wu_bloomberggpt_2023,
	title = {{BloombergGPT}: {A} {Large} {Language} {Model} for {Finance}},
	shorttitle = {{BloombergGPT}},
	url = {http://arxiv.org/abs/2303.17564},
	abstract = {The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg’s extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.},
	language = {en},
	urldate = {2024-10-09},
	publisher = {arXiv},
	author = {Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
	month = dec,
	year = {2023},
	note = {arXiv:2303.17564 [cs, q-fin]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Quantitative Finance - General Finance},
}

@misc{lis_analyzing_2023,
	title = {Analyzing {Credit} {Risk} {Model} {Problems} through {NLP}-{Based} {Clustering} and {Machine} {Learning}: {Insights} from {Validation} {Reports}},
	shorttitle = {Analyzing {Credit} {Risk} {Model} {Problems} through {NLP}-{Based} {Clustering} and {Machine} {Learning}},
	url = {http://arxiv.org/abs/2306.01618},
	abstract = {This paper explores the use of clustering methods and machine learning algorithms, including Natural Language Processing (NLP), to identify and classify problems identified in credit risk models through textual information contained in validation reports. Using a unique dataset of 657 findings raised by validation teams in a large international banking group between January 2019 and December 2022. The findings are classified into nine validation dimensions and assigned a severity level by validators using their expert knowledge. The authors use embedding generation for the findings’ titles and observations using four different pre-trained models, including "module\_url" from TensorFlow Hub and three models from the SentenceTransformer library, namely "all-mpnet-base-v2", "allMiniLM-L6-v2", and "paraphrase-mpnet-base-v2". The paper uses and compares various clustering methods in grouping findings with similar characteristics, enabling the identification of common problems within each validation dimension and severity. The results of the study show that clustering is an effective approach for identifying and classifying credit risk model problems with accuracy higher than 60\%. The authors also employ machine learning algorithms, including logistic regression and XGBoost, to predict the validation dimension and its severity, achieving an accuracy of 80\% for XGBoost algorithm. Furthermore, the study identifies the top 10 words that predict a validation dimension and severity. Overall, this paper makes a contribution by demonstrating the usefulness of clustering and machine learning for analyzing textual information in validation reports, and providing insights into the types of problems encountered in the development and validation of credit risk models.},
	language = {en},
	urldate = {2024-10-09},
	publisher = {arXiv},
	author = {Lis, Szymon and Kubkowski, Mariusz and Borkowska, Olimpia and Serwa, Dobromił and Kurpanik, Jarosław},
	month = jun,
	year = {2023},
	note = {arXiv:2306.01618 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{ghosh_recent_2023,
	title = {Recent trends in financial natural language processing research},
	volume = {8},
	issn = {27725693},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2772569323001457},
	doi = {10.1016/j.sctalk.2023.100270},
	language = {en},
	urldate = {2024-10-09},
	journal = {Science Talks},
	author = {Ghosh, Sohom and Naskar, Sudip Kumar},
	month = dec,
	year = {2023},
	pages = {100270},
}

@article{noauthor_application_2024,
	title = {Application of {Natural} {Language} {Processing} in {Financial} {Risk} {Detection}},
	volume = {7},
	issn = {25232576},
	url = {https://www.clausiuspress.com/article/12720.html},
	doi = {10.23977/ferm.2024.070401},
	abstract = {This paper explores the application of Natural Language Processing (NLP) in financial risk detection. By constructing an NLP-based financial risk detection model, this study aims to identify and predict potential risks in financial documents and communications. First, the fundamental concepts of NLP and its theoretical foundation, including text mining methods, NLP model design principles, and machine learning algorithms, are introduced. Second, the process of text data preprocessing and feature extraction is described. Finally, the effectiveness and predictive performance of the model are validated through empirical research. The results show that the NLP-based financial risk detection model performs excellently in risk identification and prediction, providing effective risk management tools for financial institutions. This study offers valuable references for the field of financial risk management, utilizing advanced NLP techniques to improve the accuracy and efficiency of financial risk detection.},
	language = {en},
	number = {4},
	urldate = {2024-10-09},
	journal = {Financial Engineering and Risk Management},
	year = {2024},
}

@article{li_financial_2024,
	title = {Financial {Risk} {Prediction} and {Management} using {Machine} {Learning} and {Natural} {Language} {Processing}},
	volume = {15},
	issn = {21565570, 2158107X},
	url = {http://thesai.org/Publications/ViewPaper?Volume=15&Issue=6&Code=ijacsa&SerialNo=23},
	doi = {10.14569/IJACSA.2024.0150623},
	abstract = {With the continuous development and changes in the global financial markets, financial risk management has become increasingly important for the stable operation of enterprises. Traditional financial risk management methods, primarily relying on financial statement analysis and historical data statistics, show clear limitations when dealing with largescale unstructured data. The rapid development of machine learning and Natural Language Processing (NLP) technologies in recent years offers new perspectives and methods for financial risk prediction and management. This paper explores and conducts empirical analysis financial risk management using these advanced technologies, with a particular focus on the application of NLP in measuring financial risk tendencies, and the financial risk prediction and management based on a Deep neural network - Factorization Machine (DeepFM) model. Through in-depth analysis and research, this paper proposes a new financial risk management model that combines NLP and deep learning technologies, aimed at improving the accuracy and efficiency of financial risk prediction. This study not only broadens the theoretical horizons of financial risk management but also provides effective technical support and decision-making references for practical operations.},
	language = {en},
	number = {6},
	urldate = {2024-10-09},
	journal = {International Journal of Advanced Computer Science and Applications},
	author = {Li, Tianyu and Dai, Xiangyu},
	year = {2024},
}

@article{xing_natural_2018,
	title = {Natural language based financial forecasting: a survey},
	volume = {50},
	issn = {0269-2821, 1573-7462},
	shorttitle = {Natural language based financial forecasting},
	url = {http://link.springer.com/10.1007/s10462-017-9588-9},
	doi = {10.1007/s10462-017-9588-9},
	abstract = {Natural language processing (NLP), or the pragmatic research perspective of computational linguistics, has become increasingly powerful due to data availability and various techniques developed in the past decade. This increasing capability makes it possible to capture sentiments more accurately and semantics in a more nuanced way. Naturally, many applications are starting to seek improvements by adopting cutting-edge NLP techniques. Financial forecasting is no exception. As a result, articles that leverage NLP techniques to predict ﬁnancial markets are fast accumulating, gradually establishing the research ﬁeld of natural language based ﬁnancial forecasting (NLFF), or from the application perspective, stock market prediction. This review article clariﬁes the scope of NLFF research by ordering and structuring techniques and applications from related work. The survey also aims to increase the understanding of progress and hotspots in NLFF, and bring about discussions across many different disciplines.},
	language = {en},
	number = {1},
	urldate = {2024-10-09},
	journal = {Artificial Intelligence Review},
	author = {Xing, Frank Z. and Cambria, Erik and Welsch, Roy E.},
	month = jun,
	year = {2018},
	pages = {49--73},
}

@inproceedings{konstantinidis_comparative_2023,
	title = {A comparative study on {ML}-based approaches for {Main} {Entity} {Detection} in {Financial} {Reports}},
	url = {https://ieeexplore.ieee.org/document/10167951/?arnumber=10167951},
	doi = {10.1109/DSP58604.2023.10167951},
	abstract = {Modern AI technologies which exploit the classification and/or prediction capacities of Deep Neural Architectures demonstrate superior performance to traditional approaches in most cases. However, they come with the unavoidable shortcoming of lack of transparency in their outcomes. This attribute renders them unsuitable for big industrial sectors, such as finance, investment management, etc. Specifically, their "black-box" nature makes them unattractive in cases where human understanding in the decision making process is required and may be legally mandatory. In such cases, traditional (i.e., non-deep learning) ML approaches are still preferred, to minimize for example the presence of false positives. In this context, this paper introduces an unsupervised, trustful, bottom-up probabilistic approach for Named Entity Recognition (NER) in financial reports, while in parallel it provides a comparative study on well-known ML-approaches in terms of their performance. The proposed approach builds on the probability of appearance of representative tokens within the given reports and utilizes Kronecker’s Delta and the Total Probability Theorem to construct a probabilistic model that estimates the overall classification probability of a document.},
	urldate = {2024-10-09},
	booktitle = {2023 24th {International} {Conference} on {Digital} {Signal} {Processing} ({DSP})},
	author = {Konstantinidis, Thanos and Xu, Yao Lei and Constantinides, Tony G. and Mandic, Danilo P.},
	month = jun,
	year = {2023},
	note = {ISSN: 2165-3577},
	keywords = {Artificial intelligence, Closed box, Decision making, Digital signal processing, Finance, Investment, Probabilistic logic},
	pages = {1--5},
}

@misc{tran_predicting_2022,
	title = {Predicting {Digital} {Asset} {Prices} using {Natural} {Language} {Processing}: a survey},
	shorttitle = {Predicting {Digital} {Asset} {Prices} using {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/2212.00726},
	abstract = {The introduction of blockchain technology has changed the way people think about how they used to store and trade their assets, as it introduced us to a whole new way to transact: using digital currencies. One of the major innovations of blockchain technology is decentralization, meaning that traditional ﬁnancial intermediaries, such as asset-backed security issuers and banks, are eliminated in the process. Even though the blockchain technology has been utilized in a wide range of industries, its most prominent application is still cryptocurrencies, with Bitcoin being the ﬁrst one proposed. At its peak in 2021, the market cap for Bitcoin once surpassed 1 trillion US dollars. The open nature of the crypto market poses various challenges and concerns for both potential retail investors and institutional investors, as the price of the investment is highly volatile and its ﬂuctuations are unpredictable. The rise of Machine Learning, and Natural Language Processing, in particular, has shed some light on monitoring and predicting the price behaviors of cryptocurrencies. This paper aims to review and analyze the recent eﬀorts in applying Machine Learning and Natural Language Processing methods to predict the prices and analyze the behaviors of digital assets such as Bitcoin and Ethereum.},
	language = {en},
	urldate = {2024-10-07},
	publisher = {arXiv},
	author = {Tran, Trang},
	month = nov,
	year = {2022},
	note = {arXiv:2212.00726 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Cryptography and Security},
}

@inproceedings{olaniyan_sentiment_2015,
	title = {Sentiment and stock market volatility predictive modelling — {A} hybrid approach},
	url = {https://ieeexplore.ieee.org/document/7344855/?arnumber=7344855},
	doi = {10.1109/DSAA.2015.7344855},
	abstract = {The frequent ups and downs are characteristic to the stock market. The conventional standard models that assume that investors act rationally have not been able to capture the irregularities in the stock market patterns for years. As a result, behavioural finance is embraced to attempt to correct these model shortcomings by adding some factors to capture sentimental contagion which may be at play in determining the stock market. This paper assesses the predictive influence of sentiment on the stock market returns by using a non-parametric nonlinear approach that corrects specific limitations encountered in previous related work. In addition, the paper proposes a new approach to developing stock market volatility predictive models by incorporating a hybrid GARCH and artificial neural network framework, and proves the advantage of this framework over a GARCH only based framework. Our results reveal also that past volatility and positive sentiment appear to have strong predictive power over future volatility.},
	urldate = {2024-10-07},
	booktitle = {2015 {IEEE} {International} {Conference} on {Data} {Science} and {Advanced} {Analytics} ({DSAA})},
	author = {Olaniyan, Rapheal and Stamate, Daniel and Ouarbya, Lahcen and Logofatu, Doina},
	month = oct,
	year = {2015},
	keywords = {Benchmark testing, EGARCH, Electric shock, GARCH, Granger causality, Monte Carlo methods, Monte Carlo simulations, Neural networks, Predictive models, Standards, Stock markets, artificial neural networks, non-parametric test, sentiment, stock market, volatility},
	pages = {1--10},
}

@article{gros-klusmann_when_2011,
	title = {When machines read the news: {Using} automated text analytics to quantify high frequency news-implied market reactions},
	volume = {18},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {09275398},
	shorttitle = {When machines read the news},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0927539810000873},
	doi = {10.1016/j.jempfin.2010.11.009},
	language = {en},
	number = {2},
	urldate = {2024-10-07},
	journal = {Journal of Empirical Finance},
	author = {Groß-Klußmann, Axel and Hautsch, Nikolaus},
	month = mar,
	year = {2011},
	pages = {321--340},
}

@inproceedings{phillips_predicting_2017,
	address = {Honolulu, HI},
	title = {Predicting cryptocurrency price bubbles using social media data and epidemic modelling},
	isbn = {978-1-5386-2726-6},
	url = {http://ieeexplore.ieee.org/document/8280809/},
	doi = {10.1109/SSCI.2017.8280809},
	abstract = {Financial price bubbles have previously been linked with the epidemic-like spread of an investment idea; such bubbles are commonly seen in cryptocurrency prices. This paper aims to predict such bubbles for a number of cryptocurrencies using a hidden Markov model previously utilised to detect influenza epidemic outbreaks, based in this case on the behaviour of novel online social media indicators. To validate the methodology further, a trading strategy is built and tested on historical data. The resulting trading strategy outperforms a buy and hold strategy. The work demonstrates both the broader utility of epidemic-detecting hidden Markov models in the identification of bubble-like behaviour in time series, and that social media can provide valuable predictive information pertaining to cryptocurrency price movements.},
	language = {en},
	urldate = {2024-10-04},
	booktitle = {2017 {IEEE} {Symposium} {Series} on {Computational} {Intelligence} ({SSCI})},
	publisher = {IEEE},
	author = {Phillips, Ross C. and Gorse, Denise},
	month = nov,
	year = {2017},
	pages = {1--7},
}

@article{ashtiani_news-based_2023,
	title = {News-based intelligent prediction of financial markets using text mining and machine learning: {A} systematic literature review},
	volume = {217},
	issn = {09574174},
	shorttitle = {News-based intelligent prediction of financial markets using text mining and machine learning},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417423000106},
	doi = {10.1016/j.eswa.2023.119509},
	abstract = {Researchers and practitioners have attempted to predict the financial market by analyzing textual (e.g., news articles and social media) and numeric data (e.g., hourly stock prices, and moving averages). Among textual data, while many papers have been published that analyze social media, news content has gained limited attention in predicting the stock market. Acknowledging that news is critical in predicting the stock market, the focus of this systematic review is on papers investigating machine learning and text mining techniques to predict the stock market using news. Using Kitchenham’s methodology, we present a systematic review of the literature on intelligent financial market prediction, examining data mining and machine learning approaches and the employed datasets. From five digital libraries, we identified 61 studies from 2015–2022 for synthesis and interpretation. We present notable gaps and barriers to predicting financial markets, then recommend future research scopes. Various input data, including numerical (stock prices and technical indicators) and textual data (news text and sentiment), have been employed for news-based stock market prediction. News data collection can be costly and time-consuming: most studies have used custom crawlers to gather news articles; however, there are financial news databases available that could significantly facilitate news collection. Furthermore, although most datasets have covered fewer than 100K records, deep learning and more sophisticated artificial neural networks can process enormous datasets faster, improving future model performance. There is a growing trend toward using artificial neural networks, particularly recurrent neural networks and deep learning models, from 2018 to 2021. Furthermore, regression and gradient-boosting models have been developed for stock market prediction during the last four years. Although word embedding approaches for feature representation have been employed recently with good accuracy, emerging language models may be a focus for future research. Advanced natural language processing methods like transformers have undeniably contributed to intelligent stock market prediction. However, stock market prediction has not yet taken full advantage of them.},
	language = {en},
	urldate = {2024-10-04},
	journal = {Expert Systems with Applications},
	author = {Ashtiani, Matin N. and Raahemi, Bijan},
	month = may,
	year = {2023},
	pages = {119509},
}

@article{zadeh_predicting_nodate,
	title = {Predicting {Market}-{Volatility} from {Federal} {Reserve} {Board} {Meeting} {Minutes} {NLP} for {Finance}},
	language = {en},
	author = {Zadeh, Reza Bosagh and Zollmann, Andreas},
}

@article{kumar_survey_2016,
	title = {A survey of the applications of text mining in financial domain},
	volume = {114},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705116303872},
	doi = {10.1016/j.knosys.2016.10.003},
	abstract = {Text mining has found a variety of applications in diverse domains. Of late, proliﬁc work is reported in using text mining techniques to solve problems in ﬁnancial domain. The objective of this paper is to provide a state-of-the-art survey of various applications of Text mining to ﬁnance. These applications are categorized broadly into FOREX rate prediction, stock market prediction, customer relationship management (CRM) and cyber security. Since ﬁnance is a service industry, these problems are paramount in operational and customer growth aspects. We reviewed 89 research papers that appeared during the period 2000–2016, highlighted some of the issues, gaps, key challenges in this area and proposed some future research directions. Finally, this review can be extremely useful to budding researchers in this area, as many open problems are highlighted.},
	language = {en},
	urldate = {2024-10-03},
	journal = {Knowledge-Based Systems},
	author = {Kumar, B. Shravan and Ravi, Vadlamani},
	month = dec,
	year = {2016},
	pages = {128--147},
}

@article{johansen_crashes_2000,
	title = {Crashes as critical points},
	volume = {03},
	issn = {0219-0249},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0219024900000115},
	doi = {10.1142/S0219024900000115},
	abstract = {We study a rational expectation model of bubbles and crashes. The model has two components: (1) our key assumption is that a crash may be caused by local self-reinforcing imitation between noise traders. If the tendency for noise traders to imitate their nearest neighbors increases up to a certain point called the "critical" point, all noise traders may place the same order (sell) at the same time, thus causing a crash. The interplay between the progressive strengthening of imitation and the ubiquity of noise is characterized by the hazard rate, i.e. the probability per unit time that the crash will happen in the next instant if it has not happened yet. (2) Since the crash is not a certain deterministic outcome of the bubble, it remains rational for traders to remain invested provided they are compensated by a higher rate of growth of the bubble for taking the risk of a crash. Our model distinguishes between the end of the bubble and the time of the crash: the rational expectation constraint has the specific implication that the date of the crash must be random. The theoretical death of the bubble is not the time of the crash because the crash could happen at any time before, even though this is not very likely. The death of the bubble is the most probable time for the crash. There also exists a finite probability of attaining the end of the bubble without crash. Our model has specific predictions about the presence of certain critical log-periodic patterns in pre-crash prices, associated with the deterministic components of the bubble mechanism. We provide empirical evidence showing that these patterns were indeed present before the crashes of 1929, 1962 and 1987 on Wall Street and the 1997 crash on the Hong Kong Stock Exchange. These results are compared with statistical tests on synthetic data.},
	number = {02},
	urldate = {2024-10-03},
	journal = {International Journal of Theoretical and Applied Finance},
	author = {Johansen, Anders and Ledoit, Olivier and Sornette, Didier},
	month = apr,
	year = {2000},
	note = {Publisher: World Scientific Publishing Co.},
	pages = {219--255},
}

@article{feigenbaum_discrete_1996,
	title = {Discrete scale invariance in stock markets before crashes},
	volume = {10},
	issn = {0217-9792},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S021797929600204X},
	doi = {10.1142/S021797929600204X},
	abstract = {We propose a picture of stock market crashes as critical points in a system with discrete scale invariance. The critical exponent is then complex, leading to log-periodic fluctuations in stock market indexes. We present “experimental” evidence in favor of this prediction. This picture is in the spirit of the known earthquake-stock market analogy and of recent work on log-periodic fluctuations associated with earthquakes.},
	number = {27},
	urldate = {2024-10-02},
	journal = {International Journal of Modern Physics B},
	author = {Feigenbaum, James A. and Freund, Peter G.o.},
	month = dec,
	year = {1996},
	note = {Publisher: World Scientific Publishing Co.},
	pages = {3737--3745},
}

@article{sornette_stock_1996,
	title = {Stock {Market} {Crashes}, {Precursors} and {Replicas}},
	volume = {6},
	copyright = {Les Editions de Physique},
	issn = {1155-4304, 1286-4862},
	url = {http://dx.doi.org/10.1051/jp1:1996135},
	doi = {10.1051/jp1:1996135},
	abstract = {Journal de Physique I, Journal de Physique Archives représente une mine d informations facile à consulter sur la manière dont la physique a été publiée depuis 1872.},
	language = {en},
	number = {1},
	urldate = {2024-10-02},
	journal = {Journal de Physique I},
	author = {Sornette, Didier and Johansen, Anders and Bouchaud, Jean-Philippe},
	month = jan,
	year = {1996},
	note = {Publisher: EDP Sciences},
	pages = {167--175},
}

@article{shu_detection_2024,
	title = {Detection of financial bubbles using a log‐periodic power law singularity ( {\textless}span style="font-variant:small-caps;"{\textgreater}{LPPLS}{\textless}/span{\textgreater} ) model},
	volume = {16},
	issn = {1939-5108, 1939-0068},
	shorttitle = {Detection of financial bubbles using a log‐periodic power law singularity ( {\textless}span style="font-variant},
	url = {https://wires.onlinelibrary.wiley.com/doi/10.1002/wics.1649},
	doi = {10.1002/wics.1649},
	abstract = {This article provides a systematic review of the theoretical and empirical academic literature on the development and extension of the log-periodic power law singularity (LPPLS) model, which is also known as the Johansen–Ledoit–Sornette (JLS) model or log-periodic power law (LPPL) model. Developed at the interface of financial economics, behavioral finance and statistical physics, the LPPLS model provides a flexible and quantitative framework for detecting financial bubbles and crashes by capturing two salient empirical characteristics of price trajectories in speculative bubble regimes: the faster-than-exponential growth of price leading to unsustainable growth ending with a finite crashtime and the accelerating log-periodic oscillations. We also demonstrate the LPPLS model by detecting the recent bubble status of the S\&P 500 index between April 2020 and December 2022, during which the S\&P 500 index reaches its all-time peak at the end of 2021. We find that the strong corrections of the S\&P 500 index starting from January 2022 stem from the increasingly systemic instability of the stock market itself, while the well-known external shocks, such as the decades-high inflation, aggressive monetary policy tightening by the Federal Reserve, and the impact of the Russia/Ukraine war, may serve as sparks.},
	language = {en},
	number = {2},
	urldate = {2024-10-02},
	journal = {WIREs Computational Statistics},
	author = {Shu, Min and Song, Ruiqiang},
	month = mar,
	year = {2024},
	pages = {e1649},
}

@article{campbell_stock_1988,
	title = {Stock {Prices}, {Earnings}, and {Expected} {Dividends}},
	volume = {43},
	issn = {0022-1082},
	url = {https://www.jstor.org/stable/2328190},
	doi = {10.2307/2328190},
	abstract = {Long historical averages of real earnings help forecast present values of future real dividends. With aggregate U.S. stock market data (1871-1986), a vector-autoregressive forecast of the present value of future dividends is, for each year, roughly a weighted average of moving-average earnings and current real price, with between two thirds and three fourths of the weight on the earnings measure. We develop the implications of this for the present-value model of stock prices and for recent results that long-horizon stock returns are highly forecastable.},
	number = {3},
	urldate = {2024-10-02},
	journal = {The Journal of Finance},
	author = {Campbell, John Y. and Shiller, Robert J.},
	year = {1988},
	note = {Publisher: [American Finance Association, Wiley]},
	pages = {661--676},
}

@article{ait-sahalia_nonparametric_1998,
	title = {Nonparametric {Estimation} of {State}-{Price} {Densities} {Implicit} in {Financial} {Asset} {Prices}},
	abstract = {Implicit in the prices of traded financial assets are Arrow–Debreu prices or, with continuous states, the state-price density (SPD). We construct a nonparametric estimator for the SPD implicit in option prices and we derive its asymptotic sampling theory. This estimator provides an arbitrage-free method of pricing new, complex, or illiquid securities while capturing those features of the data that are most relevant from an asset-pricing perspective, for example, negative skewness and excess kurtosis for asset returns, and volatility “smiles” for option prices. We perform Monte Carlo experiments and extract the SPD from actual S\&P 500 option prices.},
	language = {en},
	journal = {The Journal of Finance},
	author = {Aït-Sahalia, Yacine and Lo, Andrew W},
	month = apr,
	year = {1998},
}

@article{cox_local_2005,
	title = {Local martingales, bubbles and option prices},
	volume = {9},
	copyright = {http://www.springer.com/tdm},
	issn = {0949-2984, 1432-1122},
	url = {http://link.springer.com/10.1007/s00780-005-0162-y},
	doi = {10.1007/s00780-005-0162-y},
	abstract = {In this article we are interested in option pricing in markets with bubbles. A bubble is deﬁned to be a price process which, when discounted, is a local martingale under the risk-neutral measure but not a martingale. We give examples of bubbles both where volatility increases with the price level, and where the bubble is the result of a feedback mechanism. In a market with a bubble many standard results from the folklore become false. Put-call parity fails, the price of an American call exceeds that of a European call and call prices are no longer increasing in maturity (for a ﬁxed strike). We show how these results must be modiﬁed in the presence of a bubble. It turns out that the option value depends critically on the deﬁnition of admissible strategy, and that the standard mathematical deﬁnition may not be consistent with the deﬁnitions used for trading.},
	language = {en},
	number = {4},
	urldate = {2024-09-26},
	journal = {Finance and Stochastics},
	author = {Cox, Alexander M. G. and Hobson, David G.},
	month = oct,
	year = {2005},
	pages = {477--492},
}

@article{loewenstein_rational_2000,
	title = {Rational {Equilibrium} {Asset}-{Pricing} {Bubbles} in {Continuous} {Trading} {Models}},
	volume = {91},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00220531},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022053199925899},
	doi = {10.1006/jeth.1999.2589},
	language = {en},
	number = {1},
	urldate = {2024-09-26},
	journal = {Journal of Economic Theory},
	author = {Loewenstein, Mark and Willard, Gregory A.},
	month = mar,
	year = {2000},
	pages = {17--58},
}

@article{jarrow_inferring_2021,
	title = {Inferring financial bubbles from option data},
	volume = {36},
	issn = {1099-1255},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.2862},
	doi = {10.1002/jae.2862},
	abstract = {Financial bubbles arise when the underlying asset's market price deviates from its fundamental value. Unlike other bubble tests that use time series data and assume a reduced-form price process, we infer the existence of bubbles nonparametrically using option price data. Under no-arbitrage and acknowledging data constraints, we can partially identify asset price bubbles using a cross section of European option prices. In the empirical analysis, we obtain interval estimates of price bubbles embedded in the S\&P 500 Index. The estimated index bubbles are then used to construct profitable momentum trading strategies that consistently outperform a buy-and-hold trading strategy.},
	language = {en},
	number = {7},
	urldate = {2024-09-24},
	journal = {Journal of Applied Econometrics},
	author = {Jarrow, Robert A. and Kwok, Simon S.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jae.2862},
	keywords = {asset price bubble, fundamental value, nonparametric estimation, partial identi.cation, risk-neutral probability measure, state price distribution},
	pages = {1013--1046},
}

@misc{woodman_defining_2021,
	title = {Defining, detecting and measuring asset price bubbles - {News} \& insight},
	url = {https://www.jbs.cam.ac.uk/2021/defining-detecting-measuring-asset-price-bubbles/},
	abstract = {One of the most frequently asked questions in the financial news media this year has been whether the stock market is in a 'bubble'?},
	language = {en-GB},
	urldate = {2024-09-23},
	journal = {Cambridge Judge Business School},
	author = {Woodman, Charlie},
	month = apr,
	year = {2021},
}

@techreport{hofmann_probabilistic_1999,
	title = {Probabilistic {Latent} {Semantic} {Analysis}},
	abstract = {Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.},
	number = {arXiv:1301.6705},
	urldate = {2024-08-28},
	institution = {arXiv},
	author = {Hofmann, Thomas},
	year = {1999},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{gurkaynak_econometric_2008,
	title = {Econometric {Tests} of {Asset} {Price} {Bubbles}: {Taking} {Stock}},
	volume = {22},
	issn = {1467-6419},
	shorttitle = {Econometric {Tests} of {Asset} {Price} {Bubbles}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-6419.2007.00530.x},
	doi = {10.1111/j.1467-6419.2007.00530.x},
	abstract = {Can asset price bubbles be detected? This survey of econometric tests of asset price bubbles shows that, despite recent advances, econometric detection of asset price bubbles cannot be achieved with a satisfactory degree of certainty. For each paper that finds evidence of bubbles, there is another one that fits the data equally well without allowing for a bubble. We are still unable to distinguish bubbles from time-varying or regime-switching fundamentals, while many small sample econometrics problems of bubble tests remain unresolved.},
	language = {en},
	number = {1},
	urldate = {2024-09-20},
	journal = {Journal of Economic Surveys},
	author = {Gürkaynak, Refet S.},
	year = {2008},
	keywords = {Bubbles, Econometric tests, Identification},
	pages = {166--186},
}

@book{cooper_origin_2008,
	title = {The {Origin} of {Financial} {Crises}},
	isbn = {978-0-307-47368-4},
	abstract = {In a series of disarmingly simple arguments financial market analyst George Cooper challenges the core principles of today's economic orthodoxy and explains how we have created an economy that is inherently unstable and crisis prone. With great skill, he examines the very foundations of today's economic philosophy and adds a compelling analysis of the forces behind economic crisis. His goal is nothing less than preventing the seemingly endless procession of damaging boom-bust cycles, unsustainable economic bubbles, crippling credit crunches, and debilitating inflation. His direct, conscientious, and honest approach will captivate any reader and is an invaluable aid in understanding today's economy.},
	language = {en},
	publisher = {Knopf Doubleday Publishing Group},
	author = {Cooper, George},
	month = dec,
	year = {2008},
	keywords = {Business \& Economics / Economic History, Business \& Economics / Economics / Theory, History / United States / 21st Century},
}

@book{shiller_narrative_2019,
	title = {Narrative {Economics}: {How} {Stories} {Go} {Viral} and {Drive} {Major} {Economic} {Events}},
	isbn = {978-0-691-18229-2},
	language = {en},
	urldate = {2024-09-11},
	publisher = {Princeton University Press},
	author = {Shiller, Robert J.},
	month = oct,
	year = {2019},
}

@article{fusari_testing_2020,
	title = {Testing for {Asset} {Price} {Bubbles} using {Options} {Data}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=3670999},
	doi = {10.2139/ssrn.3670999},
	abstract = {We present a new approach to identifying asset price bubbles based on options data. We estimate asset bubbles by exploiting the diﬀerential pricing between put and call options. We apply our methodology to two stock market indexes, the S\&P 500 and the Nasdaq-100, and two technology stocks, Amazon and Facebook, over the 2014-2018 sample period. We ﬁnd that, while indexes do not exhibit signiﬁcant bubbles, Amazon and Facebook show frequent and signiﬁcant bubbles. The estimated bubbles tend to be associated with large volatility, large trading volume, and earning announcement days. Since our approach can be implemented in real time, it is useful to both policy-makers and investors. As an illustration we consider two case studies: the Nasdaq dot-com bubble (between 1999 to 2002) and GameStop (between December 2020 and January 2021). In both cases we identify signiﬁcant and persistent bubbles.},
	language = {en},
	urldate = {2024-09-02},
	journal = {SSRN Electronic Journal},
	author = {Fusari, Nicola and Jarrow, Robert and Lamichhane, Sujan},
	year = {2020},
}

@article{bashchenko_deep_2020,
	title = {Deep {Learning} for {Asset} {Bubbles} {Detection}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=3531154},
	doi = {10.2139/ssrn.3531154},
	abstract = {We develop a methodology for detecting asset bubbles using a neural network. We rely on the theory of local martingales in continuous-time and use a deep network to estimate the diffusion coefﬁcient of the price process more accurately than the current estimator, obtaining an improved detection of bubbles. We show the outperformance of our algorithm over the existing statistical method in a laboratory created with simulated data. We then apply the network classiﬁcation to real data and build a zero net exposure trading strategy that exploits the risky arbitrage emanating from the presence of bubbles in the US equity market from 2006 to 2008. The proﬁtability of the strategy provides an estimation of the economical magnitude of bubbles as well as support for the theoretical assumptions relied on.},
	language = {en},
	urldate = {2024-09-02},
	journal = {SSRN Electronic Journal},
	author = {Bashchenko, Oksana and Marchal, Alexis},
	year = {2020},
}

@article{jegadeesh_word_2013,
	title = {Word power: {A} new approach for content analysis},
	volume = {110},
	issn = {0304405X},
	shorttitle = {Word power},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304405X13002328},
	doi = {10.1016/j.jfineco.2013.08.018},
	abstract = {We present a new approach for content analysis to quantify document tone. We find a significant relation between our measure of the tone of 10-Ks and market reaction for both negative and positive words. We also find that the appropriate choice of term weighting in content analysis is at least as important as, and perhaps more important than, a complete and accurate compilation of the word list. Furthermore, we show that our approach circumvents the need to subjectively partition words into positive and negative word lists. Our approach reliably quantifies the tone of IPO prospectuses as well, and we find that the document score is negatively related to IPO underpricing.},
	language = {en},
	number = {3},
	urldate = {2024-08-29},
	journal = {Journal of Financial Economics},
	author = {Jegadeesh, Narasimhan and Wu, Di},
	month = dec,
	year = {2013},
	pages = {712--729},
}

@article{ke_recent_nodate,
	title = {Recent {Advances} in {Text} {Analysis}},
	abstract = {Text analysis is an interesting research area in data science and has various applications, such as in artificial intelligence, biomedical research, and engineering. We review popular methods for text analysis, ranging from topic modeling to the recent neural language models. In particular, we review Topic-SCORE, a statistical approach to topic modeling, and discuss how to use it to analyze the Multi-Attribute Data Set on Statisticians (MADStat), a data set on statistical publications that we collected and cleaned. The application of Topic-SCORE and other methods to MADStat leads to interesting findings. For example, we identified 11 representative topics in statistics. For each journal, the evolution of topic weights over time can be visualized, and these results are used to analyze the trends in statistical research. In particular, we propose a new statistical model for ranking the citation impacts of 11 topics, and we also build a cross-topic citation graph to illustrate how research results on different topics spread to one another. The results on MADStat provide a data-driven picture of the statistical research from 1975 to 2015, from a text analysis perspective.},
	language = {en},
	author = {Ke, Zheng Tracy and Ji, Pengsheng and Jin, Jiashun and Li, Wanshan},
}

@article{tetlock_giving_2007,
	title = {Giving {Content} to {Investor} {Sentiment}: {The} {Role} of {Media} in the {Stock} {Market}},
	volume = {62},
	copyright = {© 2007 the American Finance Association},
	issn = {1540-6261},
	shorttitle = {Giving {Content} to {Investor} {Sentiment}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.2007.01232.x},
	doi = {10.1111/j.1540-6261.2007.01232.x},
	abstract = {I quantitatively measure the interactions between the media and the stock market using daily content from a popular Wall Street Journal column. I find that high media pessimism predicts downward pressure on market prices followed by a reversion to fundamentals, and unusually high or low pessimism predicts high market trading volume. These and similar results are consistent with theoretical models of noise and liquidity traders, and are inconsistent with theories of media content as a proxy for new information about fundamental asset values, as a proxy for market volatility, or as a sideshow with no relationship to asset markets.},
	language = {en},
	number = {3},
	urldate = {2024-08-29},
	journal = {The Journal of Finance},
	author = {Tetlock, Paul C.},
	year = {2007},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1540-6261.2007.01232.x},
	pages = {1139--1168},
}

@article{renault_sentiment_2020,
	title = {Sentiment analysis and machine learning in finance: a comparison of methods and models on one million messages},
	volume = {2},
	issn = {2524-6984, 2524-6186},
	shorttitle = {Sentiment analysis and machine learning in finance},
	url = {http://link.springer.com/10.1007/s42521-019-00014-x},
	doi = {10.1007/s42521-019-00014-x},
	abstract = {We use a large dataset of one million messages sent on the microblogging platform StockTwits to evaluate the performance of a wide range of preprocessing methods and machine learning algorithms for sentiment analysis in finance. We find that adding bigrams and emojis significantly improve sentiment classification performance. However, more complex and time-consuming machine learning methods, such as random forests or neural networks, do not improve the accuracy of the classification. We also provide empirical evidence that the preprocessing method and the size of the dataset have a strong impact on the correlation between investor sentiment and stock returns. While investor sentiment and stock returns are highly correlated, we do not find that investor sentiment derived from messages sent on social media helps in predicting large capitalization stocks return at a daily frequency.},
	language = {en},
	number = {1-2},
	urldate = {2024-08-27},
	journal = {Digital Finance},
	author = {Renault, Thomas},
	month = sep,
	year = {2020},
	pages = {1--13},
}

@article{renault_intraday_2017,
	title = {Intraday online investor sentiment and return patterns in the {U}.{S}. stock market},
	volume = {84},
	issn = {03784266},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378426617301589},
	doi = {10.1016/j.jbankfin.2017.07.002},
	language = {en},
	urldate = {2024-08-27},
	journal = {Journal of Banking \& Finance},
	author = {Renault, Thomas},
	month = nov,
	year = {2017},
	pages = {25--40},
}

@misc{httpshuggingfaceco_hugging_2024,
	title = {Hugging {Face} – {The} {AI} community building the future.},
	url = {https://huggingface.co/},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-08-26},
	author = {https://huggingface.co},
	month = aug,
	year = {2024},
}

@article{ghoshal_extracting_2016,
	title = {Extracting predictive information from heterogeneous data streams using {Gaussian} {Processes}},
	volume = {5},
	issn = {21585571, 21576203},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/AF-160055},
	doi = {10.3233/AF-160055},
	abstract = {Financial markets are notoriously complex environments, presenting vast amounts of noisy, yet potentially informative data. We consider the problem of forecasting ﬁnancial time series from a wide range of information sources using online Gaussian Processes with Automatic Relevance Determination (ARD) kernels. We measure the performance gain, quantiﬁed in terms of Normalised Root Mean Square Error (NRMSE), Median Absolute Deviation (MAD) and Pearson correlation, from fusing each of four separate data domains: time series technicals, sentiment analysis, options market data and broker recommendations. We show evidence that ARD kernels produce meaningful feature rankings that help retain salient inputs and reduce input dimensionality, providing a framework for sifting through ﬁnancial complexity. We measure the performance gain from fusing each domain’s heterogeneous data streams into a single probabilistic model. In particular our ﬁndings highlight the critical value of options data in mapping out the curvature of price space and inspire an intuitive, novel direction for research in ﬁnancial prediction.},
	language = {en},
	number = {1-2},
	urldate = {2024-08-27},
	journal = {Algorithmic Finance},
	author = {Ghoshal, S. and Roberts, S.},
	month = jun,
	year = {2016},
	pages = {21--30},
}

@article{divernois_stocktwits_2023,
	title = {{StockTwits} {Classiﬁed} {Sentiment} and {Stock} {Returns}},
	abstract = {We classify the sentiment of a large sample of StockTwits messages as bullish, bearish or neutral, and create a stock-aggregate daily sentiment polarity measure. Polarity is positively associated with contemporaneous stock returns. On average, polarity is not able to predict next-day stock returns. But when we condition on speciﬁc events, deﬁned as sudden peaks of message volume, polarity has predictive power on abnormal returns. Polarity-sorted portfolios illustrate the economic relevance of our sentiment measure.},
	language = {en},
	author = {Divernois, Marc-Aurele and Filipovic, Damir},
	year = {2023},
}

@article{hansen_can_2023,
	title = {Can {ChatGPT} {Decipher} {Fedspeak}?},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=4399406},
	doi = {10.2139/ssrn.4399406},
	abstract = {Abstract This paper examines the ability of large language models (LLMs) to decipher Fedspeak, the technical language used by the Federal Reserve to communicate on policy decisions. We evaluate the ability of the GPT-3.5 and GPT-4 models to correctly classify the policy stance of FOMC announcements relative to human assessment. We find that these models outperform traditional methods in classification accuracy and provide justifications akin to human rationale. Additionally, we show that the GPT-4 model can successfully perform the elaborate and non-trivial task of identifying macroeconomic shocks using the narrative approach of Romer and Romer (1989, 2023). Finally, we show preliminary evidence that market reactions to FOMC announcements have intensified following the introduction of the GPT-4 model.},
	language = {en},
	urldate = {2024-08-27},
	journal = {SSRN Electronic Journal},
	author = {Hansen, Anne Lundgaard and Kazinnik, Sophia},
	year = {2023},
}

@article{hochreiter_long_1997,
	title = {Long short-term memory},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://www.webofscience.com/api/gateway?GWVersion=2&SrcAuth=DOISource&SrcApp=WOS&KeyAID=10.1162%2Fneco.1997.9.8.1735&DestApp=DOI&SrcAppSID=EUW1ED0F842Fvh3przuas7IhGY70X&SrcJTitle=NEURAL+COMPUTATION&DestDOIRegistrantName=MIT+Press},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error now through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {English},
	number = {8},
	urldate = {2024-08-26},
	journal = {NEURAL COMPUTATION},
	author = {Hochreiter, S. and Schmidhuber, J.},
	month = nov,
	year = {1997},
	note = {Num Pages: 46
Place: Cambridge
Publisher: Mit Press
Web of Science ID: WOS:A1997YA04500007},
	keywords = {DEPENDENCIES, RECURRENT NEURAL NETWORKS},
	pages = {1735--1780},
}

@inproceedings{vaswani_attention_2017-1,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2024-08-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Ł ukasz and Polosukhin, Illia},
	year = {2017},
}

@misc{amazon_web_services_what_nodate,
	title = {What are {Large} {Language} {Models}? - {LLM} {AI} {Explained} - {AWS}},
	shorttitle = {What are {Large} {Language} {Models}?},
	url = {https://aws.amazon.com/what-is/large-language-model/},
	abstract = {Learn what Large Language Models are and why LLMs are essential. Discover its benefits and how you can use it to create new content and ideas including text, conversations, images, video, and audio.},
	language = {en-US},
	urldate = {2024-08-26},
	journal = {Amazon Web Services, Inc.},
	author = {Amazon Web Services},
}

@article{greene_crisis_2020,
	title = {A {Crisis} of {Beliefs}: {Investor} {Psychology} and {Financial} {Fragility}, by {Nicola} {Gennaioli} and {Andrei} {Shleifer}. {Princeton}, {NJ}: {Princeton} {University} {Press}, 2018. 264 pp.},
	volume = {30},
	issn = {1052-150X, 2153-3326},
	shorttitle = {A {Crisis} of {Beliefs}},
	url = {https://www.cambridge.org/core/journals/business-ethics-quarterly/article/abs/crisis-of-beliefs-investor-psychology-and-financial-fragility-by-nicola-gennaioli-and-andrei-shleifer-princeton-nj-princeton-university-press-2018-264-pp/798037C98E29552B59799D4D8ED9AE5F},
	doi = {10.1017/beq.2020.35},
	abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS1052150X20000354/resource/name/firstPage-S1052150X20000354a.jpg},
	language = {en},
	number = {4},
	urldate = {2024-08-19},
	journal = {Business Ethics Quarterly},
	author = {Greene, Catherine},
	month = oct,
	year = {2020},
	note = {Publisher: Cambridge University Press},
	pages = {613--616},
}

@techreport{publications_monetary_2020,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Monetary {Policy} and the {Management} of {Uncertainty}: {A} {Narrative} {Approach}},
	shorttitle = {Monetary {Policy} and the {Management} of {Uncertainty}},
	url = {https://papers.ssrn.com/abstract=3627721},
	abstract = {In this paper we explore how macroeconomic theory might be augmented, and the practice of monetary policy better understood, if approached through ideas from social and psychological science. A modern, inflation-targeting central bank faces ‘radical’ uncertainty both in understanding the economy and in knowing how best to communicate policy decisions to influence behaviour. We make use of narrative theory to explore these challenges, drawing on fieldwork with the Bank’s regional Agencies and conversations with staff and policy-makers. We find that the intelligence gathered from conversations with businesses is uniquely useful for both the analysis and communication of monetary policy. Such intelligence embodies knowledge about the plans which are making the future. It also provides insights into how economic agents understand the economy they are creating. These insights can help the Monetary Policy Committee to communicate its policy as a narrative the public understands and commits to. We propose further research to advance and test these ideas.},
	language = {en},
	number = {3627721},
	urldate = {2024-08-19},
	institution = {Social Science Research Network},
	author = {Publications, Bank of England and Tuckett, David and Holmes, Douglas and Pearson, Alice and Chaplin, Graeme},
	month = jun,
	year = {2020},
	doi = {10.2139/ssrn.3627721},
	keywords = {Monetary policy, central bank communication, economic knowledge, inflation-targeting, macroeconomic theory, narrative theory, uncertainty},
}

@article{nyman_news_2021,
	title = {News and narratives in financial systems: {Exploiting} big data for systemic risk assessment},
	volume = {127},
	issn = {0165-1889},
	shorttitle = {News and narratives in financial systems},
	url = {https://www.sciencedirect.com/science/article/pii/S0165188921000543},
	doi = {10.1016/j.jedc.2021.104119},
	abstract = {This paper applies algorithmic analysis to financial market text-based data to assess how narratives and sentiment might drive financial system developments. We find changes in emotional content in narratives are highly correlated across data sources and show the formation (and subsequent collapse) of exuberance prior to the global financial crisis. Our metrics also have predictive power for other commonly used indicators of sentiment and appear to influence economic variables. A novel machine learning application also points towards increasing consensus around the strongly positive narrative prior to the crisis. Together, our metrics might help to warn about impending financial system distress.},
	urldate = {2024-08-19},
	journal = {Journal of Economic Dynamics and Control},
	author = {Nyman, Rickard and Kapadia, Sujit and Tuckett, David},
	month = jun,
	year = {2021},
	keywords = {Big data, Early warning indicators, Narratives, Sentiment, Systemic risk, Text mining, Uncertainty},
	pages = {104119},
}

@techreport{shiller_narrative_2017,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Narrative {Economics}},
	url = {https://papers.ssrn.com/abstract=2896857},
	abstract = {This address considers the epidemiology of narratives relevant to economic fluctuations. The human brain has always been highly tuned towards narratives, whether factual or not, to justify ongoing actions, even such basic actions as spending and investing. Stories motivate and connect activities to deeply felt values and needs. Narratives “go viral” and spread far, even worldwide, with economic impact. The 1920-21 Depression, the Great Depression of the 1930s, the so-called “Great Recession” of 2007-9 and the contentious political-economic situation of today, are considered as the results of the popular narratives of their respective times. Though these narratives are deeply human phenomena that are difficult to study in a scientific manner, quantitative analysis may help us gain a better understanding of these epidemics in the future.},
	language = {en},
	number = {2896857},
	urldate = {2024-08-19},
	institution = {Social Science Research Network},
	author = {Shiller, Robert J.},
	month = jan,
	year = {2017},
	doi = {10.2139/ssrn.2896857},
	keywords = {2008 Financial Crisis, Bubbles, Business Cycles, Depression of 1920, Economic Fluctuations, Epidemic, Great Depression, Kermack and McKendrick, Meme, Multipliers, Post-Truth, Profiteer, SIR Model, Stock Market Crash, Story},
}

@techreport{guilleminot_new_2014,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {A new financial stress indicator: properties and conditional asset price behavior},
	shorttitle = {A new financial stress indicator},
	url = {https://papers.ssrn.com/abstract=2317321},
	abstract = {The main goal of this paper is to introduce a new financial stress indicator, signaling regime transitions from stability to turbulence. This indicator is based on the combination of a wide range of market prices of risk, properly normalized to make them comparable across markets and time periods. After describing the construction and basic properties of the indicator, we discuss the conditional behavior of a basket of liquid assets. When the risk aversion signal breaches certain thresholds, risky assets dramatically correlate and their risk rewards deteriorate. Sharpe ratios decrease and drawdowns increase. Also, at the onset of chaotic phases, standard risk metrics fail to give an adequate representation of potential losses. These findings have significant implications for asset allocation and risk management purposes.},
	language = {en},
	number = {2317321},
	urldate = {2024-08-21},
	institution = {Social Science Research Network},
	author = {Guilleminot, Benoît and Ohana, Jean-Jacques and Ohana, Steve},
	month = jan,
	year = {2014},
	doi = {10.2139/ssrn.2317321},
	keywords = {Financial stress indicators, risk measures, risk premiums, systemic risk, tactical asset allocation},
}

@article{mackinlay_event_1997,
	title = {Event {Studies} in {Economics} and {Finance}},
	volume = {35},
	issn = {0022-0515},
	url = {https://www.jstor.org/stable/2729691},
	number = {1},
	urldate = {2024-08-21},
	journal = {Journal of Economic Literature},
	author = {MacKinlay, A. Craig},
	year = {1997},
	note = {Publisher: American Economic Association},
	pages = {13--39},
}

@article{manela_news_2017,
	title = {News implied volatility and disaster concerns},
	language = {en},
	journal = {Journal of Financial Economics},
	author = {Manela, Asaf},
	year = {2017},
}

@article{schmitt_non-stationarity_2013,
	title = {Non-stationarity in financial time series: {Generic} features and tail behavior},
	volume = {103},
	issn = {0295-5075},
	shorttitle = {Non-stationarity in financial time series},
	url = {https://dx.doi.org/10.1209/0295-5075/103/58003},
	doi = {10.1209/0295-5075/103/58003},
	abstract = {Financial markets are prominent examples for highly non-stationary systems. Sample averaged observables such as variances and correlation coefficients strongly depend on the time window in which they are evaluated. This implies severe limitations for approaches in the spirit of standard equilibrium statistical mechanics and thermodynamics. Nevertheless, we show that there are similar generic features which we uncover in the empirical multivariate return distributions for whole markets. We explain our findings by setting up a random matrix model.},
	language = {en},
	number = {5},
	urldate = {2024-08-19},
	journal = {Europhysics Letters},
	author = {Schmitt, Thilo A. and Chetalova, Desislava and Schäfer, Rudi and Guhr, Thomas},
	month = sep,
	year = {2013},
	note = {Publisher: EDP Sciences, IOP Publishing and Società Italiana di Fisica},
	pages = {58003},
}

@article{breitung_when_2013,
	title = {When bubbles burst: econometric tests based on structural breaks},
	volume = {54},
	issn = {0932-5026, 1613-9798},
	shorttitle = {When bubbles burst},
	url = {https://www.webofscience.com/api/gateway?GWVersion=2&SrcAuth=DOISource&SrcApp=WOS&KeyAID=10.1007%2Fs00362-012-0497-3&DestApp=DOI&SrcAppSID=EUW1ED0BACJXetW1p5qxpkWBJycvM&SrcJTitle=STATISTICAL+PAPERS&DestDOIRegistrantName=Springer-Verlag},
	doi = {10.1007/s00362-012-0497-3},
	abstract = {Speculative bubbles have played an important role ever since in financial economics. During an ongoing bubble it is relevant for investors and policy-makers to know whether the bubble continues to grow or whether it is already collapsing. Prices are typically well approximated by a random walk in absence of bubbles, while periods of bubbles are characterised by explosive price paths. In this paper we first propose a conventional Chow-type testing procedure for a structural break from an explosive to a random walk regime. It is shown that under the null hypothesis of a mildly explosive process a suitably modified Chow-type statistic possesses a standard normal limiting distribution. Second, a monitoring procedure based on the CUSUM statistic is suggested. It timely indicates such a structural change. Asymptotic results are derived and small-sample properties are studied via Monte Carlo simulations. Finally, two empirical applications illustrate the merits and limitations of our suggested procedures.},
	language = {English},
	number = {4},
	urldate = {2024-08-19},
	journal = {STATISTICAL PAPERS},
	author = {Breitung, Joerg and Kruse, Robinson},
	month = nov,
	year = {2013},
	note = {Num Pages: 20
Place: New York
Publisher: Springer
Web of Science ID: WOS:000325476100002},
	keywords = {Mildly explosive processes, Monitoring, ROOT, Speculative bubbles, Structural breaks},
	pages = {911--930},
}

@techreport{alain_understanding_2018,
	title = {Understanding intermediate layers using linear classifier probes},
	url = {http://arxiv.org/abs/1610.01644},
	abstract = {Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as "probes", trained entirely independently of the model itself. This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model.},
	number = {arXiv:1610.01644},
	urldate = {2024-08-21},
	institution = {arXiv},
	author = {Alain, Guillaume and Bengio, Yoshua},
	month = nov,
	year = {2018},
	doi = {10.48550/arXiv.1610.01644},
	note = {arXiv:1610.01644 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{wolf_transformers_2020,
	address = {Online},
	title = {Transformers: {State}-of-the-{Art} {Natural} {Language} {Processing}},
	shorttitle = {Transformers},
	url = {https://aclanthology.org/2020.emnlp-demos.6},
	doi = {10.18653/v1/2020.emnlp-demos.6},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
	urldate = {2024-08-21},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
	editor = {Liu, Qun and Schlangen, David},
	month = oct,
	year = {2020},
	pages = {38--45},
}

@article{shapiro_measuring_2022,
	title = {Measuring news sentiment✩},
	abstract = {This paper demonstrates state-of-the-art text sentiment analysis tools while developing a new time-series measure of economic sentiment derived from economic and financial newspaper articles from January 1980 to April 2015. We compare the predictive accuracy of a large set of sentiment analysis models using a sample of articles that have been rated by humans on a positivity/negativity scale. The results highlight the gains from combining existing lexicons and from accounting for negation. We also generate our own sentiment-scoring model, which includes a new lexicon built specifically to capture the sentiment in economic news articles. This model is shown to have better predictive accuracy than existing ‘‘off-the-shelf’’ models. Lastly, we provide two applications to the economic research on sentiment. First, we show that daily news sentiment is predictive of movements of survey-based measures of consumer sentiment. Second, motivated by Barsky and Sims (2012), we estimate the impulse responses of macroeconomic variables to sentiment shocks, finding that positive sentiment shocks increase consumption, output, and interest rates and dampen inflation.},
	language = {en},
	journal = {Journal of Econometrics},
	author = {Shapiro, Adam Hale and Sudhof, Moritz and Wilson, Daniel J},
	year = {2022},
}

@techreport{dong_fnspid_2024,
	title = {{FNSPID}: {A} {Comprehensive} {Financial} {News} {Dataset} in {Time} {Series}},
	shorttitle = {{FNSPID}},
	url = {http://arxiv.org/abs/2402.06698},
	abstract = {Financial market predictions utilize historical data to anticipate future stock prices and market trends. Traditionally, these predictions have focused on the statistical analysis of quantitative factors, such as stock prices, trading volumes, inflation rates, and changes in industrial production. Recent advancements in large language models motivate the integrated financial analysis of both sentiment data, particularly market news, and numerical factors. Nonetheless, this methodology frequently encounters constraints due to the paucity of extensive datasets that amalgamate both quantitative and qualitative sentiment analyses. To address this challenge, we introduce a large-scale financial dataset, namely, Financial News and Stock Price Integration Dataset (FNSPID). It comprises 29.7 million stock prices and 15.7 million time-aligned financial news records for 4,775 S\&P500 companies, covering the period from 1999 to 2023, sourced from 4 stock market news websites. We demonstrate that FNSPID excels existing stock market datasets in scale and diversity while uniquely incorporating sentiment information. Through financial analysis experiments on FNSPID, we propose: (1) the dataset's size and quality significantly boost market prediction accuracy; (2) adding sentiment scores modestly enhances performance on the transformer-based model; (3) a reproducible procedure that can update the dataset. Completed work, code, documentation, and examples are available at github.com/Zdong104/FNSPID. FNSPID offers unprecedented opportunities for the financial research community to advance predictive modeling and analysis.},
	number = {arXiv:2402.06698},
	urldate = {2024-08-20},
	institution = {arXiv},
	author = {Dong, Zihan and Fan, Xinyu and Peng, Zhiyuan},
	month = feb,
	year = {2024},
	doi = {10.48550/arXiv.2402.06698},
	note = {arXiv:2402.06698 [q-fin]
type: article},
	keywords = {Quantitative Finance - Statistical Finance},
}

@article{kraus_decision_2017,
	title = {Decision support from financial disclosures with deep neural networks and transfer learning},
	volume = {104},
	issn = {01679236},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167923617301793},
	doi = {10.1016/j.dss.2017.10.001},
	abstract = {Company disclosures greatly aid in the process of ﬁnancial decision-making; therefore, they are consulted by ﬁnancial investors and automated traders before exercising ownership in stocks. While humans are usually able to correctly interpret the content, the same is rarely true of computerized decision support systems, which struggle with the complexity and ambiguity of natural language. A possible remedy is represented by deep learning, which overcomes several shortcomings of traditional methods of text mining. For instance, recurrent neural networks, such as long short-term memories, employ hierarchical structures, together with a large number of hidden layers, to automatically extract features from ordered sequences of words and capture highly non-linear relationships such as context-dependent meanings. However, deep learning has only recently started to receive traction, possibly because its performance is largely untested. Hence, this paper studies the use of deep neural networks for ﬁnancial decision support. We additionally experiment with transfer learning, in which we pre-train the network on a different corpus with a length of 139.1 million words. Our results reveal a higher directional accuracy as compared to traditional machine learning when predicting stock price movements in response to ﬁnancial disclosures. Our work thereby helps to highlight the business value of deep learning and provides recommendations to practitioners and executives.},
	language = {en},
	urldate = {2024-08-20},
	journal = {Decision Support Systems},
	author = {Kraus, Mathias and Feuerriegel, Stefan},
	month = dec,
	year = {2017},
	pages = {38--48},
}

@article{loughran_when_2011,
	title = {When {Is} a {Liability} {Not} a {Liability}? {Textual} {Analysis}, {Dictionaries}, and 10‐{Ks}},
	volume = {66},
	issn = {0022-1082, 1540-6261},
	shorttitle = {When {Is} a {Liability} {Not} a {Liability}?},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2010.01625.x},
	doi = {10.1111/j.1540-6261.2010.01625.x},
	abstract = {Previous research uses negative word counts to measure the tone of a text. We show that word lists developed for other disciplines misclassify common words in ﬁnancial text. In a large sample of 10-Ks during 1994 to 2008, almost three-fourths of the words identiﬁed as negative by the widely used Harvard Dictionary are words typically not considered negative in ﬁnancial contexts. We develop an alternative negative word list, along with ﬁve other word lists, that better reﬂect tone in ﬁnancial text. We link the word lists to 10-K ﬁling returns, trading volume, return volatility, fraud, material weakness, and unexpected earnings.},
	language = {en},
	number = {1},
	urldate = {2024-08-20},
	journal = {The Journal of Finance},
	author = {Loughran, Tim and Mcdonald, Bill},
	month = feb,
	year = {2011},
	pages = {35--65},
}

@article{malo_good_2014,
	title = {Good debt or bad debt: {Detecting} semantic orientations in economic texts},
	volume = {65},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	issn = {2330-1635, 2330-1643},
	shorttitle = {Good debt or bad debt},
	url = {https://asistdl.onlinelibrary.wiley.com/doi/10.1002/asi.23062},
	doi = {10.1002/asi.23062},
	abstract = {The use of robo‐readers to analyze news texts is an emerging technology trend in computational finance. Recent research has developed sophisticated financial polarity lexicons for investigating how financial sentiments relate to future company performance. However, based on experience from fields that commonly analyze sentiment, it is well known that the overall semantic orientation of a sentence may differ from that of individual words. This article investigates how semantic orientations can be better detected in financial and economic news by accommodating the overall phrase‐structure information and domain‐specific use of language. Our three main contributions are the following: (a) a human‐annotated finance phrase bank that can be used for training and evaluating alternative models; (b) a technique to enhance financial lexicons with attributes that help to identify expected direction of events that affect sentiment; and (c) a linearized phrase‐structure model for detecting contextual semantic orientations in economic texts. The relevance of the newly added lexicon features and the benefit of using the proposed learning algorithm are demonstrated in a comparative study against general sentiment models as well as the popular word frequency models used in recent financial studies. The proposed framework is parsimonious and avoids the explosion in feature space caused by the use of conventional n‐gram features.},
	language = {en},
	number = {4},
	urldate = {2024-08-20},
	journal = {Journal of the Association for Information Science and Technology},
	author = {Malo, Pekka and Sinha, Ankur and Korhonen, Pekka and Wallenius, Jyrki and Takala, Pyry},
	month = apr,
	year = {2014},
	pages = {782--796},
}

@article{kirtac_sentiment_2024,
	title = {Sentiment trading with large language models},
	volume = {62},
	issn = {15446123},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1544612324002575},
	doi = {10.1016/j.frl.2024.105227},
	abstract = {We analyse the performance of the large language models (LLMs) OPT, BERT, and FinBERT, alongside the traditional Loughran-McDonald dictionary, in the sentiment analysis of 965,375 U.S. financial news articles from 2010 to 2023. Our findings reveal that the GPT-3-based OPT model significantly outperforms the others, predicting stock market returns with an accuracy of 74.4\%. A long-short strategy based on OPT, accounting for 10 basis points (bps) in transaction costs, yields an exceptional Sharpe ratio of 3.05. From August 2021 to July 2023, this strategy produces an impressive 355\% gain, outperforming other strategies and traditional market portfolios. This underscores the transformative potential of LLMs in financial market prediction and portfolio management and the necessity of employing sophisticated language models to develop effective investment strategies based on news sentiment.},
	language = {en},
	urldate = {2024-08-20},
	journal = {Finance Research Letters},
	author = {Kirtac, Kemal and Germano, Guido},
	month = apr,
	year = {2024},
	pages = {105227},
}

@article{sohangir_big_2018,
	title = {Big {Data}: {Deep} {Learning} for financial sentiment analysis},
	volume = {5},
	issn = {2196-1115},
	shorttitle = {Big {Data}},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-017-0111-6},
	doi = {10.1186/s40537-017-0111-6},
	abstract = {Deep Learning and Big Data analytics are two focal points of data science. Deep Learning models have achieved remarkable results in speech recognition and computer vision in recent years. Big Data is important for organizations that need to collect a huge amount of data like a social network and one of the greatest assets to use Deep Learning is analyzing a massive amount of data (Big Data). This advantage makes Deep Learning as a valuable tool for Big Data. Deep Learning can be used to extract incredible information that buried in a Big Data. The modern stock market is an example of these social networks. They are a popular place to increase wealth and generate income, but the fundamental problem of when to buy or sell shares, or which stocks to buy has not been solved. It is very common among investors to have professional financial advisors, but what is the best resource to support the decisions these people make? Investment banks such as Goldman Sachs, Lehman Brothers, and Salomon Brothers dominated the world of financial advice for more than a decade. However, via the popularity of the Internet and financial social networks such as StockTwits and SeekingAlpha, investors around the world have new opportunity to gather and share their experiences. Individual experts can predict the movement of the stock market in financial social networks with the reasonable accuracy, but what is the sentiment of a mass group of these expert authors towards various stocks? In this paper, we seek to determine if Deep Learning models can be adapted to improve the performance of sentiment analysis for StockTwits. We applied several neural network models such as long short-term memory, doc2vec, and convolutional neural networks, to stock market opinions posted in StockTwits. Our results show that Deep Learning model can be used effectively for financial sentiment analysis and a convolutional neural network is the best model to predict sentiment of authors in StockTwits dataset.},
	language = {en},
	number = {1},
	urldate = {2024-08-20},
	journal = {Journal of Big Data},
	author = {Sohangir, Sahar and Wang, Dingding and Pomeranets, Anna and Khoshgoftaar, Taghi M.},
	month = dec,
	year = {2018},
	pages = {3},
}

@article{qorib_covid-19_2023,
	title = {Covid-19 vaccine hesitancy: {Text} mining, sentiment analysis and machine learning on {COVID}-19 vaccination {Twitter} dataset},
	volume = {212},
	issn = {0957-4174},
	shorttitle = {Covid-19 vaccine hesitancy},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422017407},
	doi = {10.1016/j.eswa.2022.118715},
	abstract = {In 2019 there was an outbreak of coronavirus pandemic also known as COVID-19. Many scientists believe that the pandemic originated from Wuhan, China, before spreading to other parts of the globe. To reduce the spread of the disease, decision makers encouraged measures such as hand washing, face masking, and social distancing. In early 2021, some countries including the United States began administering COVID-19 vaccines. Vaccination brought a relief to the public; it also generated a lot of debates from anti-vaccine and pro-vaccine groups. The controversy and debate surrounding COVID-19 vaccine influenced the decision of several people in either to accept or reject vaccination. Because of data limitations, social media data, collected through live streaming public tweets using an Application Programming Interface (API) search, is considered a viable and reliable resource to study the opinion of the public on Covid-19 vaccine hesitancy. Thus, this study examines 3 sentiment computation methods (Azure Machine Learning, VADER, and TextBlob) to analyze COVID-19 vaccine hesitancy. Five learning algorithms (Random Forest, Logistics Regression, Decision Tree, LinearSVC, and Naïve Bayes) with different combination of three vectorization methods (Doc2Vec, CountVectorizer, and TF-IDF) were deployed. Vocabulary normalization was threefold; potter stemming, lemmatization, and potter stemming with lemmatization. For each vocabulary normalization strategy, we designed, developed, and evaluated 42 models. The study shows that Covid-19 vaccine hesitancy slowly decreases over time; suggesting that the public gradually feels warm and optimistic about COVID-19 vaccination. Moreover, combining potter stemming and lemmatization increased model performances. Finally, the result of our experiment shows that TextBlob + TF-IDF + LinearSVC has the best performance in classifying public sentiment into positive, neutral, or negative with an accuracy, precision, recall and F1 score of 0.96752, 0.96921, 0.92807 and 0.94702 respectively. It means that the best performance was achieved when using TextBlob sentiment score, with TF-IDF vectorization and LinearSVC classification model. We also found out that combining two vectorizations (CountVectorizer and TF-IDF) decreases model accuracy.},
	urldate = {2024-08-20},
	journal = {Expert Systems with Applications},
	author = {Qorib, Miftahul and Oladunni, Timothy and Denis, Max and Ososanya, Esther and Cotae, Paul},
	month = feb,
	year = {2023},
	keywords = {Covid-19, Machine Learning, Sentiment Analysis, Twitter, Vaccine Hesitancy},
	pages = {118715},
}

@article{agaian_financial_2017,
	title = {Financial {Sentiment} {Analysis} {Using} {Machine} {Learning} {Techniques}},
	abstract = {The rise of web content has presented a great opportunity to extract indicators of investor moods directly from news and social media. Gauging this sentiment or general prevailing attitude of investors may simplify the analysis of large, unstructured textual datasets and help anticipate price developments in the market. There are several challenges in developing a scalable and effective framework for financial sentiment analysis, including: identifying useful information content, representing unstructured text in a structured format under a scalable framework, and quantifying this structured sentiment data. To address these questions, a corpus of positive and negative financial news is introduced. Various supervised machine learning algorithms are applied to gage article sentiment and empirically evaluate the performance of the proposed framework on introduced media content.},
	language = {en},
	author = {Agaian, Sarkis and Kolm, Petter},
	year = {2017},
}

@techreport{liu_roberta_2019-1,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	number = {arXiv:1907.11692},
	urldate = {2024-08-20},
	institution = {arXiv},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	doi = {10.48550/arXiv.1907.11692},
	note = {arXiv:1907.11692 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
}

@article{loughran_textual_2016,
	title = {Textual {Analysis} in {Accounting} and {Finance}: {A} {Survey}},
	volume = {54},
	issn = {0021-8456, 1475-679X},
	shorttitle = {Textual {Analysis} in {Accounting} and {Finance}},
	url = {https://www.webofscience.com/api/gateway?GWVersion=2&SrcAuth=DOISource&SrcApp=WOS&KeyAID=10.1111%2F1475-679X.12123&DestApp=DOI&SrcAppSID=EUW1ED0BACJXetW1p5qxpkWBJycvM&SrcJTitle=JOURNAL+OF+ACCOUNTING+RESEARCH&DestDOIRegistrantName=Wiley+%28Blackwell+Publishing%29},
	doi = {10.1111/1475-679X.12123},
	abstract = {Relative to quantitative methods traditionally used in accounting and finance, textual analysis is substantially less precise. Thus, understanding the art is of equal importance to understanding the science. In this survey, we describe the nuances of the method and, as users of textual analysis, some of the tripwires in implementation. We also review the contemporary textual analysis literature and highlight areas of future research.},
	language = {English},
	number = {4},
	urldate = {2024-08-19},
	journal = {JOURNAL OF ACCOUNTING RESEARCH},
	author = {Loughran, Tim and Mcdonald, Bill},
	month = sep,
	year = {2016},
	note = {Num Pages: 44
Place: Hoboken
Publisher: Wiley
Web of Science ID: WOS:000380964000007},
	keywords = {ANNUAL-REPORT READABILITY, CONFERENCE CALLS, CURRENT EARNINGS, DISCLOSURE, INFORMATION-CONTENT, MEDIA SLANT, Naive Bayes, PRESS RELEASES, SENTIMENT, TONE, VOLATILITY, Zipf's law, bag of words, cosine similarity, readability, sentiment analysis, textual analysis, word lists},
	pages = {1187--1230},
}

@inproceedings{pui_cheong_fung_stock_2003,
	title = {Stock prediction: {Integrating} text mining approach using real-time news},
	shorttitle = {Stock prediction},
	url = {https://ieeexplore.ieee.org/document/1196287},
	doi = {10.1109/CIFER.2003.1196287},
	abstract = {Mining textual documents and time series concurrently, such as predicting the movements of stock prices based on news articles, is an emerging topic in data mining society nowadays. Previous research has already suggested that the relationship between news articles and stock prices do exist. However, all of the existing approaches are concerning in mining single time series only. The interrelationships among different stocks are not well-addressed. Mining multiple time series concurrently is not only more informative but also far more challenging. Research in such a direction is lacking. In this paper, we try to explore such an opportunity and propose a systematic framework for mining multiple time series based on Efficient Market Hypothesis.},
	urldate = {2024-08-19},
	booktitle = {2003 {IEEE} {International} {Conference} on {Computational} {Intelligence} for {Financial} {Engineering}, 2003. {Proceedings}.},
	author = {Pui Cheong Fung, G. and Xu Yu, J. and Lam, Wai},
	month = mar,
	year = {2003},
	keywords = {Broadcasting, Data engineering, Data mining, Fluctuations, Frequency, Humans, Research and development management, Stock markets, Systems engineering and theory, Text mining},
	pages = {395--402},
}

@article{carta_event_2021,
	title = {Event {Detection} in {Finance} {Using} {Hierarchical} {Clustering} {Algorithms} on {News} and {Tweets}},
	volume = {7},
	doi = {10.7717/peerj-cs.438},
	abstract = {In the current age of overwhelming information and massive production of textual data on the Web, Event Detection has become an increasingly important task in various application domains. Several research branches have been developed to tackle the problem from different perspectives, including Natural Language Processing and Big Data analysis, with the goal of providing valuable resources to support decision-making in a wide variety of fields. In this paper, we propose a real-time domain-specific clustering-based event-detection approach that integrates textual information coming, on one hand, from traditional newswires and, on the other hand, from microblogging platforms. The goal of the implemented pipeline is twofold: (i) providing insights to the user about the relevant events that are reported in the press on a daily basis; (ii) alerting the user about potentially important and impactful events, referred to as hot events, for some specific tasks or domains of interest. The algorithm identifies clusters of related news stories published by globally renowned press sources, which guarantee authoritative, noise-free information about current affairs; subsequently, the content extracted from microblogs is associated to the clusters in order to gain an assessment of the relevance of the event in the public opinion. To identify the events of a day d, the algorithm dynamically builds a lexicon by looking at news articles and stock data of previous days up to d − 1. Although the approach can be extended to a variety of domains (e.g. politics, economy, sports), we hereby present a specific implementation in the financial sector. We validated our solution through a qualitative and quantitative evaluation, performed on the Dow Jones' Data, News and Analytics dataset, on a stream of messages extracted from the microblogging platform Stocktwits, and on the Standard \& Poor's 500 index time-series. The experiments demonstrate the effectiveness of our proposal in extracting meaningful information from real-world events and in spotting hot events in the financial sphere. An added value of the evaluation is given by the visual inspection of a selected number of significant real-world events, starting from the Brexit Referendum and reaching until the recent outbreak of the Covid-19 pandemic in early 2020.},
	journal = {PeerJ Computer Science},
	author = {Carta, Salvatore and Consoli, Sergio and Piras, Luca and Podda, Alessandro and Reforgiato Recupero, Diego},
	month = may,
	year = {2021},
}

@article{wang_fuzzy_2023,
	title = {Fuzzy inference-based {LSTM} for long-term time series prediction},
	volume = {13},
	copyright = {2023 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-023-47812-3},
	doi = {10.1038/s41598-023-47812-3},
	abstract = {Long short-term memory (LSTM) based time series forecasting methods suffer from multiple limitations, such as accumulated error, diminishing temporal correlation, and lacking interpretability, which compromises the prediction performance. To overcome these shortcomings, a fuzzy inference-based LSTM with the embedding of a fuzzy system is proposed to enhance the accuracy and interpretability of LSTM for long-term time series prediction. Firstly, a fast and complete fuzzy rule construction method based on Wang–Mendel (WM) is proposed, which can enhance the computational efficiency and completeness of the WM model by fuzzy rules simplification and complement strategies. Then, the fuzzy prediction model is constructed to capture the fuzzy logic in data. Finally, the fuzzy inference-based LSTM is proposed by integrating the fuzzy prediction fusion, the strengthening memory layer, and the parameter segmentation sharing strategy into the LSTM network. Fuzzy prediction fusion increases the network reasoning capability and interpretability, the strengthening memory layer strengthens the long-term memory and alleviates the gradient dispersion problem, and the parameter segmentation sharing strategy balances processing efficiency and architecture discrimination. Experiments on publicly available time series demonstrate that the proposed method can achieve better performance than existing models for long-term time series prediction.},
	language = {en},
	number = {1},
	urldate = {2024-08-14},
	journal = {Scientific Reports},
	author = {Wang, Weina and Shao, Jiapeng and Jumahong, Huxidan},
	month = nov,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Energy science and technology, Environmental sciences, Mathematics and computing},
	pages = {20359},
}

@article{fischer_deep_2018,
	title = {Deep learning with long short-term memory networks for financial market predictions},
	volume = {270},
	issn = {03772217},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221717310652},
	doi = {10.1016/j.ejor.2017.11.054},
	language = {en},
	number = {2},
	urldate = {2024-08-15},
	journal = {European Journal of Operational Research},
	author = {Fischer, Thomas and Krauss, Christopher},
	month = oct,
	year = {2018},
	pages = {654--669},
}

@article{bollen_twitter_2011,
	title = {Twitter mood predicts the stock market},
	volume = {2},
	issn = {18777503},
	url = {http://arxiv.org/abs/1010.3003},
	doi = {10.1016/j.jocs.2010.12.007},
	abstract = {Behavioral economics tells us that emotions can profoundly affect individual behavior and decision-making. Does this also apply to societies at large, i.e. can societies experience mood states that affect their collective decision making? By extension is the public mood correlated or even predictive of economic indicators? Here we investigate whether measurements of collective mood states derived from large-scale Twitter feeds are correlated to the value of the Dow Jones Industrial Average (DJIA) over time. We analyze the text content of daily Twitter feeds by two mood tracking tools, namely OpinionFinder that measures positive vs. negative mood and Google-Proﬁle of Mood States (GPOMS) that measures mood in terms of 6 dimensions (Calm, Alert, Sure, Vital, Kind, and Happy). We cross-validate the resulting mood time series by comparing their ability to detect the public’s response to the presidential election and Thanksgiving day in 2008. A Granger causality analysis and a Self-Organizing Fuzzy Neural Network are then used to investigate the hypothesis that public mood states, as measured by the OpinionFinder and GPOMS mood time series, are predictive of changes in DJIA closing values. Our results indicate that the accuracy of DJIA predictions can be signiﬁcantly improved by the inclusion of speciﬁc public mood dimensions but not others. We ﬁnd an accuracy of 87.6\% in predicting the daily up and down changes in the closing values of the DJIA and a reduction of the Mean Average Percentage Error by more than 6\%.},
	language = {en},
	number = {1},
	urldate = {2024-08-14},
	journal = {Journal of Computational Science},
	author = {Bollen, Johan and Mao, Huina and Zeng, Xiao-Jun},
	month = mar,
	year = {2011},
	note = {arXiv:1010.3003 [physics]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computational Engineering, Finance, and Science, Computer Science - Social and Information Networks, Physics - Physics and Society},
	pages = {1--8},
}

@article{lefort_mixing_2024,
	title = {Mixing {Financial} {Stress} with {GPT}-4 {News} {Sentiment} {Analysis} for {Optimal} {Risk}-{On}/{Risk}-{Off} {Decisions}},
	abstract = {This paper introduces a new risk-on risk-off strategy for the stock market, which combines a financial stress indicator with a sentiment analysis done by ChatGPT reading and interpreting Bloomberg daily market summaries. Forecasts of market stress derived from volatility and credit spreads are enhanced when combined with the financial news sentiment derived from GPT4. As a result, the strategy shows improved performance, evidenced by higher Sharpe ratio and reduced maximum drawdowns. The improved performance is consistent across the NASDAQ, the S\&P 500 and the six major equity markets, indicating that the method generalizes across equities markets.},
	language = {en},
	author = {Lefort, Baptiste and Benhamou, Eric and Ohana, Jean-Jacques and Saltiel, David and Guez, Beatrice and Jacquot, Thomas},
	year = {2024},
}

@article{kumbure_machine_2022,
	title = {Machine learning techniques and data for stock market forecasting: {A} literature review},
	volume = {197},
	issn = {09574174},
	shorttitle = {Machine learning techniques and data for stock market forecasting},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417422001452},
	doi = {10.1016/j.eswa.2022.116659},
	abstract = {In this literature review, we investigate machine learning techniques that are applied for stock market prediction. A focus area in this literature review is the stock markets investigated in the literature as well as the types of variables used as input in the machine learning techniques used for predicting these markets. We examined 138 journal articles published between 2000 and 2019. The main contributions of this review are: (1) an extensive examination of the data, in particular, the markets and stock indices covered in the predictions, as well as the 2173 unique variables used for stock market predictions, including technical indicators, macroeconomic variables, and fundamental indicators, and (2) an in-depth review of the machine learning techniques and their variants deployed for the predictions. In addition, we provide a bibliometric analysis of these journal articles, highlighting the most influential works and articles.},
	language = {en},
	urldate = {2024-08-13},
	journal = {Expert Systems with Applications},
	author = {Kumbure, Mahinda Mailagaha and Lohrmann, Christoph and Luukka, Pasi and Porras, Jari},
	month = jul,
	year = {2022},
	pages = {116659},
}

@article{mokhtari_effectiveness_2021,
	title = {Effectiveness of {Artificial} {Intelligence} in {Stock} {Market} {Prediction} based on {Machine} {Learning}},
	volume = {183},
	issn = {09758887},
	url = {http://www.ijcaonline.org/archives/volume183/number7/mokhtari-2021-ijca-921347.pdf},
	doi = {10.5120/ijca2021921347},
	abstract = {This paper tries to address the problem of stock market prediction leveraging artificial intelligence (AI) strategies. The stock market prediction can be modeled based on two principal analyses called technical and fundamental. In the technical analysis approach, the regression machine learning (ML) algorithms are employed to predict the stock price trend at the end of a business day based on the historical price data. In contrast, in the fundamental analysis, the classification ML algorithms are applied to classify the public sentiment based on news and social media. In the technical analysis, the historical price data is exploited from Yahoo Finance, and in fundamental analysis, public tweets on Twitter associated with the stock market are investigated to assess the impact of sentiments on the stock market’s forecast. The results show a median performance, implying that with the current technology of AI, it is too soon to claim AI can beat the stock markets.},
	language = {en},
	number = {7},
	urldate = {2024-08-13},
	journal = {International Journal of Computer Applications},
	author = {Mokhtari, Sohrab and Yen, Kang K. and Liu, Jin},
	month = jun,
	year = {2021},
	pages = {1--8},
}

@article{copyright_how_2024,
	title = {How can we use {ChatGPT} better: {A} research of {API}-{Enhanced} {ChatGPT} in stock prediction},
	abstract = {In this study, we propose an API-enhanced ChatGPT structure that incorporates stock price and news data to improve stock price movement predictions. By integrating external data sources and prompt engineering techniques, our approach demonstrates a significant improvement in predictive performance compared to using only stock price data. The inclusion of news data alongside stock prices results in an approximately 10\% increase in accuracy and F1 scores, as well as a 20\% improvement in risk-adjusted returns, as measured by Sharpe ratios and information ratios. Our findings highlight the potential of leveraging conversational AI and large language models for stock market analysis, while also identifying areas for further research and optimization, such as addressing stock-specific challenges and developing cost-effective strategies for implementation. This study contributes to the limited body of literature on the application of large language models in finance and paves the way for future research in enhancing the capabilities of AI-driven investment decision-making tools.},
	language = {en},
	author = {Copyright, Qichang Zheng},
	year = {2024},
}
