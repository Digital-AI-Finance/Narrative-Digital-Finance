\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\usepackage{enumitem}
\setlist[enumerate]{label*=\arabic*.}
% \usepackage{natbib}
\usepackage[backend=biber, style=apa,natbib=true]{biblatex}
\addbibresource{Systematic Literature review/SLR_references.bib}

\title{Response to Reviewers}
\date{}

\begin{document}
\maketitle

\section*{General Statement}

We thank the Editor and all reviewers for their thorough and constructive comments.  
We have carefully revised the manuscript and believe that the changes have substantially improved clarity, methodological transparency, and overall contribution.  
All modifications are directly integrated into the revised manuscript.  
Below, we respond to each comment point by point.

\bigskip

\section*{Response to Reviewer 1}

\textbf{Overall Comment.}
This manuscript presents a novel and methodologically rigorous framework for conducting algorithmic systematic literature reviews (SLRs), applied to the emerging field of financial narratives. The integration of transformer-based NLP models, clustering algorithms, and interpretability tools demonstrates both technical sophistication and practical utility. The use of a real-world case study focused on financial narratives adds strong empirical grounding to the proposed method.

The paper is clearly written, well-structured, and offers significant contributions to both literature review methodology and financial economics. The distinction between narrative understanding and narrative modeling in the results section is particularly insightful, and the authors effectively highlight the conceptual and methodological fragmentation in current research.

Strengths:
\begin{itemize}
    \item Strong methodological innovation through the use of embedding models and clustering.
    \item Clear and well-executed application to a topical subject area in finance.
    \item Good balance between technical detail and conceptual analysis.
\end{itemize}

Overall, this is an impressive and timely contribution. I recommend minor revisions to enhance clarity and breadth of impact.

\textbf{Response.}
We sincerely thank the reviewer for the positive and encouraging assessment. We are pleased that the methodological contribution, the application to financial narratives, and the distinction between narrative understanding and narrative modeling were well received. Following the reviewer’s recommendation, we made several clarifications throughout the manuscript to improve readability and broaden the practical relevance of the proposed framework. All revisions are highlighted in the manuscript.

\medskip

\textbf{Comment 1.}  
Consider including a more explicit comparison with traditional manual SLRs to better highlight the advantages of the algorithmic approach.

\textbf{Response.}
We appreciate this suggestion and have revised the manuscript to draw a clearer contrast between traditional manual SLRs and the proposed algorithmic framework. Two changes were made.

First, Section~2 (Methodology) now explains more explicitly why manual SLRs remain biased and with limited reproducibility, and we compare our approach to prior algorithmic or bibliometric frameworks. The revised paragraph (lines 136–155) clarifies that earlier automated approaches relied primarily on lexical similarity, whereas the present framework incorporates transformer-based semantic embeddings, dimensionality reduction, and quantitative clustering evaluation.

Second, a dedicated subsection was added in the Discussion (Section~4, “Methodological Contribution”, lines 316–346). This subsection outlines how the framework improves reproducibility, transparency, and selection quality compared with manual SLRs. It also explains the role of semantic relevance assessment and quantitative quality metrics such as relevance scores, Silhouette values, and selected paper counts.

\medskip

\textbf{Comment 2.}  
It would be helpful to further elaborate on potential limitations of the method, such as the exclusion of gray literature or papers without accessible full texts.

\textbf{Response.}  
Thank you for this suggestion. We have expanded the discussion of methodological limitations and clarified their practical implications. These revisions appear in Section~4, subsection “Limitations” (lines 324–372).

The updated text now (i) explains the consequences of relying solely on Scopus, including the exclusion of gray literature and early-stage research; (ii) addresses the bias introduced by discarding papers without accessible full texts; and (iii) discusses limitations related to the embedding–clustering pipeline, including model dependence, pre-training biases, and the implications of excluding medium-relevance clusters. We also added a short discussion of model choice, noting that bidirectional encoders may adapt more smoothly to new terminology than autoregressive models, and we outline how hybrid manual–automatic review of medium-relevance clusters could mitigate over-exclusion. This revised section also clarifies the trade-offs between open-source and closed-source models, and highlights computational constraints that arise when scaling the review to very large corpora.

\medskip

\textbf{Comment 3.}  
Consider elaborating on the practical challenges of implementing this pipeline at scale (e.g., computational cost, access to full texts).

\textbf{Response.}  
Thank you for raising this point. We expanded the discussion of practical constraints in Section~4, “Limitations” (lines 348–372). The revised text now clarifies the computational requirements of scaling the pipeline, including the cost of embedding large corpora, the potential need for parallelization and advanced computation resources, and the effect of model choice on runtime and reproducibility. We also address the issue of restricted full-text access, explaining how the necessity of excluding inaccessible documents may introduce selection bias and how hybrid data-sourcing strategies (e.g., combining Scopus with open repositories) can mitigate this constraint.


\newpage
\section*{Response to Reviewer 2}

\textbf{Overall Comment.}
This manuscript presents a semi-automated framework for conducting systematic literature reviews (SLRs) using NLP tools such as transformer-based embeddings, cosine similarity scoring, PCA, and clustering. The authors apply this methodology to financial narratives to address two research questions.
1.	How can NLP and textual analysis techniques be used to quantify and model financial narratives
2.	Can financial narrative modeling enhance the understanding of financial market dynamics?

The methodology is promising for literature review automation; however, in its current application, it does not address the stated research questions. A future version applying this framework to genuine financial narratives and empirically testing their relationship with market dynamics would be far more suitable for Financial Innovation.

\textbf{Response.}  
We thank the reviewer for the constructive feedback. The concern regarding the alignment between the stated research questions and the empirical contribution has been carefully addressed in the revised manuscript, by clarifying the overall scope of the paper.

The purpose of the review is to synthesize how the academic literature defines, measures, and models financial narratives, not to conduct new empirical tests on narrative effects. Section~1 and the revised discussion now make this more explicit. The two research questions have been reframed as questions about the state of the literature, and the abstract, introduction, and contributions paragraph have been updated to reflect this clarification.

We also highlight why this focus remains relevant for \textit{Financial Innovation}: understanding the methodological landscape of narrative modeling is a prerequisite for any empirical application linking narratives to market behaviour. The systematic review could therefore provides the conceptual and methodological map needed before implementing the framework on primary narrative data.

\medskip

\textbf{Comment 1.}  
Introduction:

Although the technical framework for literature selection is well-structured, there is a fundamental misalignment between the stated research questions and the actual methodology and data. This study does not extract or analyze real-world financial narratives, instead relying solely on Scopus-indexed academic publications discussing the topic.

Consequently, the second research question was not empirically examined, and the case study did not generate new evidence on financial market dynamics. Given Financial Innovation’s emphasis on original empirical and methodological contributions, the manuscript does not meet the journal’s expectations in its present form.

\textbf{Response.}  
We thank the reviewer for this important observation. The issue of misalignment between the initial research questions and the scope of the empirical work has been addressed throughout the revised manuscript. To clarify the intent of the study, we have rewritten the abstract and introduction, as well as refrmaed the research questions so that the aims of the paper are stated explicitly.

First, the abstract (line 79) has been rewritten so that the paper is clearly presented as a systematic literature review supported by an algorithmic framework, rather than an empirical analysis of narrative data. The opening paragraph now states that the paper presents a systematic literature review on financial narratives, supported (rather than driven) by an algorithmic framework.
Second, the introduction has been revised (lines 99–115) to clarify the purpose of the study and to state explicitly that the contribution lies in synthesizing how the academic literature defines, measures, and models financial narratives.
Lastly, the research questions were reformulated to reflect that the contribution of the paper lies in synthesizing how the academic literature defines, measures, and models financial narratives, rather than extracting narratives from primary data or examining their direct impact on financial markets.

\medskip

\textbf{Comment 2.}  
Contribution:

\begin{itemize}
    \item Potential Value: The proposed NLP-based pipeline for automating SLRs is a promising tool for bibliometric studies and can be applied across multiple domains.
    \item Limitations of the contribution:
    \begin{itemize}
        \item This study does not provide original insights into financial narratives as behavioral economic phenomena.
        \item The dataset (titles, abstracts, and keywords from Scopus articles) is meta-level discourse, not primary narrative content from financial markets.
        \item The paper’s framing suggests direct narrative modeling, which was not performed.
    \end{itemize}
    \item Missed Opportunity: Applying the framework to genuine financial textual data (e.g., news, earnings calls, social media) could yield meaningful results aligned with the research questions.
\end{itemize}

\textbf{Response.}  
We thank the reviewer for raising this important point. We agree that the original framing may have suggested that the paper aimed to extract or model financial narratives directly, which was not the intention of the study. To remove this ambiguity, we revised both the abstract and the introduction so that the contribution is clearly articulated as a systematic, algorithmic review of how the academic literature defines, measures, and models financial narratives. The revised research questions now explicitly focus on synthesising methodological practices rather than analysing primary narrative data.

In addition to the details provided in the previous response, the introduction makes clear that the dataset consists exclusively of Scopus-indexed academic publications and that our objective is to document how prior studies conceptualise narratives, which NLP methods they employ, and how these methods have evolved with the introduction of transformer models. The discussion has also been updated to emphasise that the contribution lies in providing a structured overview of existing approaches and in presenting an algorithmic framework that improves reproducibility and consistency in systematic reviews.

We fully agree that applying the framework to primary financial texts such as news articles, earnings calls, or regulatory communications would produce valuable empirical insights. This is a separate line of research that we are currently developing, and the present paper is intended to provide the systematic foundation upon which such empirical applications can be built.

\medskip

\textbf{Comment 3.}  
Execution:

\begin{itemize}
    \item Strengths:
    \begin{itemize}
        \item A clear explanation of each stage in the SLR pipeline.
        \item Integration of modern NLP techniques (embedding models, dimensionality reduction, and clustering).
        \item Transparent keyword search and inclusion/exclusion criteria.
    \end{itemize}
    \item Methodological Issues:
    \begin{itemize}
        \item Research Question 2 Not Directly Addressed: The enhancement of market dynamics understanding is only discussed via prior literature and not tested with original modeling.
        \item Data–Object Mismatch: Financial narratives are not retrieved or modeled; instead, academic discussions about them are analyzed.
        \item Single-Source Limitation: Exclusive reliance on Scopus may omit key works from SSRN, NBER, arXiv, and other sources.
        \item Subjectivity in Inclusion Criteria: Relevance scoring is based on researcher-defined statements; no validation set or robustness check was provided.
        \item Exclusion of Medium-Relevance Papers: Potentially useful studies may have been omitted for the sake of automation.
    \end{itemize}
    \item Implication: These issues cannot be solved with minor edits; the manuscript requires a substantial redesign with a new dataset of actual financial narratives to fulfill its stated aims.
\end{itemize}

\textbf{Response.}  
We appreciate the reviewer’s detailed assessment and agree that the original framing may have contributed to a misunderstanding of the manuscript’s objectives. The purpose of the paper is not to extract or model financial narratives directly, but to conduct a systematic review of how existing studies conceptualise and operationalise them. To avoid ambiguity, we revised both the abstract and the introduction so that the goals of the study and the nature of the dataset are clearly stated. The research questions were reformulated accordingly and now explicitly concern the methods used in the literature, rather than empirical narrative extraction or market prediction.

Regarding the data–object distinction, the methodology section now clarifies that the corpus consists solely of Scopus-indexed academic publications and that the analysis addresses how prior research defines and models narratives. This reframing resolves the perceived mismatch between the stated aims and the type of data used.

The concern about source coverage has been addressed in two complementary ways. First, Section 2.1 now explains why Scopus was selected, specifying its metadata reliability, peer-review curation standards, and accessible API, while acknowledging that this choice excludes part of the gray literature. Second, the limitations section has been expanded to discuss the implications of relying on a single database and to outline how future extensions could integrate open bibliometric sources such as OpenAlex to widen coverage.

The reviewer also raises questions about subjectivity in inclusion criteria. This issue is explicitly treated in Sections 2.2 and 3.4, where we explain how paraphrased variants of the research statements were generated and averaged to reduce sensitivity to wording and mitigate the impact of researcher-defined phrasing. Because this mitigation is embedded directly in the pipeline, additional robustness checks on wording were not performed.

The exclusion of medium-relevance papers is now discussed both in the methodology and the limitations. We clarify that medium-level papers tended to lie outside the scope during manual inspection and that their exclusion ensures procedural reproducibility. We also acknowledge that a semi-automatic variant, where medium-relevance papers are reviewed under transparent inclusion rules, could be used when interpretative flexibility is desired.

Finally, the reviewer notes that fully addressing the originally stated RQ2 would have required empirical modeling of primary financial texts. Since this is beyond the intended scope of the systematic review, we revised the introduction, abstract, and research questions so that the aims are aligned with methodological synthesis rather than empirical financial modeling. This adjustment resolves the perceived need for a substantial redesign.

\medskip

\textbf{Comment 4.}  
Exposition:

\begin{itemize}
    \item Clarity: The paper is well written, logically structured, and easy to follow. The workflow is effectively illustrated, and the tables and figures are informative.
    \item Framing Issues: The title, abstract, and research questions suggest empirical modeling of narratives, which is misleading, given the data and methods used.
    \item Recommendations for Improvement:
    \begin{itemize}
        \item Clearly distinguish between financial narratives and the scholarly literature on financial narratives.
        \item Reframe the case study to reflect its true scope (meta-analysis of the literature) or redesign the study to analyze primary financial narrative data.
        \item Explicitly acknowledge the limitations of using Scopus-only academic data to answer behavioral finance questions.
    \end{itemize}
\end{itemize}

\textbf{Response.}  
We thank the reviewer for the positive assessment of the exposition and for highlighting the framing issues. The comments were particularly helpful in clarifying how the original abstract and research questions could create the impression that the study involved empirical modeling of narrative data. To resolve this ambiguity, we revised the abstract, introduction, and both research questions so that the scope of the paper is defined explicitly as a systematic literature review of academic work on financial narratives, supported by an algorithmic selection framework.

To strengthen the distinction between narratives as objects of analysis and the scholarly discourse surrounding them, the introduction now states clearly that the dataset consists solely of academic publications and that the purpose of the review is to map conceptual and methodological practices in the field. We also revised the opening of Section 3 to make it explicit that this section reports the classification and synthesis of the academic literature rather than any empirical narrative extraction. The revised introduction to Section 3 explains how the literature is organised into two groups and sets clear expectations about what the reader will find in the results.

The reviewer is correct that using Scopus as the sole source limits the breadth of coverage, especially for early-stage or fast-moving research. This limitation is now acknowledged explicitly in Section 4, where we discuss the implications of this choice and outline how open bibliometric sources such as OpenAlex could be incorporated in future extensions of the framework.

\medskip



\newpage
\section*{Response to Reviewer 3}

\textbf{Overall Comment.}
This manuscript, "An Algorithmic Framework for Systematic Literature Reviews: A Case Study for Financial Narratives" proposes a methodological pipeline for SLRs that combines NLP sentence embeddings, dimensionality reduction, clustering, and interpretability, and applies it to the emerging "financial narratives" literature; it frames two clear research questions on how NLP can quantify/model narratives and whether such modeling improves understanding of market dynamics. The methods include the comparison of multilingual-e5 vs. OpenAI embeddings and a three-stage selection workflow.

[...] Even though several aspects of the manuscript obscure its contribution, I confirm that the paper make a methodological contribution to the financial narratives' literature. Addressing the points made [bellow] would substantially improve clarity and rigor of the paper.

\textbf{Response.}  
We thank the reviewer for the thorough evaluation and for recognising the methodological contribution of the paper. The comments provided clear guidance on how to improve the framing, clarity, and presentation of the work. In the revised manuscript, we clarified the purpose and scope of the study, refined the research questions, and strengthened the methodological exposition to reduce the ambiguity noted in the original version. All reviewer points have been addressed in detail below, and corresponding revisions have been implemented in the manuscript.

\medskip

\textbf{Comment 1.}  
There is a real aim mismatch in introduction section. The "Introduction" foregrounds narratives' role in investor behavior/market dynamics (a domain claim) but the core contribution is an algorithmic SLR framework applied to papers about narratives (a methods claim). That's not fatal, but it needs alignment. Author/s should (1) Reframe the opening as brief background (why narratives matter) and immediately pivot to the SLR problem the paper solves; (2) State the primary contribution plainly ("a reproducible, automated SLR pipeline") and the secondary; (3) Bridge research questions, methods, results in a short "Contribution \& Scope" paragraph so readers don't expect new decision-making evidence.

\textbf{Response.}  
We appreciate this observation and agree that the original introduction created an expectation of empirical narrative modeling rather than a methodological SLR. To address this issue, we substantially revised the framing of the paper.

First, the introduction was rewritten to provide a concise background on why narratives matter in economics and finance, followed by an immediate pivot to the problem the paper addresses: the lack of conceptual clarity and methodological consistency in the existing literature. We now state explicitly that the primary contribution is a SLR synthesizes the financial narrative literature is the application domain through which the framework is demonstrated. The secondary contribution of the paper is the reproducible, automated SLR pipeline.

Second, the research questions were reformulated so they clearly target how the literature defines and models narratives, rather than suggesting any direct empirical extraction of narratives from market data. This re-alignment is explained at the end of the introduction, ensuring that readers do not expect new evidence on decision-making or market behaviour.

Finally, we also introduced a short “Contribution and Scope” paragraph within the introduction, as well as a "Methodological Contribution" subsection within the "Discussion" section, that explicitly distinguish between (i) the methodological contribution (the algorithmic SLR pipeline) and (ii) the substantive contribution (a structured synthesis of how financial narratives are conceptualized and modeled in academic research). These modifications clarifies that the paper does not analyse primary narrative data, and that its findings should be read as a systematic sythesis of existing methods and definitions.

\medskip

\textbf{Comment 2.}  
Author/s seem that they used AI to prepare their literature review and it seems that they even did not read those papers. It also seems that they also made use of AI heavily (at least Sections 3.2, 3.3 and abstract are detected as AI).

\textbf{Response.}  
We thank the reviewer for raising this concern. We would like to clarify that all papers included in the review were read, screened, and annotated manually. The synthesis in Section~3 is based on detailed reading notes that we compiled for each study, and which informed both the conceptual categorization and the methodological analysis. To document this process transparently, we will include the complete set of reading notes as a supplementary Excel file, corresponding directly to the entries reported in Appendix Table 3.

We acknowledge that, as with most contemporary academic writing, language models were used in a limited manner to assist with stylistic refinement (for example, improving sentence clarity and correcting minor grammatical issues). They were not used to generate substantive content, summarize papers, or replace manual analysis. All conceptual interpretation, methodological classification, and extraction of study characteristics were performed by the authors.

\medskip

\textbf{Comment 2.a.}  
P1-line 56: Although there are many highly cited articles on the criticisms of efficient market hypothesis, they only mentioned one of them, which was "Grossman (1980)". But the problem is that Grossman and Stiglitz developed this article and then they published the latest version as "Grossman \& Stiglitz (1980), "On the Impossibility of Informationally Efficient Markets", American Economic Review, 70(3):393-408. Furthermore, author/s suggests Grossman (1980) "aligns more closely with the semi-strong EMH … allowing delayed diffusion." Actually, Grossman and Stiglitz argue that perfectly informationally efficient markets cannot exist because costly information must be rewarded. This is a paradox against fully revealing efficiency, not an endorsement of semi-strong EMH. That is why authors should rephrase this sentence as "Grossman-Stiglitz (1980) show that prices cannot be perfectly informationally efficient when information is costly".

\textbf{Response.}  
thank you for the feedback regarding the contribution of \textcite{grossman_impossibility_1980}. Our original phrasing did not accurately reflect the Grossman–Stiglitz argument. We have revised the relevant passage in the introduction to state clearly that the paper demonstrates the impossibility of perfectly informationally efficient markets when information is costly, rather than suggesting alignment with semi-strong efficiency. The revised text now reads: "\textcite{grossman_impossibility_1980} show that when information is costly, prices cannot be perfectly informationally efficient, since fully revealing markets would remove incentives to acquire information.” This correction aligns the discussion with the established interpretation of their paradox and improves the logic of the introduction.

\medskip

\textbf{Comment 2.b.}  
P2-Line 59: they cite Varsha et. al. (2024) article as "S et. al. (2024)" both in the text and in references.

\textbf{Response.}  
We thank the reviewer for flagging this citation issue. The in-text citation and the corresponding entry in the reference list have been corrected to properly cite \textcite{varsha_how_2024}. The revised version now reads: “This structured framework, initially based on \textcite{varsha_how_2024}, starts by defining the research problem and formulating clear research questions.”

\medskip

\textbf{Comment 2.c.}  
There are important, highly-cited papers on (economic/financial) narratives not in your references that they should add: (i) Shiller (2017), American Economic Review (Papers \& Proceedings) — "Narrative Economics." Canonical framing that virtually all later empirical "financial narratives" papers reference. ; (ii) Bybee, Kelly \& Su (2023), Review of Financial Studies — "Narrative Asset Pricing: Interpretable Systematic Risk Factors from News Text." ; (iii) Bybee, Kelly, Manela \& Xiu (2024), Journal of Finance — "Business News and Business Cycles." ; Flynn \& Sastry (2024), NBER Working Paper — "The Macroeconomics of Narratives." Formalizes viral, belief-altering narratives in a business-cycle model and measures narratives in firms' 10-Ks.

\textbf{Response.}  
We appreciate this observation and fully agree that these publications are essential to any comprehensive overview of financial narratives. They represent some of the most relevant recent contributions to the field and capture the latest developments in how narratives are conceptualized and empirically examined in finance. We have therefore incorporated all four suggested works into the introduction and revised the surrounding discussion to reflect their significance.

The revised text now situates the field more clearly within the established narrative economics literature, highlights the empirical contributions of \textcite{bybee_narrative_2023, bybee_business_2024}, and integrates the theoretical perspective introduced by \textcite{flynn_macroeconomics_2024}. These additions strengthen the conceptual grounding of the review and ensure that readers are introduced to the most influential contributions in the area before engaging with the methodological and empirical synthesis that follows.

\medskip

\textbf{Comment 2.d.}  
Because of these reasons, as a reviewer it is very difficult to decide which is AI written and which is a contribution to the relevant literature. Author/s definitely read cited articles in their manuscript and make sure that they are relevant ones, citations and contents are correct.

\textbf{Response.}  
Thank you for raising this concern. We carefully reworked the manuscript to remove AI-style phrasing, improve academic tone, and ensure that all cited works are correctly interpreted and integrated into the argument. All references included in the review were manually screened, read, and validated for relevance, and we cross-checked each citation for accuracy during the revision. We trust that the revised version now reflects a clearer scholarly voice and a more precise connection between citations and their contributions.

\medskip

\textbf{Comment 3.}  
P2-Line 4-6: that sentence is a big causal chain stated as fact and it appears without an inline citation in the manuscript: "Yet, we observe that … individual interpretations eventually converge into dominant narratives … shape financial market behavior." It asserts three linked mechanisms: (1) micro interpretations → convergence, (2) convergence → dominant/collective narratives, and (3) narratives → market outcomes. Each step is contestable and should be grounded in prior work. Therefore, authers should add Solid citations for each mechanism.

\textbf{Response.}  
We appreciate this observation. The original sentence indeed relied on an intuitive chain of reasoning that was not explicitly supported by the literature. To avoid overstating claims, we removed the causal formulation entirely. In its place, we introduced citations to established works that document specific mechanisms supported by theory and evidence. In particular, \textcite{tuckett_role_2017} provides evidence on how conviction narratives shape coordinated investor behaviour under uncertainty, while \textcite{shiller_popular_2020} discusses how widely shared economic narratives influence collective expectations and macro-financial dynamics. These references allow us to articulate the point clearly without asserting mechanisms that are not directly tested in this study.

\medskip

\textbf{Comment 4.}  
P4- Line 7-38: there's a concrete syntax error right in the printed Scopus query block, plus a minor year-filter inconsistency. Dangling term + mismatched parentheses in the KEY() part. Year filter doesn't match the text: You state "2010 or later," but the filter is AND PUBYEAR > 2010, which excludes 2010.

\textbf{Response.}  
Thank you for pointing out these issues. Both the query syntax error and the year-filter inconsistency have been corrected in the revised manuscript. The dangling term and mismatched parentheses in the \texttt{KEY()} block were fixed, and the publication-year filter has been aligned with the intended criterion (“2010 or later”). The query now parses correctly, and the text and filter are fully consistent.

\medskip

\textbf{Comment 5.}  
Author/s argue that "their paper introduces an algorithmic framework for conducting systematic literature reviews (SLRs), designed to improve efficiency, reproducibility, and selection quality assessment in the literature review process". The proposed "algorithmic framework" follows a very standard, off-the-shelf semantic screening in some parts, but it also add important algorithmic steps such as PCA, clustering pipeline and bibliometric filtering. But the exact contribution of the paper over and above the existing SLRs were not discussed, not clear. These points should be clarified and should be explained through which channels and how improvements suggested in the paper affect efficiency, reproducibility, and selection quality assessment in the literature review process. It is very difficult to understand the methodological advances proposed and used in the paper.

\textbf{Response.}  
Thank you for raising this point. We clarified the methodological contribution of the paper more explicitly in both the methodology and discussion sections.

Although individual components of the pipeline (semantic embeddings, PCA, clustering) are standard tools, their integration into a reproducible, rule-based SLR workflow has, to our knowledge, never been implemented in the literature. Existing automated or semi-automated review procedures typically rely on keyword dictionaries or bag-of-words filtering, which tend to overfit to specific terminology and may fail to capture conceptually relevant studies that use different phrasing. Our semantic screening step addresses this limitation by operating at the conceptual rather than lexical level, which improves recall of relevant studies and reduces false positives caused by superficial keyword overlaps. This distinction and its implication for selection quality were added to section 2.2.

We also strengthened the methodological positioning in the discussion (section 4.2), where we now explain more clearly how each step of the pipeline contributes to efficiency, reproducibility, and selection quality assessment. Moreover, the introduction of quantitative quality metrics (retained paper count, mean relevance, Silhouette score) adds an evaluative layer that is largely absent in traditional manual SLRs. These metrics make it possible to validate the selection outcome and to compare alternative configurations in a principled manner.

Finally, we clarified the practical channels through which the framework improves the review process. Automation reduces manual workload and ensures consistent application of criteria; the standardized pipeline enhances reproducibility across domains; and the framework supports iterative updates as new papers enter the field.

\medskip

\textbf{Comment 6.}  
See more compact algorithm is given by Oliveira, Otávio \& da Silva, Fabio \& Juliani, Fernando \& Ferreira, Luis \& Nunhes, Thaís. (2019). Bibliometric Method for Mapping the State-of-the-Art and Identifying Research Gaps and Trends in Literature: An Essential Instrument to Support the Development of Scientific Projects. 10.5772/intechopen.85856.

\textbf{Response.}  
Thank you for pointing to the bibliometric framework of \textcite{oliveira_bibliometric_2019}. We now refer to this work explicitly in the methodology section in order to clarify how our contribution extends existing automated screening procedures. We revised Section 4.2 to make this distinction clearer and to articulate the methodological advances relative to \textcite{oliveira_bibliometric_2019}. 

As noted, earlier frameworks such as \textcite{oliveira_bibliometric_2019} provide valuable structure for mapping research fields, but they rely primarily on lexical similarity through keyword co-occurrence, citation networks, and manual post-filtering. These approaches are effective for identifying broad thematic areas but are less suited to domains where terminology evolves rapidly or where conceptually similar studies may not share overlapping vocabularies. This creates risks of both false exclusions (missing relevant papers expressed with different wording) and false inclusions (capturing unrelated studies that use overlapping keywords).

The semantic embedding step in our framework directly addresses these limitations. By representing each document through embeddings, the screening process captures conceptual similarity rather than surface-level lexical overlap. This substantially reduces sensitivity to wording differences and allows the algorithm to identify coherent groups of papers even when they use diverse or domain-specific terminology. In addition, the clustering and quality-assessment components (relevance score, Silhouette score, retained-paper count) introduce quantitative evaluation layers that are not present in bibliometric methods relying solely on lexical or network features.

\medskip

\textbf{Comment 7.}  
"Section 4. Discussion" doesn't read like a tight, academic discussion, its content and framing need work. The section opens with broad statements and a three-bucket structure (conceptual/methodological/practical) but doesn't explicitly answer the two research questions or synthesize concrete takeaways, which is what editors expect in a Discussion. Additionally, this section reads like extended results/theory, not a discussion. Much of 4.1-4.2 reiterates themes ("holistic perspective… dynamic frameworks," "levels—micro/meso/macro…") without tying them to your corpus evidence or to implications/limitations. Furthermore, a strong Discussion should include: (i) answers to research questions, (ii) implications (theory/method/practice), (iii) limitations \& threats to validity (selection bias, transformer drift, Scopus coverage), and (iv) future research that follows directly from your findings. Right now, (ii)-(iv) are thin or implicit.

\textbf{Response.}  
Thank you for this detailed and constructive comment. We fully agree that the previous version of Section 4 did not function as a proper academic discussion and that it blurred the line between presenting results and interpreting them. Following your recommendations, we substantially rewrote the entire section so that it now reads as a structured, concise, and integrative discussion.

The revised section (i) explicitly answers both research questions; (ii) synthesizes the conceptual and methodological implications of the reviewed literature; (iii) provides a clearer exposition of the methodological contribution of our framework; (iv) includes a dedicated and substantial limitations subsection addressing database coverage, model bias, transformer drift, selection bias, and scalability; and (v) introduces a focused "Implications and Future Research Directions" subsection that follows directly from our findings.

More specifically, the discussion now begins with a short integrative overview linking back to the paper’s objectives. It then addresses RQ1 and RQ2 separately, summarizing how existing studies define and model financial narratives, and what methodological patterns and gaps emerge from the literature. The methodological contribution is now presented distinctly and concisely, clarifying how semantic embeddings, dimensionality reduction, and clustering provide improvements over traditional and bibliometric SLR procedures. We also expanded the limitations subsection to include threats to validity such as Scopus-only sourcing, exclusion of non–full-text papers, semantic drift in embedding models, and trade-offs between open-source and closed-source embeddings. Finally, we added a "future work" subsection that outlines theoretical, methodological, and practical implications for future research, including the need for standardized benchmarks, narrative typologies, interpretability protocols, and dynamic modeling frameworks.

\medskip



\newpage
\section*{Response to Reviewer 4}

\textbf{Overall Comment.}
Is good.

The introduction situates financial narratives within economics and finance, citing foundational works (Somers 1994, Shiller 2019, Tuckett \& Nikolic 2017). It highlights the gap between theoretical recognition of narratives and the methodological fragmentation in how they are measured. The link to systematic literature review (SLR) methodology is stated the need to automate and make reviews more reproducible.

Overall, the paper is strong and presents a robust methodology. However, the issues noted [bellow] should be addressed to improve clarity, precision.

\textbf{Response.}  
We thank the reviewer for the positive overall assessment and for recognizing both the relevance of the topic and the robustness of the proposed methodology. We appreciate the constructive suggestions aimed at improving clarity and precision. All comments have been carefully addressed in the revised manuscript, and we provide detailed responses to each point below.

\medskip

\textbf{Comment 1.1.}  
Conceptual Clarity:

"…financial narratives, a subset of economic narratives, which we define as structured interpretations or explanatory frameworks concerning financial markets or economic events…"
This definition seems is too broad and overlaps with sentiment indices or topic models. The introduction risks conflating "narrative," "theme," and "sentiment."
Providing a sharper boundary, could draw more explicitly from narrative theory.

\textbf{Response.}  
We thank the reviewer for highlighting the need for more precise conceptual boundaries. We have revised the introduction to clarify that financial narratives are not interchangeable with sentiment analysis, thematic topics, or simple textual groupings. We now draw more explicitly on narrative theory and on recent advances in narrative economics to underline that narratives possess causal structure, temporal coherence, and transmissibility, which distinguish them from sentiment indices or topic models.

We refined the definition accordingly: financial narratives are presented as structured interpretative frameworks that connect events, actors, and expectations over time, allowing them to influence belief formation and market dynamics. This framing aligns with influential work such as \textcite{shiller_narrative_2019} and \textcite{tuckett_role_2017}, as well as more recent empirical contributions \parencite{bybee_narrative_2023, bybee_business_2024, flynn_macroeconomics_2024} that model narratives as measurable, economically relevant drivers of pricing, forecasting performance, and macro-financial fluctuations. These studies show that narratives function as coherent explanatory structures rather than surface-level textual signals, reinforcing the conceptual distinction we adopt in the manuscript.

\medskip

\textbf{Comment 1.2.}  
Theoretical Anchoring:

The introduction frames narratives mostly as a data science task but does not fully develop the economic or social stakes (e.g., why narratives matter for price formation, bubbles, contagion).

\textbf{Response.}
Thank you for this helpful feedback. We substantially expanded and reorganized the introduction to make the economic and social stakes of narratives explicit before discussing the data-science perspective. The revised version now begins with foundational work in social theory, then connects these ideas to information economics and behavioral finance, and finally shows why narratives matter for price formation, belief contagion, and market cycles. We also incorporated the relevant theoretical and empirical references. This structure clarifies that narrative modeling is not only a technical task but is motivated by well-established mechanisms through which narratives influence expectations and financial dynamics.

\medskip

\textbf{Comment 1.3.}  
Research Questions:

"This study addresses the following research questions: (i) How can NLP and textual analysis techniques be used to quantify and model financial narratives? (ii) Can financial narratives modeling enhance financial market dynamics understanding?"
While good, these are method-driven and narrow. They do not engage with broader gaps such as conceptual definitions, methodological consistency, or cross-disciplinary perspectives.
Adding how are financial narratives conceptually defined in the literature? Could be good.

\textbf{Response.}
Thank you for this suggestion. We agree that the original research questions were framed too narrowly around methodological aspects. To address this, we revised them to explicitly incorporate a conceptual dimension. The discussion section now presents a third component within RQ2, asking how financial narratives are defined, operationalized, and differentiated across the literature. This aligns the review more closely with the broader gaps identified by the reviewer, namely the lack of definitional clarity and the heterogeneity of methodological practices. We also strengthened the discussion section to synthesize how existing studies conceptualize narratives, which theoretical traditions they draw from, and how these definitions translate into empirical modeling choices.

\medskip

\textbf{Comment 2.1.}  
Database Choice:

"Using Python, this review sources the publications from the Scopus database…"
Why only Scopus? Did you test or compare with other databases such as Web of Science? justify it.

\textbf{Response.}
We clarified and strengthened the justification for using Scopus as the primary source. Scopus was selected because it provides consistent, peer-reviewed metadata in economics, finance, and computer science, and its open API makes the retrieval process fully automatable and reproducible. Other repositories such as Web of Science require institutional authorization, while SSRN and arXiv mix reviewed and non-reviewed material, which would reduce methodological consistency in a study focused on peer-reviewed evidence. 

To acknowledge the limits of a single-source strategy, we expanded the Discussion section to explicitly mention the coverage restrictions of Scopus and the potential value of hybrid sourcing (e.g., complementing Scopus with OpenAlex to capture emerging or gray literature). These revisions make the rationale transparent while clearly identifying the trade-offs involved.

\medskip

\textbf{Comment 2.2.}  
Embedding Models:

"…multilingual-e5-large-instruct offers the advantage of being open-source… OpenAI's text-embedding-3-small demonstrates better performance… and is therefore selected for the study."
Did you just test the open-source model and then remove it?

If its only advantage is openness and efficiency rather than accuracy, why include it at all?

\textbf{Response.}
Thank you for pointing out this inconsistency. The initial draft mentioned the multilingual-e5 model in order to position the framework relative to emerging open-source alternatives, but the model was not actually used in the final pipeline. Since the study ultimately relies exclusively on \texttt{text-embedding-3-small} for all embedding computations, we removed references to multilingual-e5 to avoid any confusion about the models tested. The revised text now focuses solely on the chosen embedding model and clarifies the criteria behind this selection.

\medskip

\textbf{Comment 2.3.}  
PCA Retention Rule:

"…we apply PCA and retain the number of components that together explain 99\% of the variance."
Why such a high threshold? This may indicate multicollinearity and defeats the purpose of reduction. Why not Use conventional thresholds (80-90\%) or justify it. Consider UMAP/t-SNE as alternatives.

\textbf{Response.}
Thank you for raising this point. We revised the methodology section to clarify why a 99\% variance threshold is appropriate in this specific setting. PCA is not used here for aggressive dimensionality reduction but rather as an explainability and diagnostic tool to inspect redundancy across the six statement-based similarity scores. Because the input space is low-dimensional, retaining nearly all variance preserves interpretability of the loadings, which is important for identifying overlaps between statements and refining them if needed. We also show in the stability checks that the PCA threshold has no effect on the final set of selected papers.

Additionally we didn't opt for non-linear methods such as UMAP or t-SNE because these techniques are valuable for visualization but distort the variance structure and do not provide interpretable loadings, which makes them less suitable for the analytical and transparency-oriented purpose of this step.

\medskip

\textbf{Comment 2.4.}  
Clustering Strategy:

"…we adopted a three-cluster approach… but removed the medium- and low-relevance ones because manual review would reintroduce subjectivity."
Why remove all medium-relevance papers? Isn't this over-restricts the corpus and may bias results?

\textbf{Response.}
We clarified this point in the manuscript. The medium-relevance cluster was not discarded mechanically; we conducted a manual inspection of papers near the boundary between the high- and medium-relevance clusters. This assessment showed that most borderline papers were either conceptually peripheral or addressed adjacent research domains rather than financial narrative modeling. Including them would have increased noise without materially improving coverage. For this reason, we retained only the high-relevance cluster for the core synthesis.

That said, we explicitly acknowledge in the revised discussion that alternative designs are possible. A semi-systematic variant could incorporate a secondary manual screening of medium-relevance papers under transparent, rule-based criteria (e.g., thresholds on specific statement scores). This would allow researchers to expand coverage when the review objective requires greater breadth. We added this clarification in the limitations subsection to ensure that the implications of our design choice are transparent.

\medskip

\textbf{Comment 2.5.}  
AI Usage:

The paper relies heavily on AI-style language and generic terms, which blurs its academic precision. Phrases such as "financial market dynamics", "automated knowledge discovery", exaggerated terms like "in fact", and the overuse of connectors like "particularly," "specifically," and "—s" make the text read more like promotional writing than scholarly argument. Humanized it.

\textbf{Response.}
Thank you for highlighting this issue. We undertook a careful revision of the manuscript to remove AI-style phrasing, promotional wording, and overly generic connectors. The introduction, results, and discussion sections were rewritten to use clearer, more direct academic language and to ensure that claims are supported and precisely formulated. These changes improve readability and align the tone of the manuscript with standard scholarly writing.

\medskip


\newpage
\section*{Summary of Changes in the Manuscript}

This revision implements substantial improvements in conceptual framing, methodological clarity, and overall structure. The introduction was rewritten to align the scope of the paper with its actual contribution, distinguishing clearly between financial narratives as an empirical phenomenon and the scholarly literature that studies them. The research questions were reformulated to reflect this focus, and the abstract was adjusted accordingly.

Several sections of the methodology were clarified, particularly regarding database choice, statement construction, embedding models, PCA usage, and clustering decisions. The Discussion section was entirely reworked to answer the research questions directly, articulate the methodological contribution, and present limitations and implications more systematically. Writing throughout the manuscript was also refined to improve academic tone and remove AI-style phrasing.

\begin{itemize}
    \item[1.] Major structural revisions include a rewritten Introduction, a reorganized Discussion section with explicit answers to the research questions, and clearer transitions between conceptual, methodological, and empirical components.
    \item[2.] Methodological clarifications were added on database selection, relevance scoring, PCA retention rationale, clustering decisions, and the role of paraphrasing in mitigating statement-level subjectivity.
    \item[3.] Additional analyses include revised stability checks, expanded justification for algorithmic choices, and corrections to the Scopus query syntax and year filter.
    \item[4.] The manuscript was edited throughout for clarity, concision, and academic precision, including improvements to definitions, citations, and overall narrative flow.
\end{itemize}

\bigskip

\noindent All changes have been implemented in the revised version of the manuscript.  
We thank the reviewers again for their valuable guidance.

\end{document}
