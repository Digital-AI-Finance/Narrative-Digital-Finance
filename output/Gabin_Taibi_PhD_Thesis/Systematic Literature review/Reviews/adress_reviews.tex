Overall:
- improve writing style: avoid AI tone
- check references and their contents


--------------------------------------------------------------------------------------------------------------------------------------------


Abstract and Introduction:
- There is a real aim mismatch in introduction section. The "Introduction" foregrounds narratives' role in investor behavior/market dynamics (a domain claim) but the core contribution is an algorithmic SLR framework applied to papers about narratives (a methods claim). That's not fatal, but it needs alignment. Author/s should (1) Reframe the opening as brief background (why narratives matter) and immediately pivot to the SLR problem the paper solves; (2) State the primary contribution plainly ("a reproducible, automated SLR pipeline") and the secondary; (3) Bridge research questions, methods, results in a short "Contribution & Scope" paragraph so readers don't expect new decision-making evidence.
> See paragraph 4 and 5 in 1. Introduction

- There is a fundamental misalignment between the stated research questions and the actual methodology and data. This study does not extract or analyze real-world financial narratives, instead relying solely on Scopus-indexed academic publications discussing the topic; consequently, the second research question was not empirically examined, and the case study did not generate new evidence on financial market dynamics, and given Financial Innovation’s emphasis on original empirical and methodological contributions, the manuscript does not meet the journal’s expectations in its present form.
> See paragraph 4 and 5 in 1. Introduction

- Theoretical Anchoring: The introduction frames narratives mostly as a data science task but does not fully develop the economic or social stakes (e.g., why narratives matter for price formation, bubbles, contagion).
> See paragraph 1 to 3 in 1. Introduction

- Reframe the case study to reflect its true scope (meta-analysis of the literature) or redesign the study to analyze primary financial narrative data.
> See paragraph 4 and 5 in 1. Introduction

- There are important, highly-cited papers on (economic/financial) narratives not in your references that they should add: (i) Shiller (2017), American Economic Review (Papers & Proceedings) — "Narrative Economics." Canonical framing that virtually all later empirical "financial narratives" papers reference. ; (ii) Bybee, Kelly & Su (2023), Review of Financial Studies — "Narrative Asset Pricing: Interpretable Systematic Risk Factors from News Text." ; (iii) Bybee, Kelly, Manela & Xiu (2024), Journal of Finance — "Business News and Business Cycles." ; Flynn & Sastry (2024), NBER Working Paper — "The Macroeconomics of Narratives." Formalizes viral, belief-altering narratives in a business-cycle model and measures narratives in firms' 10-Ks.
> See paragraph 2 and 3 in 1. Introduction

- P1-Line 52-53: "…financial narratives, a subset of economic narratives…": This definition seems is too broad and overlaps with sentiment indices or topic models. The introduction risks conflating "narrative," "theme," and "sentiment." Providing a sharper boundary, could draw more explicitly from narrative theory.
> See end of paragraph 3 in 1. Introduction

- P2-Line 4-6: that sentence is a big causal chain stated as fact and it appears without an inline citation in the manuscript: "Yet, we observe that … individual interpretations eventually converge into dominant narratives … shape financial market behavior." It asserts three linked mechanisms: (1) micro interpretations → convergence, (2) convergence → dominant/collective narratives, and (3) narratives → market outcomes. Each step is contestable and should be grounded in prior work. Therefore, authers should add Solid citations for each mechanism.
> Removed from 1. Introduction since it is in fact too strong statement without proofs

- P2-Line 21-23: Research Questions: While good, these are method-driven and narrow. They do not engage with broader gaps such as conceptual definitions, methodological consistency, or cross-disciplinary perspectives. Adding how are financial narratives conceptually defined in the literature? Could be good.
> See paragraph 4 in 1. Introduction


--------------------------------------------------------------------------------------------------------------------------------------------


Section 2. Methodology:
- P4- Line 7-38: there's a concrete syntax error right in the printed Scopus query block, plus a minor year-filter inconsistency. Dangling term + mismatched parentheses in the KEY() part. Year filter doesn't match the text: You state "2010 or later," but the filter is AND PUBYEAR > 2010, which excludes 2010.
> Fixed 

- P2-Line 59: they cite Varsha et. al. (2024) article as "S et. al. (2024)" both in the text and in references.
> Fixed (wrong citation form): thank you for your valuable comment, we have changed ... very polite, "we appreciate that your comments have improved..."

- Database Choice: Why only Scopus? Did you test or compare with other databases such as Web of Science? justify it. Single-Source Limitation: Exclusive reliance on Scopus may omit key works from SSRN, NBER, arXiv, and other sources.
> Addressed (furhter explanations)

- Subjectivity in Inclusion Criteria: Relevance scoring is based on researcher-defined statements; no validation set or robustness check was provided.
> Addressed (furhter explanations)

- Embedding Models: Did you just test the open-source model and then remove it? If its only advantage is openness and efficiency rather than accuracy, why include it at all?
> TBD: True, the open-source model was just tested, to be removed and we keep the OpenAi's model

- PCA Retention Rule: Why such a high threshold? This may indicate multicollinearity and defeats the purpose of reduction. Why not Use conventional thresholds (80-90\%) or justify it. Consider UMAP/t-SNE as alternatives.
> Addressed (furhter explanations)

- Clustering Strategy: Why remove all medium-relevance papers? Isn't this over-restricts the corpus and may bias results? Potentially useful studies may have been omitted for the sake of automation.
> Addressed (furhter explanations)

- Author/s argue that "their paper introduces an algorithmic framework for conducting systematic literature reviews (SLRs), designed to improve efficiency, reproducibility, and selection quality assessment in the literature review process". The proposed "algorithmic framework" follows a very standard, off-the-shelf semantic screening in some parts, but it also add important algorithmic steps such as PCA, clustering pipeline and bibliometric filtering. But the exact contribution of the paper over and above the existing SLRs were not discussed, not clear (See more compact algorithm is given by Oliveira, Otávio & da Silva, Fabio & Juliani, Fernando & Ferreira, Luis & Nunhes, Thaís. (2019). Bibliometric Method for Mapping the State-of-the-Art and Identifying Research Gaps and Trends in Literature: An Essential Instrument to Support the Development of Scientific Projects. 10.5772/intechopen.85856).
> Addressed (furhter explanations and included paper)

[My own comment]
- Lack of validation and robustness checks: Say that the drift risk coming from the research statement is mitigated using the paraphrases, so one could decide to increase the number of paraphrases per statement to for instance 10. Add robustness checks on the variance threshold? Pass clustering techniques as robustness checks? 
> Address my own comment in section 3.4



--------------------------------------------------------------------------------------------------------------------------------------------


Section 3. Results:
- Clearly distinguish between financial narratives and the scholarly literature on financial narratives.
> Done: added some keywords in section introduction and in the text to recall we're talking about the literature


--------------------------------------------------------------------------------------------------------------------------------------------


Section 4. Discussion: this section doesn't read like a tight, academic discussion, its content and framing need work.
- The section opens with broad statements and a three-bucket structure (conceptual/methodological/practical) but doesn't explicitly answer the two research questions or synthesize concrete takeaways, which is what editors expect in a Discussion.
- Additionally, this section reads like extended results/theory, not a discussion. Much of 4.1-4.2 reiterates themes ("holistic perspective… dynamic frameworks," "levels—micro/meso/macro…") without tying them to your corpus evidence or to implications/limitations. Furthermore, a strong Discussion should include: (i) answers to research questions, (ii) implications (theory/method/practice), (iii) limitations & threats to validity (selection bias, transformer drift, Scopus coverage), and (iv) future research that follows directly from your findings. Right now, (ii)-(iv) are thin or implicit.
> TBD: rework the entire section to match the proposed discussion format

- The exact contribution of the paper over and above the existing SLRs were not discussed, not clear. These points should be clarified and should be explained through which channels and how improvements suggested in the paper affect efficiency, reproducibility, and selection quality assessment in the literature review process. It is very difficult to understand the methodological advances proposed and used in the paper
> TBD: "exact contribution" in section 4.2 and "improvements ... affect efficiency..." in 4.3 and 4.4

- It would be helpful to further elaborate on potential limitations of the method (such as the exclusion of gray literature or papers without accessible full texts) and on the practical challenges of implementing this pipeline at scale (e.g., computational cost, access to full texts)
- Explicitly acknowledge the limitations of using Scopus-only academic data to answer behavioral finance questions
> TBD: add these points in 4.3 from previous points.