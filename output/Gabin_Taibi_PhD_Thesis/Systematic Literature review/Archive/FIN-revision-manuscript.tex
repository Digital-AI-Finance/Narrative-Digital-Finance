\documentclass[12pt]{article}

% Encoding and fonts
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% Language and formatting
\usepackage[english]{babel}
\usepackage{setspace}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{microtype}
\usepackage[normalem]{ulem}

% Links
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}

% Math and symbols
\usepackage{amsmath, amssymb, amsfonts}

% Bibliography
\usepackage[backend=biber, style=apa,natbib=true]{biblatex}
\addbibresource{Systematic Literature review/SLR_references.bib}

% Figures and tables
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{tikz}

% Code
\usepackage{listings}
\usepackage{xcolor}

% Other
\usepackage{lipsum}
\usepackage{pdflscape}
\usepackage{authblk}

% Keywords handling command
\providecommand{\keywords}[1]
{
  \small	
  \textbf{\textit{Keywords---}} #1
}

% ===== latexdiff macros =====
\newcommand{\DIFadd}[1]{\textcolor{blue}{#1}}
\newcommand{\DIFdel}[1]{\textcolor{red}{\sout{#1}}}

\newcommand{\DIFaddbegin}{}
\newcommand{\DIFaddend}{}
\newcommand{\DIFdelbegin}{\bgroup\color{red}\uwave}
\newcommand{\DIFdelend}{\egroup}

% Variables
\renewcommand\Authfont{\normalsize}
\renewcommand\Affilfont{\small}
\newcommand{\nbpapersinitial}{288} % Initial query
\newcommand{\nbpapersfilters}{142} % Additional filters
\newcommand{\nbpapersjournalareas}{125} % Journal area filter
\newcommand{\nbpapersalgo}{24} % Algorithmic filtering
\newcommand{\nbpapersavailable}{20} % Only available papers
\newcommand{\nbpapersworkshops}{16} % No workshop proceeds
\newcommand{\conditionnumber}{370}
\newcommand{\kmoscore}{0.815}

% Figure design
\usetikzlibrary{shapes.geometric, arrows.meta, positioning}
\tikzstyle{database} = [cylinder, shape border rotate=90, aspect=0.25, draw, minimum height=1.2cm, text width=2cm, align=center]
\tikzstyle{process} = [rectangle, draw, minimum height=1.2cm, minimum width=4.5cm, text width=4.3cm, align=center]
\tikzstyle{phase} = [draw=none, text width=1cm, align=center]
\tikzstyle{decision} = [diamond, draw, aspect=2, minimum width=3.5cm, minimum height=1.2cm, text width=3.3cm, align=center]
\tikzstyle{arrow} = [thick, -{Stealth}]


\begin{document}


% ------------------------------------------------------------------------------------------------------------------------


\begin{abstract}
This paper \DIFdelbegin \DIFdel{introduces an algorithmic framework for conducting systematic literature reviews (SLRs), designed to improve }\DIFdelend \DIFaddbegin \DIFadd{presents a systematic literature review on financial narratives and the methods used to model them. To support this review, we develop an algorithmic framework that improves the }\DIFaddend efficiency, reproducibility, and \DIFdelbegin \DIFdel{selection quality assessment in the literature review process. The proposed method integrates }\DIFdelend \DIFaddbegin \DIFadd{consistency of study selection. The framework uses }\DIFaddend Natural Language Processing (NLP) techniques, clustering algorithms, and interpretability tools to automate \DIFdelbegin \DIFdel{and structure the selection and analysis of academic publications. The framework is applied to a case study focused on }\DIFdelend \DIFaddbegin \DIFadd{key steps of the screening and analysis process. We apply this approach to the study of }\DIFaddend financial narratives, \DIFdelbegin \DIFdel{an emerging area }\DIFdelend \DIFaddbegin \DIFadd{a growing field }\DIFaddend in financial economics \DIFdelbegin \DIFdel{that examines how structured accounts }\DIFdelend \DIFaddbegin \DIFadd{concerned with how structured interpretations }\DIFaddend of economic events \DIFdelbegin \DIFdel{, formed by the convergence of individual interpretations, influence market dynamics }\DIFdelend \DIFaddbegin \DIFadd{shape market behaviour }\DIFaddend and asset prices. Drawing from the Scopus\footnote{\url{https://www.scopus.com/}} database of peer-reviewed literature, the review \DIFdelbegin \DIFdel{highlights research efforts }\DIFdelend \DIFaddbegin \DIFadd{identifies research practices used }\DIFaddend to model financial narratives \DIFdelbegin \DIFdel{using various NLP techniques. Results reveal that while advances have been made}\DIFdelend \DIFaddbegin \DIFadd{with a range of NLP methods. Results show that although technical progress has been significant}\DIFaddend , the conceptualization of financial narratives remains fragmented \DIFdelbegin \DIFdel{, often reduced to sentiment analysis, topic modeling, or their combination, without a unified theoretical framework}\DIFdelend \DIFaddbegin \DIFadd{and is often simplified to sentiment-based measures. A large share of studies relies on media sources, which is understandable given their availability, but the short length of these texts introduces noise and reduces predictive reliability. Using longer and more diverse textual materials would help to address this limitation and provide a broader view of narratives circulating among economic agents}\DIFaddend . The findings \DIFdelbegin \DIFdel{underscore the value of more rigorous and dynamic narrative modeling approaches and demonstrate the effectiveness }\DIFdelend \DIFaddbegin \DIFadd{also highlight the importance of more comprehensive forms of narrative modeling that go beyond sentiment alone, and they illustrate the practical usefulness }\DIFaddend of the proposed algorithmic SLR methodology.
\end{abstract}

\keywords{Systematic Literature Review, Text Processing, NLP, Financial Narrative, Financial Market Dynamics}


% ------------------------------------------------------------------------------------------------------------------------


\section{Introduction}
\label{sec:intro}

%DIF <  We explain that narratives is not precisely defined in financial market
The influence of narratives on financial markets has recently become a prominent area of study in both economics and finance. The analysis of \DIFdelbegin \DIFdel{narrative }\DIFdelend \DIFaddbegin \DIFadd{narratives }\DIFaddend includes understanding how stories evolve, spread, and impact financial markets over time. By examining the mechanisms through which they form and propagate, researchers aim to uncover their role in shaping expectations, driving investor behavior, and consequently \DIFdelbegin \DIFdel{impact }\DIFdelend \DIFaddbegin \DIFadd{impacting }\DIFaddend market cycles. \DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  Here, he highlights that narrative is a broad concept that arised from social science research
\DIFdel{In Social Science}\DIFdelend \DIFaddbegin \DIFadd{In social science}\DIFaddend , narratives are structured accounts or interpretive frameworks through which people understand and act in the social world. The work of \textcite{somers_narrative_1994} advances that they are not simply representations but ontological structures: social life itself is storied. Narratives constitute social identities, guide actions, and embed individuals in relational settings over time and space. These narratives are constituted through emplotment (causal linking of events), selective appropriation, and their temporal and spatial connectivity.

%DIF <  We focus now on narrative definition in economics and thus financial narratives
In economics, narratives \DIFdelbegin \DIFdel{are studied under the term Narrative Economics, a field focusing on how economic agents use stories to navigate and make sense of complex environments. \textcite{shiller_narrative_2019} }\DIFdelend \DIFaddbegin \DIFadd{have gained traction as a bridge between behavioral and informational theories of markets. \textcite{grossman_impossibility_1980} shows that when information is costly, prices cannot be perfectly informationally efficient, as perfectly revealing markets would remove incentives to acquire information. This insight departs from the view of fully efficient markets and acknowledges the role of delayed or selective information diffusion. Thus, if information diffusion is neither instantaneous nor uniform, then the stories through which it is interpreted play an important role in how beliefs form and prices adjust. The work of \textcite{shiller_narrative_2017} highlights this point by demonstrating that contagious economic stories can drive fluctuations in aggregate activity. Additionally, \textcite{shiller_narrative_2019} goes further and }\DIFaddend argues that economic fluctuations cannot be fully understood through quantitative models alone, as narratives play a critical role in shaping market movements. \DIFdelbegin \DIFdel{Thus, \textcite{tuckett_role_2017} highlights }\DIFdelend \DIFaddbegin \DIFadd{Similarly, \textcite{tuckett_role_2017} shows }\DIFaddend that conviction narratives\DIFdelbegin \DIFdel{—}\DIFdelend \DIFaddbegin \DIFadd{, defined as }\DIFaddend coherent and emotionally compelling stories\DIFdelbegin \DIFdel{—}\DIFdelend \DIFaddbegin \DIFadd{, }\DIFaddend enable investors to act with confidence despite uncertainty, reinforcing collective behaviors in markets\DIFdelbegin \DIFdel{and }\DIFdelend \DIFaddbegin \DIFadd{. Building on this theoretical foundation, }\DIFaddend \textcite{shiller_popular_2020} \DIFdelbegin \DIFdel{completes this statement describing economic narratives as “stories that offer interpretations of economic events”.
We extend this concept by focusing on financial narratives , a subset of economic narratives , which we define as structured }\DIFdelend \DIFaddbegin \DIFadd{emphasizes that economic narratives function as widely shared interpretative stories about economic events, shaping collective beliefs and the propagation of expectations across agents, thereby providing a conceptual link between individual interpretation and aggregate market dynamics.
}

\DIFadd{Recent advances in the literature further demonstrate that narratives are not only descriptive but can themselves actively shape observable macro-financial outcomes. \textcite{bybee_narrative_2023} demonstrate that news-based narrative factors provide explanatory power for risk premia and future investment opportunities, aligning with predictions from intertemporal asset pricing theory, while \textcite{bybee_business_2024} analyze news attention extracted from large-scale textual corpora and show that it closely tracks business cycles and helps forecast market returns. More recently, \textcite{flynn_macroeconomics_2024} developed a macroeconomic model in which narratives generate persistent, belief-driven fluctuations with empirical evidence from firm-level disclosures. Drawing from these perspectives, we define financial narratives as structured, transmissible }\DIFaddend interpretations or explanatory frameworks concerning financial markets or economic events, based on available information \DIFdelbegin \DIFdel{. Still, the notion of perfect market efficiency has been challenged by \textcite{grossman_impossibility_1980}, who argue that if all information were instantly reflected in prices, investors would have no incentive to acquire costly private information. This critique aligns more closely with the semi-strong form of the Efficient Market Hypothesis, which allows for delayed information diffusion. Furthermore, individual market participants do not perceive and process information uniformly, as their interpretations are shaped by personal expectations and objectives, contextual knowledge, and prior experience, leading them to adopt or reject specific perspectives selectively.
Yet, we observe that, over time, these individual interpretations eventually converge into dominant narratives that influence collective beliefs which, in turn, shape financial market behavior.
}\DIFdelend \DIFaddbegin \DIFadd{and possessing causal temporal structure.
}\DIFaddend 

%DIF <  Theroetical contribution of SLR
Despite growing interest in the field, the concept of financial narratives remains underdefined, and the methodologies used to analyze them are diverse and often inconsistent. The purpose of this systematic literature review is to address this gap by offering a clearer picture of how narratives are conceptualized, modeled, and employed in the study of financial markets. \DIFdelbegin \DIFdel{In particular, this review aims to clarify how financial narratives are defined and processed across the literature, and assess their reported impact on financial or macroeconomic variables. }\DIFdelend It also provides a comprehensive overview of the textual analysis techniques employed, with particular attention to the evolution of methods following the introduction of transformer-based architectures. \DIFdelbegin \DIFdel{In doing so, the review assesses }\DIFdelend \DIFaddbegin \DIFadd{The review seeks to assess }\DIFaddend whether significant methodological advances have been made since early contributions, \DIFdelbegin \DIFdel{aiming }\DIFdelend \DIFaddbegin \DIFadd{and }\DIFaddend to identify both conceptual gaps and best practices in this emerging research area. The study addresses the following research questions: \DIFdelbegin %DIFDELCMD < \\
%DIFDELCMD < %%%
\DIFdelend (i) How can NLP and textual analysis techniques be used to quantify and model financial narratives? \DIFdelbegin %DIFDELCMD < \\
%DIFDELCMD < %%%
\DIFdelend (ii) \DIFdelbegin \DIFdel{Can financial narratives modeling enhance financial market dynamics understanding? }\DIFdelend \DIFaddbegin \DIFadd{How is the concept of financial narrative defined and applied, and what methodological patterns emerge from the existing literature? These research questions focus on synthesizing how financial narratives are studied in the academic literature, rather than an empirical examination of narrative data itself. The results should therefore be read as an account of how the existing literature conceptualizes, operationalizes, and models narratives in financial markets. 
}\DIFaddend 

%DIF <  Methodological contribution of SLR
\DIFdelbegin \DIFdel{More importantly, the relevance }\DIFdelend \DIFaddbegin \DIFadd{The contribution }\DIFaddend of this research lies \DIFdelbegin \DIFdel{not only in its theoretical insights but also in its methodological contributions. In fact, the review advances automated knowledge discoveryby advanced NLP techniques and machine learning to systematically identify, evaluate, and extract relevant academic literature}\DIFdelend \DIFaddbegin \DIFadd{both in its synthesis of existing studies and in the methodology used to conduct the review. We propose a reproducible, algorithmic framework for systematic literature reviews that enhances the systematic discovery, screening, and synthesis of research using transformer-based embeddings, dimensionality reduction, and clustering}\DIFaddend . This approach contributes to the development of scalable, data-driven techniques for literature synthesis and \DIFdelbegin \DIFdel{lays the groundwork for }\DIFdelend \DIFaddbegin \DIFadd{supports }\DIFaddend more reproducible research \DIFaddbegin \DIFadd{in fast-evolving domains}\DIFaddend . The proposed framework \DIFdelbegin \DIFdel{incorporates transformer-based sentence embedding, dimensionality reduction, and clustering to evaluate relevance against pre-defined research criteria. By integrating these components, the framework not only }\DIFdelend enables a more systematic and \DIFdelbegin \DIFdel{reproducible literature review process , but also improves the assessment of selection quality , ensuring that the selected studies align closely with the research questions.
Furthermore, textual analysis is employed to provide researchers with a structured understanding of the remaining publications, thus facilitating the organization and synthesis of findings, as well as the assessment of the thematic relevance and quality of the selected studies.
}\DIFdelend \DIFaddbegin \DIFadd{transparent review process while improving selection quality and alignment with research objectives.
}\DIFaddend 

The review is structured as follows: Section~\ref{sec:methodology} presents the algorithmic framework for systematic literature reviews, including the query design, filtering steps, clustering approach, and quality assessment procedures. Section~\ref{sec:results} applies the methodology to the domain of financial narratives, analyzing research trends, modeling techniques, and conceptual challenges identified in the selected literature. Section~\ref{sec:discussions} discusses the implications of the findings, \DIFaddbegin \DIFadd{the }\DIFaddend limitations of existing approaches, and potential directions for future research in financial narrative modeling.


% ------------------------------------------------------------------------------------------------------------------------


\section{Methodology}
\label{sec:methodology}

Our approach in this literature review is inspired by the work of \textcite{amato_how_2024}, who structured the review process into eight distinct steps: defining the research question, developing and implementing the review methodology, conducting literature exploration and analysis, applying selection criteria (inclusion/exclusion), assessing the quality of selected studies, extracting relevant data, synthesizing findings, and reporting insights. This structured framework, \DIFdelbegin \DIFdel{actually based on }\DIFdelend \DIFaddbegin \DIFadd{initially based on }\DIFaddend \textcite{\DIFdelbegin \DIFdel{s}\DIFdelend \DIFaddbegin \DIFadd{varsha}\DIFaddend _how_2024}, starts by defining the research problem and formulating clear research questions. A structured review methodology is then developed to guide literature search and analysis. Thus, specific inclusion criteria are applied to filter studies and assess their relevance and quality. Once the final set of papers is selected, key information is extracted and synthesized to generate \DIFdelbegin \DIFdel{meaningful }\DIFdelend \DIFaddbegin \DIFadd{key }\DIFaddend insights, organized and presented in a structured report.

\begin{table}[h]
    \centering
    \caption{Summary of article selection criteria.}
    \label{tab:selection_criteria}
    \begin{tabular}{p{10cm} c}
        \toprule
        \textbf{Criteria} & \textbf{Decision} \\
        \midrule
        Inclusion of pre-defined keywords in title, abstract, or keyword list & Inclusion \\
        Article not published in a scientific journal & Exclusion \\
        Article in a language other than English & Exclusion \\
        Article published \DIFdelbeginFL \DIFdelFL{before }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{in }\DIFaddendFL 2010 \DIFaddbeginFL \DIFaddFL{or before }\DIFaddendFL & Exclusion \\
        Duplicates & Exclusion \\
        Low relevance to research questions & Exclusion \\
        Unavailability of the article online & Exclusion \\
        Workshop Proceeds & Exclusion \\
        \bottomrule
    \end{tabular}
\end{table}

However, because the core phase of the current methodology, the study selection, depends heavily on the researcher’s judgment, the overall process remains subjective, difficult to replicate, and prone to bias. To address this, we extend the framework by integrating \DIFdelbegin \DIFdel{transformer-based models }\DIFdelend \DIFaddbegin \DIFadd{algorithmic rules }\DIFaddend to automate and refine the selection phase, improving the \DIFaddbegin \DIFadd{reproducibility and }\DIFaddend consistency of inclusion decisions and the assessment of study relevance. \DIFdelbegin \DIFdel{This enhancement strengthens quality control and supports a more scalable and reproducible review. }\DIFdelend As shown in Table~\ref{tab:selection_criteria}, \DIFdelbegin \DIFdel{the }\DIFdelend \DIFaddbegin \DIFadd{our }\DIFaddend paper selection process remains fully systematic, following predefined rules. \DIFaddbegin \DIFadd{The idea of algorithmically structuring the literature selection process follows work proposed by \textcite{oliveira_bibliometric_2019}, who emphasized automated mapping of research fields through keyword co-occurrence and citation networks. While these earlier frameworks demonstrated the value of systematic automation, they remained limited by their reliance on lexical similarity rather than semantic understanding. The present framework extends this aspect using transformer-based embeddings for semantic relevance assessment, interpretable dimensionality reduction for redundancy control, and quantitative evaluation of clustering quality. }\DIFaddend In this section, we present the overall review process summarized in Figure~\ref{fig:paper_selection_diagram}, from the initial research query definition to the data extraction and result analysis phase.

\subsection{Initial \DIFdelbegin \DIFdel{research sourcing}\DIFdelend \DIFaddbegin \DIFadd{Research Sourcing}\DIFaddend }

The first phase involves defining the research context and the type of information we aim to extract from the literature. \DIFdelbegin \DIFdel{Using Python, this }\DIFdelend \DIFaddbegin \DIFadd{This }\DIFaddend review sources the publications from the Scopus database, specifically retrieving basic information from the results through the Application Programming Interface (API) and scraping the abstract, authors and keywords from Scopus URLs. We also \DIFdelbegin \DIFdel{use the same programming language to automatically get the journal ranking and journal information}\DIFdelend \DIFaddbegin \DIFadd{automatically retrieved journals metadata (ranking and overall information) }\DIFaddend from SCImago Journal Ranking website \footnote{\url{https://www.scimagojr.com/journalrank.php}}.

\DIFaddbegin \DIFadd{Scopus was selected as the primary data source because it offers reliable coverage of peer-reviewed research in economics, finance, and computer science, with metadata that is curated in a consistent and transparent manner. Its API provides immediate access for academic users, which allows the entire retrieval process to be automated and reproduced without institutional restrictions. Other repositories such as SSRN or arXiv contain valuable material but mix peer-reviewed and non-reviewed documents, making quality control and comparability more challenging. While recent open initiatives like OpenAlex }\parencite{priem_openalex_2022} \DIFadd{broaden access to bibliographic data, this study prioritizes the stability and curation standards of Scopus, which remains one of the most academically recognized databases for peer-reviewed research.
}

\DIFaddend As this study focuses on narrative modeling using NLP techniques to better understand financial market\DIFdelbegin \DIFdel{dynamics}\DIFdelend , we are particularly interested in literature that addresses both the methodological development of narrative modeling from large textual corpora and the application of NLP \DIFdelbegin \DIFdel{—}\DIFdelend \DIFaddbegin \DIFadd{(}\DIFaddend both traditional and recent approaches\DIFdelbegin \DIFdel{—}\DIFdelend \DIFaddbegin \DIFadd{) }\DIFaddend in finance. This leads us to formulate two research questions: How can NLP and textual analysis techniques be used to quantify and model financial narratives? \DIFdelbegin \DIFdel{Can financial narrative modeling improve our understanding of financial market dynamics}\DIFdelend \DIFaddbegin \DIFadd{How is the concept of financial narrative defined and applied, and what methodological patterns emerge from the existing literature}\DIFaddend ? Accordingly, we seek to identify rigorous and relevant academic work across three main themes: theoretical discussions of narratives in finance and economics (financial narratives theory), the use of NLP and text analysis to extract narratives in financial contexts (NLP for narratives processing), and more \DIFdelbegin \DIFdel{specifically}\DIFdelend \DIFaddbegin \DIFadd{precisely}\DIFaddend , the quantification and tracking of narratives for financial applications (financial narratives modeling).

To initiate the selection process, we designed a \DIFdelbegin \DIFdel{targeted }\DIFdelend search query using the advanced search function of the Scopus database. We experimented with various combinations of keywords in an effort to construct a query that is both broad enough to capture relevant research and narrow enough to exclude unrelated domains. The chosen query captures a wide range of pertinent literature, though it also retrieved some irrelevant papers \DIFdelbegin \DIFdel{, such as those focused on }\DIFdelend \DIFaddbegin \DIFadd{(e.g. }\DIFaddend narratives in health or psychology, \DIFaddbegin \DIFadd{domains }\DIFaddend where narratives are also a major topic of study\DIFaddbegin \DIFadd{)}\DIFaddend . The query was constructed to retrieve publications related to narratives, NLP, and financial markets:
\begin{itemize} 
    \item \textbf{Economic/Financial Narratives in title, abstract or keywords:} \newline TITLE-ABS-KEY("financial narrative" OR "financial narratives" OR "economic narrative" OR "economic narratives")
    \item \textbf{Narratives, NLP/text analysis and financial terms in title:} \newline OR TITLE(narrative* AND ("language processing" OR nlp OR "language understanding" OR nlu OR "text mining" OR "textual analysis" OR "text analysis" OR "text processing" OR lexicon OR sentiment OR "topic modeling" OR "topic extraction" OR "word embedding" OR "word embeddings" OR "entity recognition" OR "narrative processing" OR "narrative modeling") AND (economic* OR macroeconomic* OR "financial market" OR "financial markets" OR "market dynamics" OR "market movements" OR "price dynamics" OR "price movements" OR "financial forecasting" OR "stock markets" OR "equity markets" OR "foreign exchange" OR commodit* OR bond* OR "asset prices" OR "price returns" OR "asset returns" OR "market returns" OR volatility OR "risk management" OR "portfolio management"))
    \item \textbf{Narratives, NLP/text analysis and financial terms in keywords:} \newline OR KEY(narrative* AND ("language processing" OR nlp OR "language understanding" OR nlu OR "text mining" OR "textual analysis" OR "text analysis" OR "text processing" OR lexicon OR sentiment OR "topic modeling" OR "topic extraction" OR "word embedding" OR "word embeddings" OR "entity recognition" OR "narrative processing" OR "narrative modeling") AND (economic* OR macroeconomic* OR "financial market" OR "financial markets" OR "market dynamics" OR "market movements" OR "price dynamics" OR "price movements" OR "financial forecasting" OR "stock markets" OR "equity markets" OR "foreign exchange" OR commodit* OR bond* OR "asset prices" OR "price returns" OR "asset returns" OR "market returns" OR volatility OR "risk management" OR "portfolio management"))
\DIFdelbegin \DIFdel{Explainability" OR "Counterfactual Explainability" ) )
}\DIFdelend \end{itemize}
The inclusion of the term “narrative(s)” is central, as it ensures thematic relevance to our research focus. We prioritized papers that explicitly mention “financial narrative(s)” or “economic narrative(s)” in the title, abstract, or keywords. Additionally, to broaden the scope while maintaining relevance, we formulated a more inclusive search strategy targeting titles and keywords (but not abstracts because they tend to be more descriptive and often mention broader methodological or contextual terms that may not reflect the core focus of the paper), combining “narrative(s)” with terms related to text analysis or NLP, and with terms referring to financial markets.

The second phase involves applying additional filters to refine the search\DIFdelbegin \DIFdel{and ensure greater homogeneity in the retrieved literature, and remove }\DIFdelend \DIFaddbegin \DIFadd{, as well as removing }\DIFaddend duplicates entries. In our case, we extended the initial query by restricting the publication year to \DIFdelbegin \DIFdel{2010 }\DIFdelend \DIFaddbegin \DIFadd{2011 }\DIFaddend or later, limiting the results to peer-reviewed journal articles, and including only English-language publications\DIFdelbegin \DIFdel{, resulting in }%DIFDELCMD < \nbpapersfilters{} %%%
\DIFdel{papers}\DIFdelend :
\begin{itemize} 
    \item \textbf{Economic/Financial Narratives in title, abstract or keywords:} \newline Identical to phase 1
    \item \textbf{Narratives, NLP/text analysis and financial terms in title:} \newline Identical to phase 1
    \item \textbf{Narratives, NLP/text analysis and financial terms in keywords:} \newline Identical to phase 1
    \item \textbf{Time constraint:} \newline AND PUBYEAR > 2010
    \item \textbf{Language constraint:} \newline AND LANGUAGE(english)
    \item \textbf{Article constraint:} \newline AND DOCTYPE(ar)
\end{itemize}
At this stage of the selection process, an additional filtering criterion was introduced to enhance thematic alignment. \DIFdelbegin \DIFdel{Specifically, journal }\DIFdelend \DIFaddbegin \DIFadd{Journal }\DIFaddend subject areas were cross-referenced with disciplinary classifications from the SCI journal ranking database to identify and remove publications originating from fields clearly unrelated to the research context. Journals primarily associated with disciplines outside the intended scope were excluded\DIFdelbegin \DIFdel{to eliminate noise and improve the quality of the dataset for subsequent phases}\DIFdelend . Conversely, journals whose subject areas could plausibly intersect with the research objectives, or for which thematic relevance could not be reliably assessed at this stage, were retained for further evaluation.
\DIFdelbegin \DIFdel{This approach allowed for the early elimination of clearly irrelevant publications while preserving potentially relevant studies for more detailed analysis in later stages of the selection pipeline.
}\DIFdelend 

\subsection{Algorithmic \DIFdelbegin \DIFdel{selection framework}\DIFdelend \DIFaddbegin \DIFadd{Selection Framework}\DIFaddend }

This section presents the third phase, consisting in an algorithmic process used to filter research papers based on their relevance. The selection framework leverages NLP and machine learning techniques to classify papers into three categories: high, medium, and low relevance. The process consists of three main stages: textual analysis, dimensionality reduction, and clustering.

\subsubsection{Textual \DIFdelbegin \DIFdel{analysis}\DIFdelend \DIFaddbegin \DIFadd{Analysis}\DIFaddend : \DIFdelbegin \DIFdel{research properties statements}\DIFdelend \DIFaddbegin \DIFadd{Research Properties Statements}\DIFaddend }

The selection methodology relies on a \DIFdelbegin \DIFdel{statement similarity }\DIFdelend \DIFaddbegin \DIFadd{zero-shot latent embedding }\DIFaddend approach, in which the user defines key statements \DIFdelbegin \DIFdel{that }\DIFdelend \DIFaddbegin \DIFadd{which }\DIFaddend describe the criteria that selected papers should meet. In our case, the statements respectively reflect the research focus, research type, context, methodology, data sources, and research questions:
\begin{itemize}
    \item ``The research discusses Financial Narrative Processing, Financial Narrative Modeling or the use of textual data to understand financial markets'';
    \item ``The research is highly relevant in the context of financial markets, including: equities, foreign exchange, cryptocurrencies, bonds, commodities, or real estate'';
    \item ``The research is \DIFdelbegin \DIFdel{a }\DIFdelend \DIFaddbegin \DIFadd{an }\DIFaddend empirical study showcasing the use of textual data to model narratives and understand financial markets dynamics'';
    \item ``The research methodologies include: textual analysis, text mining or Natural Language Processing techniques such as topic modeling, emotion analysis, sentiment analysis, word embeddings or transformer-based models'';
    \item ``The research leverages the following data: large textual datasets, including financial reports, news articles, social media posts, audio or video transcripts, or any other form of financial textual data'';
    \item ``The research helps answering the research question(s): How can NLP and textual analysis techniques be used to quantify and model financial narratives? Can financial narratives modeling enhance financial market dynamics understanding?''
\end{itemize}

\DIFdelbegin \DIFdel{These statements define the inclusion criteria for selecting relevant studies, ensuring that only research focused on financial narrative modeling, NLP-based textual analysis, and their impact on financial markets is considered.  
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{To evaluate how well each paper aligns with the research statements, two transformer-based sentence embedding models were tested. The first is }\texttt{\DIFdel{intfloat/multilingual-e5-large-instruct}}%DIFAUXCMD
\DIFdel{, an open-source model designed for multilingual tasks }%DIFDELCMD < \parencite{wang_multilingual_2024} %%%
\DIFdel{and implemented using the HuggingFace SentenceTransformer}\footnote{%DIFDELCMD < \url{https://huggingface.co/sentence-transformers}%%%
} %DIFAUXCMD
\addtocounter{footnote}{-1}%DIFAUXCMD
\DIFdel{library in Python. The second is }\DIFdelend \DIFaddbegin \DIFadd{To limit the sensitivity of the selection procedure to the exact wording of the user, each statement was expanded into several paraphrased variants using }\DIFaddend OpenAI’s \DIFdelbegin \DIFdel{proprietary }\texttt{\DIFdel{text-embedding-3-small}}%DIFAUXCMD
\DIFdel{, also implemented with the Python API library. While }\texttt{\DIFdel{multilingual-e5-large-instruct}} %DIFAUXCMD
\DIFdel{offers the advantage of being open-source and lightweight in terms of computational requirements, }\DIFdelend \DIFaddbegin \DIFadd{chat-completion API (}\texttt{\DIFadd{gpt-3.5-turbo}} \DIFadd{model), and embeddings were averaged. This reduces the influence of any single formulation on the similarity scores and provides a more stable representation of the underlying research criteria.
}

\DIFadd{For text embedding tasks (statements and candidate research papers), we opted for }\DIFaddend OpenAI’s \texttt{text-embedding-3-small} \DIFdelbegin \DIFdel{demonstrates better performance in terms of semantic relevance and alignmentaccuracy, and is therefore selected for the study. }%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Rather than embedding the six statements directly, we use the chat completion capabilities of OpenAI’s API to generate five close paraphrases for each statement, reducing sensitivityto specific wording. Each set of six paraphrases (the original plus five variations) is embedded individually, and their mean vector is computed to obtain a single representative embedding per statement. In parallel, we concatenate }\DIFdelend \DIFaddbegin \DIFadd{because it provides strong semantic alignment, low embedding distortion, and consistent performance across diverse academic and technical domains. Among the available models, it offers an optimal trade-off between accuracy, computational efficiency, and contextual sensitivity. It is worth mentioning that, as the model outputs unit-normalized vectors, cosine similarity reduces to a simple dot product, which simplifies computation at scale. For each potential paper, }\DIFaddend the title, abstract, and keywords \DIFdelbegin \DIFdel{of each paper into a single textual input and generate embeddings for these as well. The cosine similarity }\DIFdelend \DIFaddbegin \DIFadd{are concatenated and embedded. Cosine similarities }\DIFaddend between each paper embedding and each of the six statement embeddings \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{are }\DIFaddend then computed, \DIFdelbegin \DIFdel{resulting in }\DIFdelend \DIFaddbegin \DIFadd{yielding }\DIFaddend six similarity scores per paper\DIFdelbegin \DIFdel{. Finally, the average of these six scores is computed to represent the paper’s }\DIFdelend \DIFaddbegin \DIFadd{, which are then averaged to obtain an }\DIFaddend overall relevance score.

\subsubsection{Data \DIFdelbegin \DIFdel{preparation}\DIFdelend \DIFaddbegin \DIFadd{Preparation}\DIFaddend }

Before proceeding with the clustering step, the matrix of similarity scores is \DIFdelbegin \DIFdel{Z-score standardized for each of the six dimensions }\DIFdelend \DIFaddbegin \DIFadd{standardized (Z-score) }\DIFaddend to ensure each dimension contributes equally to the clustering algorithm. The next step is to reduce the dimensionality of the data while preserving its most informative components, in the case where we observe high correlation between relevance features. This is achieved using Principal Component Analysis (PCA), which transforms the six-dimensional similarity space into a lower-dimensional representation by identifying the most important variance-explaining components.

We use the Kaiser-Meyer-Olkin (KMO) score \DIFdelbegin \DIFdel{—which assesses the adequacy of the data for factor analysis by measuring sampling adequacy—}\DIFdelend and the Condition Number (CN) \DIFdelbegin \DIFdel{, which evaluates multicollinearity by measuring the sensitivity of a matrix to numerical inversion. These two indicators guide our decision on whether to apply PCA. Specifically, if the KMO scoreis }\DIFdelend \DIFaddbegin \DIFadd{as diagnostic indicators to assess whether applying Principal Component Analysis (PCA) is appropriate. The KMO score, while traditionally used in factor analysis, serves here as a heuristic measure of inter-variable correlation strength. A value }\DIFaddend below 0.5 \DIFdelbegin \DIFdel{, PCA is not recommended due to poor sampling adequacy. If the KMO score exceeds 0.7, PCA is considered appropriate. If the KMO score falls between 0.5 and }\DIFdelend \DIFaddbegin \DIFadd{suggests that variables share little common variance, making PCA uninformative, whereas values above }\DIFaddend 0.7 \DIFaddbegin \DIFadd{indicate sufficient correlation structure for meaningful components. When KMO falls in the intermediate range (0.5-0.7)}\DIFaddend , we additionally \DIFdelbegin \DIFdel{check the CN: if it is greater than 100, indicating severe multicollinearity}\DIFdelend \DIFaddbegin \DIFadd{consider the CN}\DIFaddend , \DIFaddbegin \DIFadd{which quantifies multicollinearity. If $\mathrm{CN} > 100$, implying near-linear dependencies among features, }\DIFaddend PCA is applied \DIFaddbegin \DIFadd{to stabilize the representation}\DIFaddend ; otherwise, it is \DIFdelbegin \DIFdel{not.
We therefore apply PCA and retain the number of components that together }\DIFdelend \DIFaddbegin \DIFadd{skipped.
}

\DIFadd{When PCA is applied, we retain enough components to }\DIFaddend explain 99\% of \DIFdelbegin \DIFdel{the variance. This lower-dimensional representation enhances the efficiency of the subsequent clustering process by eliminating noise and redundancy. }\DIFdelend \DIFaddbegin \DIFadd{total variance. Unlike predictive or visualization-oriented uses of PCA, this study employs PCA primarily for diagnostic and interpretive purposes. The small number of input variables mitigates overfitting risk, and retaining nearly all variance allows transparent inspection of factor loadings to identify redundant or semantically overlapping features. We show later that this parameter has no effect on the selected papers. }\DIFaddend Papers that initially shared similar similarity profiles across multiple research statements are now grouped based on their principal components, which encapsulate the most distinguishing features of their textual content. This \DIFdelbegin \DIFdel{transformation should refine the classification process, ensuring that only the most relevant dimensions contribute to the selection framework.
%DIF < Ultimately, mapping research papers into this optimized space improves interpretability and strengthens the robustness of clustering results.
}\DIFdelend \DIFaddbegin \DIFadd{approach contrasts with non-linear dimensionality-reduction methods, which, while effective for visualization, distort variance structure and are less suited for analytical interpretation.
}\DIFaddend 

\subsubsection{\DIFaddbegin \DIFadd{Clustering }\DIFaddend Research \DIFdelbegin \DIFdel{clustering}\DIFdelend \DIFaddbegin \DIFadd{Selection}\DIFaddend }

\DIFdelbegin \DIFdel{After reducing the dimensionality of the similarity scores, the }\DIFdelend \DIFaddbegin \DIFadd{The }\DIFaddend next step in the selection process involves clustering the papers into distinct relevance groups \DIFdelbegin \DIFdel{. This classification is performed using three clustering methods: }\DIFdelend \DIFaddbegin \DIFadd{using }\DIFaddend K-means\DIFdelbegin \DIFdel{, Gaussian Mixture Models (GMM), and Agglomerative Clustering (AC). These methods were selected for their flexibility and their ability to explicitly control the number of output clusters, which is essential for our relevance-based filtering. K-means partitions the data into a fixed number of clusters by minimizing the }\DIFdelend \DIFaddbegin \DIFadd{. This method provides stable partitions in low-dimensional spaces, and its objective function of minimizing }\DIFaddend within-cluster variance \DIFdelbegin \DIFdel{, assigning each point to the nearest cluster centroid. Gaussian Mixture Models extend this approach by assuming that the data is generated from a mixture of Gaussian distributions, allowing for probabilistic cluster assignments and more flexibility in capturing ellipsoidal shapes in the data. Agglomerative Clustering is a hierarchical method that builds clusters by iteratively merging the closest pairs based on a linkage criterion, producing a dendrogram from which a desired number of clusters can be extracted.
The clustering is performed in the four-dimensional space obtained from the PCA transformation.
}\DIFdelend \DIFaddbegin \DIFadd{offers a simple and transparent criterion that aligns with the reproducibility goals of this study. Its interpretability also facilitates downstream validation, which is important for a systematic review framework.
}\DIFaddend 

Given the diversity in methodologies, objectives, and data sources within the dataset, we adopted a three-cluster \DIFdelbegin \DIFdel{approach to categorize papers into }\DIFdelend \DIFaddbegin \DIFadd{optimization. After applying K-means, we computed the mean relevance score within each cluster and assigned relevance tiers by ranking these means from highest to lowest. This procedure resulted in three groups that we refer to as }\DIFaddend high, medium, and low relevance. \DIFdelbegin \DIFdel{This method provides a more granular classification, where high-relevance papers strongly align with the systematic review criteria, low-relevance papers are largely unrelated, }\DIFdelend \DIFaddbegin \DIFadd{Manual inspection of “frontier” papers located near the decision boundary between the high- }\DIFaddend and medium-relevance \DIFdelbegin \DIFdel{papers share some relevant characteristics and require further evaluation. The }\DIFdelend \DIFaddbegin \DIFadd{clusters revealed that most borderline cases did not fall within the scope of this review. Because these borderline studies added conceptual noise without improving coverage, the }\DIFaddend medium-relevance \DIFdelbegin \DIFdel{group can be manually reviewed to refine the selection, but we decided to opt for a single high-relevance cluster and remove the medium- and low-relevance ones because manual review would reintroduce subjectivity into the selection process, undermining the systematic and reproducible nature of the proposed framework. Moreover, our initial search queried a large number of papers with diverse methodologies, objectives, and data sources. Given this heterogeneity, retaining only the high-relevance category ensures that }\DIFdelend \DIFaddbegin \DIFadd{cluster was not retained in }\DIFaddend the final selection\DIFdelbegin \DIFdel{consists of studies that closely align with the research objectives while minimizing the inclusion of marginally relevant literature, improving both the focus and }\DIFdelend \DIFaddbegin \DIFadd{. To evaluate the }\DIFaddend quality of the \DIFdelbegin \DIFdel{systematic review.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{We compare the results of each clustering method based on three main evaluation criteria }\DIFdelend \DIFaddbegin \DIFadd{produced clusters, three metrics were }\DIFaddend computed on the \DIFaddbegin \DIFadd{resulting }\DIFaddend high-relevance cluster: the average relevance score, the Silhouette score \DIFdelbegin \DIFdel{, and the number of papers retained. The Silhouette score measures how well each sample fits within its assigned cluster, balancing cohesion (similarity }\DIFdelend \DIFaddbegin \DIFadd{(capturing both cohesion }\DIFaddend within the cluster \DIFdelbegin \DIFdel{) and separation (dissimilarity with }\DIFdelend \DIFaddbegin \DIFadd{and separation from the }\DIFaddend other clusters)\DIFdelbegin \DIFdel{; higher values indicate more distinct and well-separated clusters. The objective is to maximize all three metrics to ensure that the high-relevance cluster is both coherent and substantial in size. To identify the most suitable method, we computed a composite score by standardizing each metric (Z-score scaling) and assigning weights of 50\% to the average relevance score, 20\% to the Silhouette score, and 30\% to the }\DIFdelend \DIFaddbegin \DIFadd{, and the }\DIFaddend number of papers \DIFaddbegin \DIFadd{retained (which we aim to be high)}\DIFaddend .

\subsection{\DIFdelbegin \DIFdel{Data cleaning }\DIFdelend \DIFaddbegin \DIFadd{Final Validation }\DIFaddend and \DIFdelbegin \DIFdel{extraction}\DIFdelend \DIFaddbegin \DIFadd{Data Extraction}\DIFaddend }

Once the algorithmic selection process was finalized, a \DIFdelbegin \DIFdel{final }\DIFdelend \DIFaddbegin \DIFadd{last }\DIFaddend manual validation phase was \DIFdelbegin \DIFdel{carried out }\DIFdelend \DIFaddbegin \DIFadd{conducted }\DIFaddend to refine the dataset\DIFdelbegin \DIFdel{and ensure its quality and consistency}\DIFdelend . This stage \DIFdelbegin \DIFdel{aimed }\DIFdelend \DIFaddbegin \DIFadd{aims }\DIFaddend to address limitations that algorithmic methods alone could not fully resolve. \DIFdelbegin \DIFdel{Specifically}\DIFdelend \DIFaddbegin \DIFadd{More precisely}\DIFaddend , publications whose full text was not accessible, either because of paywall restrictions or the lack of institutional access, were excluded from the corpus. \DIFdelbegin \DIFdel{The inability to review these documents in their entirety prevented the assessment of their thematic relevance and methodological quality, thus warranting their removal. }\DIFdelend In addition, unconventional documents such as workshop proceedings and compilation volumes were identified and excluded. These types of publications often aggregate a large number of individual studies under a general theme, without guaranteeing a consistent focus aligned with the research objectives. Moreover, the scale and heterogeneity of such documents made them impractical to process within the structured framework of this study.
\DIFdelbegin \DIFdel{This final validation step ensured that the remaining corpus was both thematically coherent and methodologically appropriate for the subsequent literature analysis.
}\DIFdelend 

To structure the extraction process, a reporting framework was designed to capture key aspects of each study. This framework includes details on the research purpose, methodology, data sources, dataset characteristics, main findings, and practical implications, the result of which is summarized in appendix (Table~\ref{tab:data_extraction_table}). \DIFdelbegin \DIFdel{The purpose of this step is to document how each study contributes to the understanding of financial narratives, particularly in terms of narrative modeling techniques, data sources, and the integration of NLP methodologies. }\DIFdelend While the current extraction process remains manual, future work will \DIFdelbegin \DIFdel{explore the possibility of automating parts of the extraction using additional NLP frameworks. Automation could improve efficiency and scalability, allowing for the rapid processing of a larger number of studies }\DIFdelend \DIFaddbegin \DIFadd{examine the use of unsupervised clustering approaches to group similar studies into coherent groups and limit subjective judgments during synthesis. Combining contextual clustering with automated content descriptors (TF-IDF representations or Large Language Models (LLM) summarization), would make it possible to identify, both quantitatively and qualitatively, the common characteristics shared across studies in a transparent and replicable manner}\DIFaddend .


% ------------------------------------------------------------------------------------------------------------------------


\section{Results}
\label{sec:results}

This section presents the key results of our systematic review, \DIFdelbegin \DIFdel{dividing the selected contributions into two complementary strands: papers focusing on narrative understanding and those focused on narrative modeling }\DIFdelend \DIFaddbegin \DIFadd{categorizing the academic literature into two categories: studies focused on detecting narratives in financial markets, and studies focused on modeling them empirically}\DIFaddend . The distinction lies in the epistemic and methodological aim. Narrative understanding papers aim to conceptualize or interpret the nature and role of narratives in economic contexts\DIFdelbegin \DIFdel{, often rooted in theory or qualitative methods}\DIFdelend . Narrative modeling papers, on the other hand, propose empirical or algorithmic techniques to quantify, extract, or use narratives for forecasting or explanatory purposes.

\subsection{Selection Phase Results}

The paper selection process followed the four-phase pipeline presented earlier\DIFdelbegin \DIFdel{, ensuring thematic relevance and methodological rigor}\DIFdelend . In the first phase, the initial query was applied to retrieve \nbpapersinitial{} publications without preliminary restrictions on date or subject area, maximizing the initial recall of potentially relevant studies. In the second phase, a series of metadata-based filters were applied. Publications were restricted to journal articles written in English and dated after 2010. To eliminate papers clearly outside the financial or economic domains, we automatically excluded publications that appeared in journals associated with fields unrelated to the topic. These were manually identified by subject areas such as 'Arts and Humanities', 'Medicine', 'Health Professions', 'Earth and Planetary Sciences', 'Environmental Science', 'Agricultural and Biological Sciences', 'Biochemistry, Genetics and Molecular Biology', and 'Energy'. However, we chose not to exclude papers from journals listed as 'Engineering', 'Social Sciences', 'Multidisciplinary', or with no assigned category, as their relevance is evaluated during later stages of the selection process. Duplicate entries were also identified and removed during this phase, reducing the corpus to \nbpapersjournalareas{} publications.

The third phase introduced the algorithmic selection framework. \DIFdelbegin \DIFdel{Transformer-based sentence }\DIFdelend \DIFaddbegin \DIFadd{Text }\DIFaddend embeddings were computed for the title, abstract, and keywords of each paper. In our use-case, the KMO score is \kmoscore{}, indicating strong sampling adequacy, and the CN is \conditionnumber{}, which further confirms significant multicollinearity. Both indicators suggest that PCA is well suited for this dataset and we therefore obtained four components for use in subsequent analysis. \DIFdelbegin \DIFdel{Various clustering methods were evaluated: for K-means, GMM, and AC, we obtained average relevance scores of 0.507, 0.503, and 0.481; }\DIFdelend \DIFaddbegin \DIFadd{The clustering selection resulted in }\nbpapersalgo{} \DIFadd{papers, with an average relevance and }\DIFaddend Silhouette scores of \DIFdelbegin \DIFdel{0.352, 0.288, and 0.403; and retained paper counts of 24, 22, and 50, respectively . Based on the weighted composite score, K-means was selected as the most appropriate clustering method, resulting in a final set of 24 papers in the high-relevance group.
}\DIFdelend \DIFaddbegin \DIFadd{respectively $0.507$ and $0.352$.
}\DIFaddend 

In the final phase, the manual validation process, two papers were found to be inaccessible, and several workshop proceedings were identified among the selected documents. \DIFdelbegin \DIFdel{Specifically, four }\DIFdelend \DIFaddbegin \DIFadd{Four }\DIFaddend workshop papers were detected, each containing more than one hundred individual studies. Given the uncertainty regarding their thematic alignment with the research objective\DIFdelbegin \DIFdel{and the impracticality of processing them individually within the scope of this review}\DIFdelend , these documents were excluded from the final dataset. After this refinement step, a total of \nbpapersworkshops{} papers remained for detailed analysis. The evolution of the paper count through each phase of the process is summarized in appendix (Figure~\ref{fig:paper_selection_diagram}).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{images/paper_temporal_quantile_distribution.png}
    \caption{Temporal \DIFaddbeginFL \DIFaddFL{and rank }\DIFaddendFL distributions of selected research papers.}
    \label{fig:paper_temporal_distribution}
\end{figure}

As illustrated on Figure~\ref{fig:paper_temporal_distribution}, the temporal distribution of the final selected research spans from 2014 to 2025, with a majority of studies published in recent years. This distribution suggests a growing interest in the application of NLP techniques to financial narrative analysis, particularly in the last five years. The papers originate from journals covering multiple academic disciplines, including 'Economics, Econometrics and Finance', 'Social Sciences', 'Business, Management and Accounting', 'Computer Science', 'Mathematics', 'Psychology', and 'Decision Sciences'.
\DIFdelbegin \DIFdel{The ranking of the journals, obtained from the SJR website, reveals that 6 papers were published in Q1 journals, also 6 in Q2 journals, and 5 papers published in journals that do not have a ranking.
}\DIFdelend 

\subsection{Narrative Understanding Papers}

\DIFdelbegin \DIFdel{The }\DIFdelend \DIFaddbegin \DIFadd{Across the reviewed literature, the }\DIFaddend exploration of narrative understanding in financial contexts has grown significantly, particularly as researchers \DIFdelbegin \DIFdel{began to recognize }\DIFdelend \DIFaddbegin \DIFadd{noticed }\DIFaddend that financial narratives go beyond \DIFdelbegin \DIFdel{surface-level }\DIFdelend sentiment or topic extraction. Early works laid a foundation for understanding how narratives shape belief systems, institutional expectations, and investor behavior.

\textcite{hu_annotation_2021} developed a detailed annotation model to detect opinion and emotion expressions in economic texts. Their methodology, which integrates intra-sentential labeling with economic terminology and rhetorical patterns, emphasizes the importance of understanding how subjective language conveys implicit judgments and anticipatory assessments in financial contexts. The resulting corpus, drawn from central bank speeches, 10-k fillings, news, and social media, allows for fine-grained linguistic modeling of investor and institutional sentiment, offering a window into how narratives materialize from text. From a multilingual and applied perspective, \textcite{zmandar_cofif_2022} introduced CoFiF Plus, the first large-scale corpus of French financial narrative summaries. Though the paper’s primary aim was dataset construction, the authors contextualized financial narratives as communicative devices used by firms to shape investor perceptions. Their discussion reinforces the idea that narratives \DIFdelbegin \DIFdel{are vehicles for }\DIFdelend \DIFaddbegin \DIFadd{play a central role in conveying }\DIFaddend trust and persuasion \DIFdelbegin \DIFdel{, embedded in }\DIFdelend \DIFaddbegin \DIFadd{within }\DIFaddend institutional and linguistic \DIFdelbegin \DIFdel{norms}\DIFdelend \DIFaddbegin \DIFadd{conventions}\DIFaddend . \textcite{sy_fine-grained_2023} also proposed a fine-grained argument mining approach applied to financial earnings calls, using BERT ensembles to classify and relate argumentative units. Their focus on logic structures and discourse coherence highlights the interpretative structure of financial narratives, especially in how sentiment and logic intertwine in decision-relevant communication.

\textcite{liu_beyond_2024} addressed the subtlety of year-over-year semantic drift in financial reports. They introduced the Financial-STS task to quantify nuanced differences in narratives from company disclosures. Their work \DIFdelbegin \DIFdel{moves beyond superficial }\DIFdelend \DIFaddbegin \DIFadd{goes beyond }\DIFaddend textual similarity, proposing a categorization of semantic shifts (e.g., intensified sentiment, emerging situations) that reflect how managerial narratives evolve over time. This conceptual framework offers a way to systematically detect narrative shifts relevant for investors.

Finally, \textcite{roos_narratives_2024} took a more theoretical stance, arguing that much of the empirical literature confuses topics with narratives. They provided a rigorous definition of collective economic narratives as socially shared, action-oriented stories that emerge in context and suggest coordinated beliefs or behaviors. Their contribution lies in reconciling insights from institutional economics, literary theory, and complexity economics to establish conceptual clarity. Importantly, they emphasize that narratives are distinct from mere themes or sentiment\DIFdelbegin \DIFdel{—they }\DIFdelend \DIFaddbegin \DIFadd{, and }\DIFaddend must be sense-making and socially transmitted.

Together, these \DIFdelbegin \DIFdel{contributions emphasize that financial narratives are not simply }\DIFdelend \DIFaddbegin \DIFadd{studies show that the literature conceptualizes financial narratives not as simple }\DIFaddend collections of words or topics, but \DIFaddbegin \DIFadd{as }\DIFaddend structured, evaluative, and often persuasive expressions \DIFdelbegin \DIFdel{grounded in }\DIFdelend \DIFaddbegin \DIFadd{shaped by }\DIFaddend economic language, argumentation, and context. \DIFdelbegin \DIFdel{The understanding of }\DIFdelend \DIFaddbegin \DIFadd{Understanding }\DIFaddend narratives therefore requires both conceptual clarity and methodological precision\DIFdelbegin \DIFdel{—ranging }\DIFdelend \DIFaddbegin \DIFadd{, }\DIFaddend from theoretical definitions to corpus-level annotation and semantic modeling.

\subsection{Narrative Modeling Papers}

The second group of studies focuses on modeling narratives through quantitative methods, aiming to extract, represent, and utilize narrative signals in financial prediction or macroeconomic analysis. These works share a common methodological objective: translating textual narratives into structured, predictive signals for the economy or financial markets.

Among the earliest contributions, \textcite{tuckett_tracking_2014} proposed a social-psychological framework \DIFdelbegin \DIFdel{grounded in }\DIFdelend \DIFaddbegin \DIFadd{based on }\DIFaddend conviction narrative theory, analyzing the dynamics of emotionally charged \DIFdelbegin \DIFdel{narratives---termed }\DIFdelend \DIFaddbegin \DIFadd{narratives (named }\DIFaddend ``phantastic objects''\DIFdelbegin \DIFdel{---and }\DIFdelend \DIFaddbegin \DIFadd{) and }\DIFaddend their impact on financial decision-making. Using sentiment shift detection on Reuters articles and the Enron email archive, they developed a method to capture rising emotional conviction and eventual disillusionment in narrative content, offering one of the first systematic approaches to narrative regime detection in financial text.

\textcite{hsu_narrative_2021} explored how narrative salience in historical Chinese news sources related to the U.S. Silver Purchase Act could forecast economic indicators such as price levels. This study applied textual frequency analysis combined with regularized regression, highlighting the value of narrative-based proxies even in early 20th-century macroeconomic settings.

In contemporary financial markets, \textcite{chen_covid_2022} extracted narrative features related to the COVID-19 pandemic and studied their influence on financial volatility. They showed that high narrative virality and pessimism coincided with extreme market movements, demonstrating the real-time utility of narrative indicators in turbulent contexts.

\textcite{zhu_sentiment_2023} applied deep learning and natural language processing on social media data to model housing market sentiment in China. They built forward- and backward-looking indices using LSTM-based models trained on real estate-related posts, showing that narrative sentiment predicted price dynamics and investor expectations in housing.

\textcite{borup_quantifying_2023} collected free-text investor expectations from U.S. households and applied Latent Dirichlet Allocation (LDA) topic modeling to extract narratives. Their results revealed that these subjective narrative features significantly predicted excess asset returns, outperforming traditional sentiment indices. \textcite{ma_stock_2024} also used LDA on a Narrative-based Energy General Index (NEG) constructed with news from Wall Street Journal articles, specifically targeting energy-related topics. Their results showed that NEG predicted returns in the energy sector and outperformed both macroeconomic and sentiment-based predictors. Moreover, NEG showed predictive power across multiple sectors and the aggregate market, offering a robust narrative signal for asset allocation.

From an emotional angle, \textcite{agarwal_investor_2024} used dictionary-based emotion metrics to study Chinese stock bubbles. They found that emotions such as excitement and anxiety embedded in media narratives had strong explanatory power for stock returns, trading volume, and volatility, suggesting that financial cycles are heavily narrative-driven.

\textcite{miori_narratives_2023} applied large language models (GPT-3.5) and graph theory to construct weekly networks of news narratives. Their findings indicated that fragmentation in these narrative graphs often preceded market dislocations, proposing a novel structural indicator of systemic risk. \textcite{stander_news_2024} also leveraged Transformers architecture to developed a sentiment index from South African financial news, specifically using FinBERT with the aim of improving credit risk assessment under International Financial Reporting Standard (IFRS) 9 impairments. The study showed that such a narrative-based index could serve as \DIFdelbegin \DIFdel{both a financial signal and }\DIFdelend \DIFaddbegin \DIFadd{financial signal as well as }\DIFaddend a regulatory input \DIFdelbegin \DIFdel{, especially }\DIFdelend in volatile environments.

Later, \textcite{taffler_narrative_2024} examined investor emotions during financial crises and developed crisis-specific emotion dictionaries. They demonstrated that narrative affect extracted from news explained over half of market returns during extreme events, pointing to the central role of emotion in price formation during crises.

The most recent study adopts a more structured approach: \textcite{hong_forecasting_2025} used over 880,000 WSJ articles to forecast U.S. inflation. Using topic decomposition and machine learning models, they showed that narrative predictors significantly outperformed standard macroeconomic models, especially during recessions.

\DIFdelbegin \DIFdel{Together, these studies illustrate the evolution of narrative modeling from dictionary-based sentiment measures toward dynamic, data-driven modelsleveraging deep learning, topic modeling, and Transformers models. Beyond describing narratives }\DIFdelend \DIFaddbegin \DIFadd{The literature has moved well beyond dictionary sentiment measures or topic model. Modern approaches rely on topic models}\DIFaddend , \DIFdelbegin \DIFdel{these approaches operationalize them as predictive tools for economic and financial forecasting}\DIFdelend \DIFaddbegin \DIFadd{neural networks, transformer-based representations or LLMs to capture how narratives develop and interact with market conditions, allowing them to be treated as measurable inputs in empirical analysis}\DIFaddend .

%DIF <  ------------------------------------------------------------------------------------------------------------------------
\DIFaddbegin \subsection{\DIFadd{Stability Checks}}
\DIFaddend 

\DIFdelbegin \section{\DIFdel{Discussion}}
%DIFAUXCMD
\addtocounter{section}{-1}%DIFAUXCMD
%DIFDELCMD < \label{sec:discussions}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{We propose to examine whether the selection of high-relevance papers is stable under reasonable modifications of the dimensionality reduction and clustering steps. The objective is to verify that the final corpus is not the result of specific parameter choices. Robustness is assessed using the three control metrics introduced earlier: average relevance score, Silhouette score, and retained paper count. We also report the number of papers added or removed relative to the baseline, defined as PCA retaining 99\% of explained variance followed by K-means clustering. Sensitivity to word choice in the statements was mitigated by generating multiple paraphrased versions of each statement and averaging their embeddings. This approach reduces the dependence of similarity scores on specific formulations and lowers the risk of word-sensitivity effects. Because this mitigation is built directly into the pipeline, no additional robustness checks were performed on the wording of the statements. Similarly, model-selection robustness was not performed, since high-performance closed-source embedding models tend to yield comparable semantic behaviour. Open-source testing is left for future work on portability and domain adaptation.
}\DIFaddend 

\DIFdelbegin \DIFdel{This section discusses the key insights derived from the systematic review, structured around the guiding research questions and }\DIFdelend \DIFaddbegin \DIFadd{We evaluated PCA robustness by varying the retained explained variance within 80\%, 85\%, 90\%, 95\% and 98\%. Across all configurations, K-means consistently recovered the same set of 26 high-relevance papers. The average relevance score remained fixed at 0.504, the Silhouette value stayed at 0.347, and no papers were added or removed relative to the baseline. These results show that the selection step is essentially insensitive to the choice of PCA variance threshold within a wide and standard range. PCA improves interpretability, but the exact level of dimensionality reduction does not affect the composition or the quality of the high-relevance cluster.
}

\begin{table}[h]
\centering
\caption{\DIFaddFL{Comparison of clustering configurations applied to the PCA-transformed similarity space.}}
\label{tab:clustering_comparison}
\begin{tabular}{lccc}
\toprule
\textbf{\DIFaddFL{Method}} & \textbf{\DIFaddFL{\#Papers}} & \textbf{\DIFaddFL{Avg. relevance}} & \textbf{\DIFaddFL{Silhouette}} \\
\midrule
\DIFaddFL{K-means (baseline) }& \DIFaddFL{26 }& \DIFaddFL{0.504 }& \DIFaddFL{0.347 }\\
\DIFaddFL{Gaussian Mixture Model (GMM) }& \DIFaddFL{19 }& \DIFaddFL{0.509 }& \DIFaddFL{0.270 }\\
\DIFaddFL{Agglomerative Clustering (AC) }& \DIFaddFL{50 }& \DIFaddFL{0.480 }& \DIFaddFL{0.400 }\\
\bottomrule
\end{tabular}
\end{table}

\DIFadd{We also compared the baseline K-means results to Gaussian Mixture Models (GMM) and Agglomerative Clustering (AC), applied to the same PCA space and identical number of clusters. The comparison shows that alternative clustering strategies modify the size and composition of the high-relevance group. GMM yields the highest  average relevance score but removes seven papers that appear in the baseline and retains fewer papers overall. AC produces the  highest Silhouette score but does so by expanding the cluster to fifty papers, many of which have  substantially lower relevance scores. Both alternatives therefore weaken at least one of the quality criteria: GMM reduces overlap and retains fewer relevant papers, while AC incorporates a large number of marginal studies. Relative to  these configurations, K-means maintains a stable cluster size, balanced internal cohesion, and a higher concentration of highly relevant papers, which makes it the most reliable choice for }\DIFaddend the \DIFdelbegin \DIFdel{patterns identified during the application of the proposed methodology. The discussion is organized into three main dimensions: conceptual, focusing on the definitions and theoretical foundations of narratives; methodological, examining the techniques employed for modeling and analyzing narratives; and practical, addressing the empirical challenges and applications observed in the reviewed studies. Each dimension highlights both the advancements and the limitations within the existing literature, providing a foundation for identifying future research directions. }\DIFdelend \DIFaddbegin \DIFadd{final selection.
}\DIFaddend 


\DIFdelbegin \subsection{\DIFdel{Toward a Shared Understanding of Financial Narratives}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
\DIFdelend %DIF >  ------------------------------------------------------------------------------------------------------------------------


\DIFdelbegin \DIFdel{Recent works have increasingly challenged the use of simplistic proxies—such as word frequency counts, aggregate sentiment scores}\DIFdelend \DIFaddbegin \section{\DIFadd{Discussion}}

\DIFadd{This section synthesizes the key insights of the review in relation to the two guiding research questions and the broader literature on financial narratives. We examine how existing studies define and model narratives}\DIFaddend , \DIFdelbegin \DIFdel{or isolated topic modeling—for capturing the complexity of narratives. A more holistic perspective is emerging, recognizing that narrativesoperate as evolving structures combining thematic content, emotional tone, semantic coherence, and temporal dynamics. Under this view, narratives are not staticinformation snapshots but dynamic interpretive frameworks that influence expectations, decision-making, and market behavior. }\DIFdelend \DIFaddbegin \DIFadd{how NLP techniques are used to model them, and what this reveals about the current state and direction of the field. The discussion also outlines the methodological contribution of this work, its limitations, and the implications for future research on both financial narratives and systematic literature analysis.
}\DIFaddend 

\DIFdelbegin \DIFdel{This evolution in conceptual thinking suggests that modeling financial narratives should move beyond isolated metrics toward more integrated approaches that simultaneously consider multiple dimensions of narrative structure }\DIFdelend \DIFaddbegin \subsection{\DIFadd{Adressing Research Questions}}

\textbf{\DIFadd{(RQ1) How can NLP and textual analysis techniques be used to quantify and model financial narratives?}}  
\DIFadd{The reviewed literature reveals a clear evolution from static, sentiment-based proxies toward models that account for the structure, tone, and temporal dynamics of financial discourse. Early studies typically relied on lexicon-based sentiment scores or topic frequencies, which offered limited contextual understanding and failed to capture how meaning changes with context. More recent works apply transformer-based architectures (contextual embeddings or LLMs) that enable a more nuanced understanding of narrative formation }\DIFaddend and evolution. \DIFdelbegin \DIFdel{By capturing changes in thematic content, semantic meaning, emotional tone, or interpretive coherence, more robust and actionable insights may be extracted, particularly for applications in investment context. }\DIFdelend \DIFaddbegin \DIFadd{These approaches represent a shift from simply identifying textual sentiment to modeling the relationships between ideas, actors, and events as they unfold across time.  
}\DIFaddend 

\DIFdelbegin \DIFdel{The reviewed literature also indicates an increasing awareness that financial narrativescan exhibit heterogeneous effects depending on the assets or markets they pertain to. Understanding how narratives evolve across different asset classes, sectors, or currencies—and how their semantic and emotional characteristics shift over time—could offer more precise and reliable signals than sentiment analysis alone. Such developments point toward a promising research direction where narratives are treated as dynamic, multi-faceted phenomena whose holistic analysis can enhance both explanatory power and predictive capability in financial market applications}\DIFdelend \DIFaddbegin \DIFadd{Another stream of research also seeks to represent narratives as continuous trajectories in semantic space, where their position and movement capture diffusion, persistence, and transformation. Embedding-based temporal models and context-aware clustering allow for detecting the emergence of new narratives while tracking existing ones as they evolve. Such methods move narrative modeling closer to dynamic systems analysis (capable of representing reinforcement, decay, and convergence between narratives) rather than treating them as fixed textual objects.  
}

\DIFadd{Despite these advances, the field remains fragmented. Few studies provide methodological benchmarks or reproducible validation protocols, and differences in data sources and preprocessing often limit comparability. Transformer models improve contextual understanding but also introduce challenges such as interpretability, computational cost, and out-of-sample testing. Overall, NLP techniques now make it possible to represent narratives as evolving, probabilistic structures detected in financial texts, but consistent evaluation frameworks and methodological transparency are still missing.
}

\textbf{\DIFadd{(RQ2) How is the concept of financial narrative defined and applied, and what methodological patterns emerge?}}  
\DIFadd{Across the literature, financial narratives are defined with varying nuances, ranging from broad communicative structures to more specific social mechanisms of belief formation. Despite these differences, a clear convergence has emerged: narratives are now viewed as structured interpretative frameworks that link facts, emotions, and expectations, influencing how information spreads, thus impacting financial markets}\DIFaddend .

\DIFdelbegin \DIFdel{Overall, the emerging consensus supports the need for methodologies that integrate thematic, emotional, and semantic dimensions of narratives, rather than relying on any single proxy. While the operationalization of such holistic models remains an open area of research, this shift lays the foundation for future frameworks capable of capturing the full complexity and market relevance of financial narratives}\DIFdelend \DIFaddbegin \DIFadd{Methodologically, most empirical researches operationalize narratives as clusters of semantically related texts or as latent features extracted from large corpora. The consensus is that narratives are not micro-level topics or isolated opinions but recurrent, large-scale interpretive patterns that structure discourse over time. This perspective implies that purely sentiment-based approaches, while informative about emotional tone, are insufficient for advanced narrative modeling. Sentiment captures a snapshot of mood, whereas narratives encompass the attention share, evolution and linkage of ideas that give direction to financial markets}\DIFaddend .  

\DIFdelbegin \subsection{\DIFdel{Methodological Advances in Narrative Quantification}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
\DIFdelend \DIFaddbegin \DIFadd{Consequently, progress in this field depends on treating narratives as evolving informational processes rather than static text aggregates. Future approaches should aim to capture how narratives emerge, persist, and recur across time, assigning latent intensity scores to narrative structures while allowing to detect new ones as discourse evolves. Models that can adapt to changing contexts, learn from new data, and represent the temporal continuity of narratives would make it possible to study not only their semantic content but also their life cycle: how they grow, transform, and fade.
}\DIFaddend 

\DIFdelbegin \DIFdel{From a methodological perspective, the field has still progressed from basic sentiment or emotion tagging and topic extraction toward more sophisticated architectures that better capture the complexity of narrative content }\DIFdelend \DIFaddbegin \subsection{\DIFadd{Methodological Contribution}}

\DIFadd{This paper contributes methodologically by introducing an algorithmic framework for systematic literature reviews that improves reproducibility, efficiency, and selection quality assessment. Unlike traditional SLRs based solely on manual screening, the proposed framework combines semantic embeddings, dimensionality reduction, and clustering to automate and document inclusion decisions}\DIFaddend .
\DIFdelbegin \DIFdel{This includes unsupervised models for discovering latent narrative structures, deep learning for dynamic sentiment classification, and graph-based methods for understanding narrative coherence and fragmentation over time.
}\DIFdelend 

\DIFdelbegin \DIFdel{Crucially, these methodological advances coincide with the recognition that narratives operate at different levels—micro (e.g., firm-level disclosures), meso (sectoral or thematic coverage), }\DIFdelend \DIFaddbegin \DIFadd{The framework differs from prior bibliometric approaches in two main ways. First, it evaluates study relevance semantically rather than lexically, enabling identification of conceptually related works even when terminology and key words evolve. Second, it quantifies selection quality using the number of papers retained (larger sets indicate fewer false exclusions), the average relevance and Silhouette scores, instead of relying solely on researcher judgment. These components enhance methodological transparency, reduce subjectivity, }\DIFaddend and \DIFdelbegin \DIFdel{macro (aggregate discourse on inflation, risk, or systemic events). Each level requires distinct modeling assumptions and data representations.
For example, firm-level narratives often benefit from semantic similarity measures, while macroeconomic narratives lend themselves to topic evolution tracking or network modeling}\DIFdelend \DIFaddbegin \DIFadd{make the process more scalable.
}

\DIFadd{In practice, these improvements affect efficiency and reproducibility through several channels. Automation reduces manual screening time and ensures that selection decisions follow consistent semantic criteria. The standardized embedding and clustering pipeline makes the review replicable across domains, while quantitative quality metrics provide objective validation of the selection process. The approach also supports iterative refinement: new literature can be incorporated by re-embedding and re-clustering, allowing systematic reviews to evolve rather than remain fixed snapshots}\DIFaddend .

\DIFdelbegin \DIFdel{The growing use of transformer-based models and domain-specific fine-tuning represents a turning point in narrative modeling. These architectures enable contextual understanding and greater sensitivity to modality, negation, and ambiguity—features that are essential in financial language}\DIFdelend \DIFaddbegin \subsection{\DIFadd{Limitations}}

\DIFadd{Despite the robustness of the framework, several limitations remain. The first concerns database coverage: Scopus was selected for its reliability, comprehensive metadata, and accessible API}\DIFaddend . However, \DIFdelbegin \DIFdel{this increased complexity also raises concerns about interpretability and robustness, particularly when models are used for decision-making or supervision. }\DIFdelend \DIFaddbegin \DIFadd{its focus on peer-reviewed journals inevitably narrows the scope of analysis and may omit early-stage or gray literature, particularly in fast-moving domains such as NLP and financial AI. A pragmatic solution is to combine Scopus with open bibliometric sources such as OpenAlex, which aggregates content from repositories. This hybrid approach could capture both established and emerging research, reducing potential source bias. A second area for improvement relates to data accessibility and document selection. Excluding publications without full-text access ensured consistent processing but introduced an unavoidable bias toward open-access or recently published papers.
}\DIFaddend 

\DIFdelbegin \DIFdel{The methodological landscape is rich and expanding, but still fragmented. Few studies attempt to benchmark across models or narrative levels.
Future work would benefit from methodological synthesis and from clarifying which techniques are most appropriate for different types of narrative inquiry. }\DIFdelend \DIFaddbegin \DIFadd{Another consideration concerns the embedding and clustering pipeline. Transformer models provide strong semantic alignment but reflect the biases and temporal characteristics of their pre-training data, and the model choice is crucial. Bidirectional encoders adapt more smoothly to shifts in vocabulary and phrasing than autoregressive GPT-style models, which makes them somewhat more robust when new terms or domain-specific expressions enter the corpus. Additionally, fine-tuning using domain-specific corpora or contrastive learning  could mitigate these issues. The current exclusion of medium-relevance clusters, while enhancing precision, may also remove studies at the edge of the topic that still contribute valuable perspectives. A semi-automatic variant where medium clusters are manually reviewed under transparent inclusion rules (e.g. words presences in abstract or keywords, similarity score with one or subset of scores, etc.) would balance reproducibility with interpretative flexibility, strengthening the overall robustness of the review.
}\DIFaddend 

\DIFdelbegin \subsection{\DIFdel{Narrative Analysis, Behavioral Signals and Market Applications}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
\DIFdelend \DIFaddbegin \DIFadd{Finally, the practical implementation of algorithmic literature reviews raises trade-offs between transparency, efficiency, and scalability. The choice between open- and closed-source language models could eventually affect not only accuracy but also cost, reproducibility, and control over the pipeline. Comparable open-source models allow greater transparency and fine-tuning to specific research domains but require significant computational resources. Closed-source models, in contrast, provide higher semantic precision and lower setup complexity but at the expense of interpretability, long-term accessibility, and dependency on proprietary infrastructures. Beyond the model choice, scalability also remains a challenge: embedding and clustering a few hundred abstracts is manageable on a standard workstation, but extending the process to tens of thousands of papers would require advanced computation resources.
}\DIFaddend 

\DIFdelbegin \DIFdel{Perhaps the most promising insight from the reviewed literature is the empirical validation of narratives as predictive tools. Several studies demonstrate that narrative signals, when carefully extracted, can outperform traditional indicators in forecasting asset prices, returns, volatility, or macroeconomic trends.These results provide strong evidence that narratives are not just epiphenomena but contain economically relevant information.
}\DIFdelend \DIFaddbegin \subsection{\DIFadd{Implications and Future Research Directions}}
\DIFaddend 

\DIFdelbegin \DIFdel{A notable theme is the connection between narrative features and behavioral finance. Emotional intensity, narrative coherence, and topical salience appear to correlate with phases of market exuberance, uncertainty, and dislocation. In particular, emotionally charged narratives—those expressing excitement, fear, or anxiety—are repeatedly linked to bubbles, crashes, and risk-on/risk-off cycles.
This supports the idea that narratives act as vehicles for behavioral contagion, and that narrative monitoring could serve as an early warning signal}\DIFdelend \DIFaddbegin \DIFadd{The findings of this review carry implications for both the study of financial narratives and the methodology of literature synthesis}\DIFaddend .

\DIFdelbegin \DIFdel{Beyond prediction, narrative modeling is increasingly seen as relevant for systemic risk assessment, regulatory compliance (e.g., IFRS 9 impairments), and investor sentiment analysis. These applications show that narrative analytics is moving beyond academic inquiry toward operational relevance. However, this transition also demands greater transparency and standardization in model design, evaluation, and communication. }\DIFdelend \DIFaddbegin \DIFadd{From a conceptual angle, financial narrative research would benefit from unified framework and stronger theoretical integration with behavioral finance and information economics. Future studies should establish consistent typologies and empirically test how these categories map to market phenomena such as volatility, liquidity, or structural breaks.
}\DIFaddend 

\DIFdelbegin \DIFdel{One limitation that persists is the reliance on textual sources with unclear representativeness. While surveys, news, and firm disclosures are widely used, they capture different populations and communicative intentions.
Understanding the scope and bias of each source remains necessary when translating narrative signals into financial decisions or policy interventions. }\DIFdelend \DIFaddbegin \DIFadd{From a methodological perspective, several strands of work appear particularly valuable. Comparative studies across narrative scales, from firm-specific disclosures to sectoral texts and broad macroeconomic discourse, would help clarify which NLP techniques perform best in each setting. Interpretability remains essential, since the increasing reliance on deep models requires an understanding of how textual features translate into narrative signals. A further step is to link narrative measures with quantitative market data in a coherent framework, which would make it possible to study how changes in discourse relate to price formation and other financial outcomes.
}\DIFaddend 

\DIFdelbegin \DIFdel{In sum}\DIFdelend \DIFaddbegin \DIFadd{From a practical perspective, narrative insights are beginning to influence financial practice, from risk monitoring to regulatory stress testing. However}\DIFaddend , the reviewed literature \DIFdelbegin \DIFdel{supports the idea that narratives encode both belief and behavior.
They mediate the translation of information into market action and represent a frontier where behavioral finance, macroeconomics, and data science converge}\DIFdelend \DIFaddbegin \DIFadd{still lacks standardized evaluation frameworks or open benchmarks to assess the robustness of such indicators. Collaborative efforts between academia, regulators, and industry could accelerate the development of shared datasets and transparent validation protocols.
}

\DIFadd{Overall, this review shows that the field of financial narrative modeling is advancing rapidly but unevenly. Conceptual fragmentation, data limitations, and methodological opacity remain key barriers. Addressing these issues will require stronger theoretical integration, standardized empirical protocols, and reproducible computational tools}\DIFaddend .


\DIFdelbegin \subsection{\DIFdel{Contribution and Scope}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend % ------------------------------------------------------------------------------------------------------------------------


\section{Conclusion}
\DIFdelbegin %DIFDELCMD < \label{sec:conclusion}
%DIFDELCMD < %%%
\DIFdelend 

This paper introduces a systematic literature review framework that combines algorithmic paper selection with structured data extraction to investigate how financial narratives are conceptualized, quantified, and \DIFdelbegin \DIFdel{utilized }\DIFdelend \DIFaddbegin \DIFadd{applied }\DIFaddend in financial market analysis. Leveraging NLP and machine learning tools, the review process refined an initial dataset of \nbpapersinitial{} papers down to \nbpapersworkshops{} core contributions, classified into two main categories: narrative understanding and narrative modeling. The effectiveness of the selection framework is supported by the relevance and depth of the insights obtained from the final corpus. The selected research provides key contributions toward addressing the guiding research questions, highlighting the evolving conceptualization of narratives, the methodological advances enabling their modeling, and the potential applications of narrative-based signals in understanding and forecasting financial market behavior.

Studies under narrative understanding \DIFdelbegin \DIFdel{primarily }\DIFdelend \DIFaddbegin \DIFadd{mainly }\DIFaddend seek to define the theoretical, semantic, and rhetorical boundaries of financial narratives. They offer annotation schemes, linguistic typologies, and structural representations that frame narratives as evaluative, temporal, and action-oriented discourses. These works contribute essential groundwork for developing more robust computational models. Secondly, the narrative modeling stream treats narratives as predictive signals. These studies use narrative-derived features to forecast inflation, returns, systemic risk, and investor behavior. They demonstrate that narratives can improve both the explanatory and predictive power of econometric and machine learning models, \DIFdelbegin \DIFdel{particularly }\DIFdelend \DIFaddbegin \DIFadd{especially }\DIFaddend during periods of market dislocation or heightened uncertainty. Importantly, a few of these contributions go beyond traditional sentiment or topic analysis and incorporate structural and behavioral dimensions, such as narrative virality, emotional polarity, and semantic shift.

The review highlights both the richness and the fragmentation of the current literature. On the one hand, there is increasing recognition that financial narratives are more than expressions of sentiment or thematic frequency: they are structured, evolving, and socially embedded interpretations of economic phenomena. On the other hand, approaches to operationalizing and modeling these narratives remain diverse, with varying assumptions about what constitutes a meaningful narrative signal.

From a methodological standpoint, the field has progressed rapidly, incorporating unsupervised topic modeling, deep learning, and large language models that enabled the construction of narrative indices that reflect attention, sentiment, emotion, and network structure. \DIFdelbegin \DIFdel{At the same time, foundational }\DIFdelend \DIFaddbegin \DIFadd{Foundational }\DIFaddend work continues to refine the linguistic, rhetorical, and epistemic dimensions of financial narratives, suggesting the need for continuous improvements in technical approaches. Consequently, this review finds that NLP and textual analysis techniques offer a broad set of tools for quantifying financial narratives in a systematic manner. The evidence strongly suggests that incorporating narrative modeling enhances our understanding of market dynamics, as narrative-based indicators consistently capture behavioral signals that conventional financial and macroeconomic variables often overlook. These indicators not only contribute to improved forecasting performance but also enhance the interpretability of those predictions, providing a structured lens through which to analyze market sentiment, contagion effects, and regime shifts—phenomena that are otherwise difficult to quantify.

In conclusion, the literature reviewed here supports the growing view that narratives are not peripheral to financial markets but are central to how agents form beliefs, process information, and make decisions. As tools for extracting and modeling narratives become more sophisticated, their role in empirical finance, policy analysis, and risk management is likely to expand\DIFdelbegin \DIFdel{, offering new insights into the behavioral foundations of economic activity}\DIFdelend . Nonetheless, current approaches face several limitations: the representativeness of textual data remains uncertain; interpretability of complex NLP models is still a concern; and the validation of narrative indicators across market regimes is limited. Addressing these challenges will be critical to establishing narratives as robust, transparent, and actionable components of financial analysis.


% ------------------------------------------------------------------------------------------------------------------------


\newpage

\section*{List of Abbreviations}

\begin{itemize}
  \item \textbf{AC} – Agglomerative Clustering
  \item \textbf{API} – Application Programming Interface
  \item \textbf{CN} – Condition Number
  \item \textbf{GMM} – Gaussian Mixture Model
  \item \textbf{IFRS} – International Financial Reporting Standards
  \item \textbf{KMO} – Kaiser–Meyer–Olkin Test
  \item \textbf{LDA} – Latent Dirichlet Allocation
  \item \textbf{ML} – Machine Learning
  \item \textbf{NEG} – Negative Emotion Score
  \item \textbf{NLP} – Natural Language Processing
  \item \textbf{PCA} – Principal Component Analysis
  \item \textbf{SLR(s)} – Systematic Literature Review(s)
\end{itemize}


% ------------------------------------------------------------------------------------------------------------------------


\newpage

\printbibliography


% ------------------------------------------------------------------------------------------------------------------------


\newpage

\section*{Appendix}
\label{appendix}


\begin{figure}[ht]
    \centering
    \resizebox{0.45\textwidth}{!}{
        \begin{tikzpicture}[node distance=1cm]
    
            % Nodes
            \node (database) [database] {\textbf{DataBase}\\(Scopus)};
            \node (query) [process, below=of database] {\textbf{Search Query}\\(TITLE, ABS and KEYWORDS)};
            \node (n1) [phase, right=0.1cm of query, align=center, text width=2.5cm] {Phase 1:\\N = \nbpapersinitial{}};
            
            \node (filter) [process, below=of query] {\textbf{Apply Filters}\\(journal area, date, paper type, language)};
            \node (n2) [phase, right=0.1cm of filter, align=center, text width=2.5cm] {Phase 2: N = \nbpapersjournalareas{}};
            
            \node (algo) [process, below=of filter] {\textbf{Algorithmic Selection}\\(text embedding, cosine similarity, dimensionality\\reduction, clustering)};
            \node (n3) [phase, right=0.1cm of algo, align=center, text width=2.5cm] {Phase 3: N = \nbpapersalgo{}};
            
            \node (manual) [process, below=of algo] {\textbf{Manual Exclusion}\\(papers’ online availability, workshops)};
            \node (n4) [phase, right=0.1cm of manual, align=center, text width=2.5cm] {Phase 4: N = \nbpapersworkshops{}};
            
            \node (results) [decision, below=of manual, text width=4cm] {\textbf{Results Analysis}\\and\\\textbf{Data Extraction}};
            
            % Arrows
            \draw [arrow] (database) -- (query);
            \draw [arrow] (query) -- (filter);
            \draw [arrow] (filter) -- (algo);
            \draw [arrow] (algo) -- (manual);
            \draw [arrow] (manual) -- (results);
        
        \end{tikzpicture}
    }
    \caption{Schematic representation of the paper selection process. The process includes database querying, filtering by inclusion/exclusion criteria, algorithmic selection via NLP and clustering, and manual exclusion of inaccessible studies or workshop proceeds.}
    \label{fig:paper_selection_diagram}
\end{figure}


\begin{landscape}
\begin{longtable}{p{5cm} p{4cm} p{4cm} p{4cm} p{4cm}}
    \caption{Summary of the data extraction phase.}
    \label{tab:data_extraction_table} \\
    \toprule
    \textbf{Paper} & \textbf{Label} & \textbf{Purpose} & \textbf{Method} & \textbf{Narrative} \\
    \midrule
    \endfirsthead
    
    \toprule
    \textbf{Paper} & \textbf{Label} & \textbf{Purpose} & \textbf{Method} & \textbf{Narrative} \\
    \midrule
    \endhead
    
    \bottomrule
    \endfoot

    \textcite{tuckett_tracking_2014} & narrative modeling & Track evolution of emotionally charged financial narratives (phantastic objects) before crises. & Emotion keyword dictionaries, sentiment shift scoring, network analysis of sender-receiver patterns. & Excitement vs. anxiety sentiment index + social network clustering reveal narrative divergence before collapse. \\
    \textcite{hu_annotation_2021} & narrative understanding & Develop a corpus and model for detecting opinion and emotion in financial narratives. & Manual + SpaCy annotation using appraisal theory; dependency parsing, syntactic tagging. & Appraisal-based annotations, intra-sentence pairings of opinion and targets. \\
    \textcite{hsu_narrative_2021} & narrative modeling & Assess how narrative topics in newspaper articles relate to macroeconomic indicators in 1930s China. & Keyword frequency tracking, Ridge/LASSO/Elastic Net regressions, VAR, IRF, Granger causality. & Manual keyword selection and normalized frequency analysis linked to time series regression models. \\
    \textcite{zmandar_cofif_2022} & narrative understanding & Create a large-scale French corpus for summarizing financial reports. & Heuristic + CamemBERT + manual summary extraction. & Summary-level mappings from report sections; NER for entity highlights. \\
    \textcite{chen_covid_2022} & narrative modeling & Assess narrative influence during COVID on market variables using causal testing. & LDA, LM sentiment, Word2Vec, SIR virality, VAR and Granger causality. & Narratives via LDA, semantic shift, and virality scoring, linked to econometric causality. \\
    \textcite{zhu_sentiment_2023} & narrative modeling & Build a future-oriented sentiment index from Weibo posts on housing. & LSTM sentence classification into temporal/sentiment classes, using Word2Vec. & Narrative sentiment split by temporal framing and learned via deep LSTM classifier. \\
    \textcite{sy_fine-grained_2023} & narrative understanding & Improve financial sentiment analysis via argumentative unit detection. & BERT ensemble for argument classification and relation detection. & Pairwise relation classification and claim-premise detection with BERT ensemble. \\
    \textcite{miori_narratives_2023} & narrative modeling & Use GPT + graph theory to extract narrative structure from news and link to market dislocations. & Entity extraction via GPT; co-occurrence graphs; community detection; regress network features on volatility shocks. & GPT-ranked entities + graph metrics (modularity, entropy); topic communities track narrative complexity. \\
    \textcite{borup_quantifying_2023} & narrative modeling & Analyze investor narratives via open-ended surveys during COVID and assess predictive value for markets. & LDA on survey texts; Elastic Net VARs; comparison with media narratives. & LDA-derived narrative topics tracked daily and linked to market behavior via econometric models. \\
    \textcite{stander_news_2024} & narrative modeling & Construct a news sentiment index to act as an early warning for systemic risk and credit impairments. & FinBERT sentiment scoring, PCA on macro indicators, rolling regressions, aspect-based sentiment. & FinBERT sentiment, topic-based aspect analysis, PCA and regressions to link to macro risk. \\
    \textcite{liu_beyond_2024} & narrative understanding & Detect subtle semantic shifts in firm narratives using new financial-STS task. & Triplet-based contrastive learning with GPT-labeled sentence triplets. & Triplet embedding training for narrative similarity under financial framing. \\
    \textcite{agarwal_investor_2024} & narrative modeling & Measure investor emotions from media during market bubbles and link them to returns, volatility, and volume. & Emotion keyword dictionaries; regression on market variables during two Chinese stock bubbles. & Emotion frequency indices (8 emotions), strong vs. weak emotion separation, applied in regression models. \\
    \textcite{taffler_narrative_2024} & narrative modeling & Analyze emotional content of narratives during market crises and its explanatory power over returns and uncertainty. & Keyword-based emotion indices; regression with returns, VIX, volume, EPU during three crises. & Context-specific emotion dictionary applied to financial articles; emotion score time series regressed on market indicators. \\
    \textcite{roos_narratives_2024} & narrative understanding & Review and define the concept of collective economic narratives in economics. & Theoretical review of 436 papers; derive five defining narrative features. & Defines narratives as temporal, socially emergent, sense-making, and action-suggesting story structures. \\
    \textcite{ma_stock_2024} & narrative modeling & Use WSJ-derived narrative index (NEG) to forecast stock returns in the energy sector and beyond. & LDA on WSJ energy topics; predictive regressions; Sharpe/CE utility evaluation. & NEG index from topic attention on WSJ news, tracked as monthly time series and forecast factor. \\
    \textcite{hong_forecasting_2025} & narrative modeling & Forecast U.S. inflation using WSJ-derived narrative features. & LDA for topic modeling, ML regressors (LASSO, ENet, RF, PLS). & LDA narrative topics as ML features in out-of-sample inflation prediction. \\
    
\end{longtable}
\end{landscape}


% ------------------------------------------------------------------------------------------------------------------------


\end{document}